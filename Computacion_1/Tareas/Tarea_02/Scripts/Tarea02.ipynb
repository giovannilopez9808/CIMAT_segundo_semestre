{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giovanni Gamaliel López Padilla\n",
    "#### Procesamiento de lenguaje natural\n",
    "#### Tarea 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_path(path: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Une la direccion de un archivo con su nombre\n",
    "    \"\"\"\n",
    "    return \"{}{}\".format(path, filename)\n",
    "\n",
    "\n",
    "def get_texts_from_file(path_data: str, path_labels: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Obtiene una lista de oraciones a partir de un texto con sus respectivas etiquetas\n",
    "    \"\"\"\n",
    "    # Inicilizacion de las listas\n",
    "    text = []\n",
    "    labels = []\n",
    "    # Apertura de los archivos\n",
    "    with open(path_data, \"r\") as f_data, open(path_labels, \"r\") as f_labels:\n",
    "        # Recoleccion de las oraciones\n",
    "        for tweet in f_data:\n",
    "            text += [tweet]\n",
    "        # Recoleccion de las etiquedas\n",
    "        for label in f_labels:\n",
    "            labels += [label]\n",
    "    # Etiquedas a enteros\n",
    "    labels = list(map(int, labels))\n",
    "    return text, labels\n",
    "\n",
    "\n",
    "def sort_freqdist(fdist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Ordena la lista de distribucion de frecuencias de palabras de mayor frecuencia a menor\n",
    "    \"\"\"\n",
    "    aux = [(fdist[key], key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "\n",
    "\n",
    "def split_data(data: list, max_words: int) -> list:\n",
    "    \"\"\"\n",
    "    Realiza la separacion de elementos en una lista dado el numero de elementos que se quieren conservar\n",
    "    \"\"\"\n",
    "    return data[:max_words]\n",
    "\n",
    "\n",
    "def obtain_fdist(data: list, max_words: int) -> list:\n",
    "    \"\"\"\n",
    "    Obtiene la lista de una distribucion de frecuencias de palabras ordenada de mayor a menor a partir de una lista de oraciones\n",
    "    \"\"\"\n",
    "    # Inicializacion del Tokenizador\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Inicializacion de la lista que guardara los tokens\n",
    "    corpus_palabras = []\n",
    "    for tweet in data:\n",
    "        # Creacion y guardado de los tokens\n",
    "        corpus_palabras += tokenizer.tokenize(tweet)\n",
    "    # Creacion de la distribucion de frecuencias\n",
    "    fdist = nltk.FreqDist(corpus_palabras)\n",
    "    fdist = sort_freqdist(fdist)\n",
    "    fdist = split_data(fdist, max_words)\n",
    "    return fdist\n",
    "\n",
    "\n",
    "def create_dictonary_of_index(fdist: list) -> dict:\n",
    "    \"\"\"\n",
    "    Crea un diccionario con la posición de mayor a menor frecuencia de cada palabra. La llave es la palabra a consultar\n",
    "    \"\"\"\n",
    "    # Inicializacion del diccionario\n",
    "    index = dict()\n",
    "    # Inicializacion de la posicion\n",
    "    i = 0\n",
    "    for weight, word in fdist:\n",
    "        index[word] = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "\n",
    "def build_binary_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos binarios\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = 1\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_binary_bow_with_probabilities(data: list, fdist: list, index: dict,\n",
    "                                        probability: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos binarios\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = 1\n",
    "                if word in probability:\n",
    "                    bow[docs, index[word]] = probability[word]\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_frecuency_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = tweet.count(word)\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_frecuency_bow_with_probabilities(data: list, fdist: list,\n",
    "                                           index: dict,\n",
    "                                           probability: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = tweet.count(word)\n",
    "                if word in probability:\n",
    "                    bow[docs, index[word]] *= probability[word]\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def create_empty_dictionary_of_words_and_documents(words: dict,\n",
    "                                                   data: list) -> dict:\n",
    "    \"\"\"\n",
    "    Crea un diccionario el cual contendra de forma ordenada el indice de cada palabra y su numero de frecuencias en una coleccion\n",
    "    \"\"\"\n",
    "    freq_word_per_document = dict()\n",
    "    word_count = dict()\n",
    "    for i, tweet in enumerate(data):\n",
    "        word_count[i] = 0\n",
    "    for word in words:\n",
    "        freq_word_per_document[word] = word_count\n",
    "    return freq_word_per_document\n",
    "\n",
    "\n",
    "def build_tfidf_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Inicilizacion del bow\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    # Total de oraciones\n",
    "    n = len(data)\n",
    "    # Inicializacion del diccionario que contiene la repeticion de cada palabra\n",
    "    idf_per_word_and_document = create_empty_dictionary_of_words_and_documents(\n",
    "        index.keys(), data)\n",
    "    for docs, tweet in enumerate(data):\n",
    "        # Frecuencias\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                # Descriptiva\n",
    "                tf = tweet.count(word)\n",
    "                idf_per_word_and_document[word][docs] += 1\n",
    "                bow[docs, index[word]] = np.log(tf + 1)\n",
    "\n",
    "    # Discriminativa\n",
    "    for word in index.keys():\n",
    "        idf = sum(idf_per_word_and_document[word].values())\n",
    "        idf = np.log(n / idf)\n",
    "        for docs, tweet in enumerate(data):\n",
    "            bow[docs, index[word]] = bow[docs, index[word]] * idf\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_tfidf_bow_with_probabilities(data: list, fdist: list, index: dict,\n",
    "                                       probability: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Inicilizacion del bow\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    # Total de oraciones\n",
    "    n = len(data)\n",
    "    # Inicializacion del diccionario que contiene la repeticion de cada palabra\n",
    "    idf_per_word_and_document = create_empty_dictionary_of_words_and_documents(\n",
    "        index.keys(), data)\n",
    "    for docs, tweet in enumerate(data):\n",
    "        # Frecuencias\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                # Descriptiva\n",
    "                tf = tweet.count(word)\n",
    "                idf_per_word_and_document[word][docs] += 1\n",
    "                bow[docs, index[word]] = np.log(tf + 1)\n",
    "\n",
    "    # Discriminativa\n",
    "    for word in index.keys():\n",
    "        idf = sum(idf_per_word_and_document[word].values())\n",
    "        idf = np.log(n / idf)\n",
    "        for docs, tweet in enumerate(data):\n",
    "            if word in probability:\n",
    "                bow[docs, index[word]] *= probability[word]\n",
    "            bow[docs, index[word]] *= idf\n",
    "    return bow\n",
    "\n",
    "\n",
    "def create_model(bow_tr: np.array, labels_tr: np.array) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Creacion del modelo para realizar el aprendizaje\n",
    "    \"\"\"\n",
    "    parameters_model = {\"C\": [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.LinearSVC(class_weight=\"balanced\", max_iter=1200000)\n",
    "    grid = GridSearchCV(estimator=svr,\n",
    "                        param_grid=parameters_model,\n",
    "                        n_jobs=8,\n",
    "                        scoring=\"f1_macro\",\n",
    "                        cv=5)\n",
    "    grid.fit(bow_tr, labels_tr)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate_model(bow_val: np.array, labels_val: np.array,\n",
    "                   grid: GridSearchCV) -> np.array:\n",
    "    \"\"\"\n",
    "    Resultados del modelo con el dataset de validacion\n",
    "    \"\"\"\n",
    "    y_pred = grid.predict(bow_val)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels_val,\n",
    "                                                 y_pred,\n",
    "                                                 average=\"macro\",\n",
    "                                                 pos_label=1)\n",
    "    print(confusion_matrix(labels_val, y_pred))\n",
    "    print(metrics.classification_report(labels_val, y_pred))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def normalize(bow: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Normalizacion de la BoW de dos dimensiones\n",
    "    \"\"\"\n",
    "    # Copia de la BoW\n",
    "    bow_norm = bow.copy()\n",
    "    for i in range(bow.shape[0]):\n",
    "        # Inicializacion de la norma\n",
    "        norm = 0\n",
    "        # Calculo de la norma\n",
    "        norm += sum([value**2 for value in bow[i]])\n",
    "        norm = np.sqrt(norm)\n",
    "        # Estandarizacion de la norma\n",
    "        bow_norm[i] = np.array([value / norm for value in bow[i]])\n",
    "    return bow_norm\n",
    "\n",
    "\n",
    "def build_BoE_from_EmoLex(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creacion de una bolsa de emociones a partir de la base de datos de EmoLex\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding='utf-8') as file:\n",
    "        # Inicializacion de los diccionarios\n",
    "        words_dict = dict()\n",
    "        scores = dict()\n",
    "        # Salto del header\n",
    "        for i in range(1):\n",
    "            next(file)\n",
    "        for line in file:\n",
    "            # Lectura de la informacion\n",
    "            data = line.split('\\t')\n",
    "            if data[1] != 'NO TRANSLATION':\n",
    "                # Obtencion del score\n",
    "                score = float(data[3])\n",
    "                word = data[1].lower()\n",
    "                if not word in words_dict:\n",
    "                    words_dict[word] = data[2]\n",
    "                    scores[word] = score\n",
    "                elif score > scores[word]:\n",
    "                    words_dict[word] = data[2]\n",
    "                    scores[word] = score\n",
    "    return words_dict\n",
    "\n",
    "\n",
    "def build_BoE_from_SEL(filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Creacion de una bolsa de emociones a partir de la base de datos de SEL\n",
    "    \"\"\"\n",
    "    # Apertura del archivo\n",
    "    with open(filename, \"r\", encoding='latin-1') as file:\n",
    "        # Inicializacion de los diccionarios\n",
    "        words_emotions = dict()\n",
    "        scores = dict()\n",
    "        # Salto del header\n",
    "        for i in range(1):\n",
    "            next(file)\n",
    "        # Lectura del archivo\n",
    "        for line in file:\n",
    "            # Split de los datos\n",
    "            data = line.split('\\t')\n",
    "            # Score\n",
    "            score = float(data[1])\n",
    "            # Palabra en minusculas\n",
    "            word = data[0].lower()\n",
    "            # Si no se ha guardado se guarda\n",
    "            if not word in words_emotions:\n",
    "                words_emotions[word] = data[2].replace(\"\\n\", \"\")\n",
    "                scores[word] = score\n",
    "            # Si ya existe se comprueba que sea el que contiene mayor score\n",
    "            elif score > scores[word]:\n",
    "                words_emotions[word] = data[2].replace(\"\\n\", \"\")\n",
    "                scores[word] = score\n",
    "    return words_emotions, scores\n",
    "\n",
    "\n",
    "def mask_emotion(tokens: list, word_emotions: dict) -> list:\n",
    "    \"\"\"\n",
    "    Enmascara un tweet a partir de las BoE dadas\n",
    "    \"\"\"\n",
    "    token_copy = tokens.copy()\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word in word_emotions:\n",
    "            token_copy[i] = word_emotions[word]\n",
    "    return token_copy\n",
    "\n",
    "\n",
    "def obtain_corpus_emotions(document: list, word_emotions: dict) -> list:\n",
    "    \"\"\"\n",
    "    Obtiene todo un corpus de emociones enmascarando cada tweet con la bolsa de emociones dada\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Copia del corpus\n",
    "    document_copy = document.copy()\n",
    "    for i, tweet in enumerate(document):\n",
    "        tweet = tokenizer.tokenize(tweet)\n",
    "        emotions = mask_emotion(tweet, word_emotions)\n",
    "        document_copy[i] = \" \".join(emotions)\n",
    "    return document_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"path data\": \"../Data/\",\n",
    "    \"train\": {\n",
    "        \"data\": \"mex_train.txt\",\n",
    "        \"labels\": \"mex_train_labels.txt\"\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"data\": \"mex_val.txt\",\n",
    "        \"labels\": \"mex_val_labels.txt\"\n",
    "    },\n",
    "    \"EmoLex\": \"emolex.txt\",\n",
    "    \"SEL\": \"SEL.txt\",\n",
    "    \"max words\": 5000,\n",
    "}\n",
    "# Definicion de las rutas de cada archivo de datos y validacion\n",
    "path_data_tr = join_path(parameters[\"path data\"], parameters[\"train\"][\"data\"])\n",
    "path_label_tr = join_path(parameters[\"path data\"],\n",
    "                          parameters[\"train\"][\"labels\"])\n",
    "path_data_val = join_path(parameters[\"path data\"],\n",
    "                          parameters[\"validation\"][\"data\"])\n",
    "path_label_val = join_path(parameters[\"path data\"],\n",
    "                           parameters[\"validation\"][\"labels\"])\n",
    "# Lectura de las oraciones y etiquetas de los datos de entrenamiento y validacion\n",
    "data_tr, labels_tr = get_texts_from_file(path_data_tr, path_label_tr)\n",
    "data_val, labels_val = get_texts_from_file(path_data_val, path_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr,\n",
    "                        parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1) Evalue Bow con pesos binarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329  68]\n",
      " [ 47 172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       397\n",
      "           1       0.72      0.79      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.81      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_binary_bow(data_tr,\n",
    "                                 fdist_tr,\n",
    "                                 word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_binary_bow(data_val,\n",
    "                                  fdist_tr,\n",
    "                                  word_index)\n",
    "grid = create_model(binary_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2) Evalue Bow con pesado frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[333  64]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       397\n",
      "           1       0.73      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(freq_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3) Evalue Bow con pesado tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329  68]\n",
      " [ 57 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       397\n",
      "           1       0.70      0.74      0.72       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.78      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "tfidf_bow_tr = build_tfidf_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "tfidf_bow_val = build_tfidf_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(tfidf_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4) Evalue Bow con pesos binarios normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[322  75]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       397\n",
      "           1       0.69      0.78      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.79       616\n",
      "weighted avg       0.81      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW binaria\n",
    "binary_bow_tr_norm = normalize(binary_bow_tr)\n",
    "binary_bow_val_norm = normalize(binary_bow_val)\n",
    "grid = create_model(binary_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5) Evalue Bow con pesado frecuencia normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[321  76]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       397\n",
      "           1       0.69      0.78      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en frecuencias\n",
    "freq_bow_tr_norm = normalize(freq_bow_tr)\n",
    "freq_bow_val_norm = normalize(freq_bow_val)\n",
    "grid = create_model(freq_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6) Evalue Bow con pesado tfidf normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[325  72]\n",
      " [ 50 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       397\n",
      "           1       0.70      0.77      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.80      0.79       616\n",
      "weighted avg       0.81      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en tfidf\n",
    "tfidf_bow_tr_norm = normalize(tfidf_bow_tr)\n",
    "tfidf_bow_val_norm = normalize(tfidf_bow_val)\n",
    "grid = create_model(tfidf_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7) Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" rowspan=\"2\">Método</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">Precision</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">Recall</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">F1-score</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Binario</td>\n",
    "    <td class=\"tg-0lax\">0.88</td>\n",
    "    <td class=\"tg-0lax\">0.72</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.79</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Binario <br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.87</td>\n",
    "    <td class=\"tg-0lax\">0.69</td>\n",
    "    <td class=\"tg-0lax\">0.81</td>\n",
    "    <td class=\"tg-0lax\">0.78</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Frecuencias</td>\n",
    "    <td class=\"tg-0lax\">0.87</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.78</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Frecuencias<br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.87</td>\n",
    "    <td class=\"tg-0lax\">0.69</td>\n",
    "    <td class=\"tg-0lax\">0.81</td>\n",
    "    <td class=\"tg-0lax\">0.78</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">TFIDF</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.70</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.74</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.72</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">TFIDF<br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.87</td>\n",
    "    <td class=\"tg-0lax\">0.70</td>\n",
    "    <td class=\"tg-0lax\">0.80</td>\n",
    "    <td class=\"tg-0lax\">0.80</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8) De las configuraciones anteriores elija la mejor y evalúela con más y menos términos (e.g., 1000 y 7000). Ponga una tabla dónde compare las tres configuraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1000 terminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"max words\"] = 1000\n",
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr,\n",
    "                        parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[331  66]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       397\n",
      "           1       0.72      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.81      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(binary_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[331  66]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       397\n",
      "           1       0.72      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.81      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(freq_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7000 terminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"max words\"] = 7000\n",
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr,\n",
    "                        parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334  63]\n",
      " [ 50 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       397\n",
      "           1       0.73      0.77      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(binary_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334  63]\n",
      " [ 50 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       397\n",
      "           1       0.73      0.77      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(freq_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-c3ow\" rowspan=\"2\">Método</th>\n",
    "    <th class=\"tg-0pky\" rowspan=\"2\">Términos</th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"2\">Precision</th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"2\">Recall</th>\n",
    "    <th class=\"tg-c3ow\" colspan=\"2\">F1-score</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\" rowspan=\"3\">Binario</td>\n",
    "    <td class=\"tg-0pky\">1000</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.72</td>\n",
    "    <td class=\"tg-0pky\">0.83</td>\n",
    "    <td class=\"tg-0pky\">0.78</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">5000</td>\n",
    "    <td class=\"tg-0pky\">0.88</td>\n",
    "    <td class=\"tg-0pky\">0.72</td>\n",
    "    <td class=\"tg-0pky\">0.83</td>\n",
    "    <td class=\"tg-0pky\">0.79</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">7000</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.73</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.77</td>\n",
    "    <td class=\"tg-0pky\">0.86</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\" rowspan=\"3\">Frecuencias</td>\n",
    "    <td class=\"tg-0pky\">1000</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.72</td>\n",
    "    <td class=\"tg-0pky\">0.83</td>\n",
    "    <td class=\"tg-0pky\">0.78</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">5000</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.73</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.78</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">7000</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.73</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.77</td>\n",
    "    <td class=\"tg-0pky\">0.86</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.9) Utilice el recurso léxico del Consejo Nacional de Investigación de Canadá llamado \"EmoLex\" (https://www.saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) para construir una \"Bolsa de Emociones\" de los Tweets de agresividad (Debe usar EmoLex en Español). Para esto, una estrategia sencilla sería enmascarar cada palabra con su emoción, y después construir la Bolsa de Emociones (BoE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"max words\"] = 5000\n",
    "emolex_path = join_path(parameters[\"path data\"], parameters[\"EmoLex\"])\n",
    "words_emotions = build_BoE_from_EmoLex(emolex_path)\n",
    "data_tr_emotions = obtain_corpus_emotions(data_tr, words_emotions)\n",
    "data_val_emotions = obtain_corpus_emotions(data_val, words_emotions)\n",
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr_emotions, parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.10) Evalúa tú BoE clasificando con SVM. Ponga una tabla comparativa a modo de resumen con los tres pesados, normalize cada uno si lo cree conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[330  67]\n",
      " [ 56 163]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       397\n",
      "           1       0.71      0.74      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_binary_bow(data_tr_emotions, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_binary_bow(data_val_emotions, fdist_tr, word_index)\n",
    "grid = create_model(binary_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[333  64]\n",
      " [ 59 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84       397\n",
      "           1       0.71      0.73      0.72       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.78      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow(data_tr_emotions, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow(data_val_emotions, fdist_tr, word_index)\n",
    "grid = create_model(freq_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[328  69]\n",
      " [ 59 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       397\n",
      "           1       0.70      0.73      0.71       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.78      0.78       616\n",
      "weighted avg       0.79      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "tfidf_bow_tr = build_tfidf_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "tfidf_bow_val = build_tfidf_bow(data_val, fdist_tr, word_index)\n",
    "grid = create_model(tfidf_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binario normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[323  74]\n",
      " [ 57 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       397\n",
      "           1       0.69      0.74      0.71       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.78      0.77       616\n",
      "weighted avg       0.79      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW binaria\n",
    "binary_bow_tr_norm = normalize(binary_bow_tr)\n",
    "binary_bow_val_norm = normalize(binary_bow_val)\n",
    "grid = create_model(binary_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[320  77]\n",
      " [ 52 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       397\n",
      "           1       0.68      0.76      0.72       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.78      0.78       616\n",
      "weighted avg       0.80      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en frecuencias\n",
    "freq_bow_tr_norm = normalize(freq_bow_tr)\n",
    "freq_bow_val_norm = normalize(freq_bow_val)\n",
    "grid = create_model(freq_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[328  69]\n",
      " [ 52 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84       397\n",
      "           1       0.71      0.76      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.79      0.79      0.79       616\n",
      "weighted avg       0.81      0.80      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en tfidf\n",
    "tfidf_bow_tr_norm = normalize(tfidf_bow_tr)\n",
    "tfidf_bow_val_norm = normalize(tfidf_bow_val)\n",
    "grid = create_model(tfidf_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cuadro comparativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\" rowspan=\"2\">Método</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">Precision</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">Recall</th>\n",
    "    <th class=\"tg-0lax\" colspan=\"2\">F1-score</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "    <th class=\"tg-0lax\">0</th>\n",
    "    <th class=\"tg-0lax\">1</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Binario</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.71</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.74</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Binario <br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.69</td>\n",
    "    <td class=\"tg-0lax\">0.81</td>\n",
    "    <td class=\"tg-0lax\">0.74</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.71</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Frecuencias</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.71</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.72</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Frecuencias<br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.86</td>\n",
    "    <td class=\"tg-0lax\">0.68</td>\n",
    "    <td class=\"tg-0lax\">0.81</td>\n",
    "    <td class=\"tg-0lax\">0.76</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.72</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">TFIDF</td>\n",
    "    <td class=\"tg-0lax\">0.85</td>\n",
    "    <td class=\"tg-0lax\">0.70</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.71</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">TFIDF<br>normalizado</td>\n",
    "    <td class=\"tg-0lax\">0.86</td>\n",
    "    <td class=\"tg-0lax\">0.71</td>\n",
    "    <td class=\"tg-0lax\">0.83</td>\n",
    "    <td class=\"tg-0lax\">0.76</td>\n",
    "    <td class=\"tg-0lax\">0.84</td>\n",
    "    <td class=\"tg-0lax\">0.73</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0) Utilice el recurso léxico llamado \"Spanish Emotion Lexicon (SEL)\" del Dr. Grigori Sidorov, profesor del Centro de Investigación en Computación (CIC) del Instituto Politécnico Nacional (http://www.cic.ipn.mx/∼sidorov/), para enmascarar cada palabra con su emo- ción, y después construir la Bolsa de Emociones con algún pesado (e.g., binario, tf, tfidf). Proponga alguna estrategia para incorporar el \"valor\" del \"Probability Factor of Affective use\" en su representación vectorial del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_path = join_path(parameters[\"path data\"], parameters[\"SEL\"])\n",
    "words_emotions,scores = build_BoE_from_SEL(sel_path)\n",
    "data_tr_emotions = obtain_corpus_emotions(data_tr, words_emotions)\n",
    "data_val_emotions = obtain_corpus_emotions(data_val, words_emotions)\n",
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr_emotions, parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[330  67]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       397\n",
      "           1       0.72      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.79      0.80      0.80       616\n",
      "weighted avg       0.82      0.81      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_binary_bow_with_probabilities(data_tr_emotions, fdist_tr,\n",
    "                                                    word_index, scores)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_binary_bow_with_probabilities(data_val_emotions,\n",
    "                                                     fdist_tr, word_index,\n",
    "                                                     scores)\n",
    "grid = create_model(binary_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[332  65]\n",
      " [ 50 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       397\n",
      "           1       0.72      0.77      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.80      0.80       616\n",
      "weighted avg       0.82      0.81      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow_with_probabilities(data_tr_emotions,\n",
    "                                                     fdist_tr, word_index,\n",
    "                                                     scores)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow_with_probabilities(data_val_emotions,\n",
    "                                                      fdist_tr, word_index,\n",
    "                                                      scores)\n",
    "grid = create_model(freq_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329  68]\n",
      " [ 59 160]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       397\n",
      "           1       0.70      0.73      0.72       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.78      0.78       616\n",
      "weighted avg       0.80      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "tfidf_bow_tr = build_tfidf_bow_with_probabilities(data_tr, fdist_tr,\n",
    "                                                  word_index, scores)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "tfidf_bow_val = build_tfidf_bow_with_probabilities(data_val, fdist_tr,\n",
    "                                                   word_index, scores)\n",
    "grid = create_model(tfidf_bow_tr, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val, labels_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[326  71]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       397\n",
      "           1       0.71      0.78      0.74       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.79      0.80      0.79       616\n",
      "weighted avg       0.81      0.81      0.81       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW binaria\n",
    "binary_bow_tr_norm = normalize(binary_bow_tr)\n",
    "binary_bow_val_norm = normalize(binary_bow_val)\n",
    "grid = create_model(binary_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[324  73]\n",
      " [ 52 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       397\n",
      "           1       0.70      0.76      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en frecuencias\n",
    "freq_bow_tr_norm = normalize(freq_bow_tr)\n",
    "freq_bow_val_norm = normalize(freq_bow_val)\n",
    "grid = create_model(freq_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[320  77]\n",
      " [ 46 173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       397\n",
      "           1       0.69      0.79      0.74       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.80      0.79       616\n",
      "weighted avg       0.81      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalizacion de la BoW basada en tfidf\n",
    "tfidf_bow_tr_norm = normalize(tfidf_bow_tr)\n",
    "tfidf_bow_val_norm = normalize(tfidf_bow_val)\n",
    "grid = create_model(tfidf_bow_tr_norm, labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val_norm, labels_val, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\" rowspan=\"2\">Método</th>\n",
    "    <th class=\"tg-0pky\" colspan=\"2\">Precision</th>\n",
    "    <th class=\"tg-0pky\" colspan=\"2\">Recall</th>\n",
    "    <th class=\"tg-0pky\" colspan=\"2\">F1-score</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "    <th class=\"tg-0pky\">0</th>\n",
    "    <th class=\"tg-0pky\">1</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Binario</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.72</td>\n",
    "    <td class=\"tg-0pky\">0.83</td>\n",
    "    <td class=\"tg-0pky\">0.78</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.74</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Binario <br>normalizado</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.71</td>\n",
    "    <td class=\"tg-0pky\">0.82</td>\n",
    "    <td class=\"tg-0pky\">0.78</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.74</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Frecuencias</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.72</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.77</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Frecuencias<br>normalizado</td>\n",
    "    <td class=\"tg-0pky\">0.86</td>\n",
    "    <td class=\"tg-0pky\">0.70</td>\n",
    "    <td class=\"tg-0pky\">0.82</td>\n",
    "    <td class=\"tg-0pky\">0.76</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.73</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">TFIDF</td>\n",
    "    <td class=\"tg-0pky\">0.85</td>\n",
    "    <td class=\"tg-0pky\">0.70</td>\n",
    "    <td class=\"tg-0pky\">0.83</td>\n",
    "    <td class=\"tg-0pky\">0.73</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.72<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">TFIDF<br>normalizado</td>\n",
    "    <td class=\"tg-0pky\">0.87</td>\n",
    "    <td class=\"tg-0pky\">0.69</td>\n",
    "    <td class=\"tg-0pky\">0.81</td>\n",
    "    <td class=\"tg-0pky\">0.79</td>\n",
    "    <td class=\"tg-0pky\">0.84</td>\n",
    "    <td class=\"tg-0pky\">0.74</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
