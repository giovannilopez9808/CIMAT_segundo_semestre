{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giovanni Gamaliel López Padilla\n",
    "#### Procesamiento de lenguaje natural\n",
    "#### Tarea 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_path(path: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Une la direccion de un archivo con su nombre\n",
    "    \"\"\"\n",
    "    return \"{}{}\".format(path, filename)\n",
    "\n",
    "\n",
    "def get_texts_from_file(path_data: str, path_labels: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Obtiene una lista de oraciones a partir de un texto con sus respectivas etiquetas\n",
    "    \"\"\"\n",
    "    # Inicilizacion de las listas\n",
    "    text = []\n",
    "    labels = []\n",
    "    # Apertura de los archivos\n",
    "    with open(path_data, \"r\") as f_data, open(path_labels, \"r\") as f_labels:\n",
    "        # Recoleccion de las oraciones\n",
    "        for tweet in f_data:\n",
    "            text += [tweet]\n",
    "        # Recoleccion de las etiquedas\n",
    "        for label in f_labels:\n",
    "            labels += [label]\n",
    "    # Etiquedas a enteros\n",
    "    labels = list(map(int, labels))\n",
    "    return text, labels\n",
    "\n",
    "\n",
    "def sort_freqdist(fdist: nltk.FreqDist) -> list:\n",
    "    \"\"\"\n",
    "    Ordena la lista de distribucion de frecuencias de palabras de mayor frecuencia a menor\n",
    "    \"\"\"\n",
    "    aux = [(fdist[key], key) for key in fdist]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "\n",
    "\n",
    "def split_data(data: list, max_words: int) -> list:\n",
    "    \"\"\"\n",
    "    Realiza la separacion de elementos en una lista dado el numero de elementos que se quieren conservar\n",
    "    \"\"\"\n",
    "    return data[:max_words]\n",
    "\n",
    "\n",
    "def obtain_fdist(data: list, max_words: int) -> list:\n",
    "    \"\"\"\n",
    "    Obtiene la lista de una distribucion de frecuencias de palabras ordenada de mayor a menor a partir de una lista de oraciones\n",
    "    \"\"\"\n",
    "    # Inicializacion del Tokenizador\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Inicializacion de la lista que guardara los tokens\n",
    "    corpus_palabras = []\n",
    "    for tweet in data:\n",
    "        # Creacion y guardado de los tokens\n",
    "        corpus_palabras += tokenizer.tokenize(tweet)\n",
    "    # Creacion de la distribucion de frecuencias\n",
    "    fdist = nltk.FreqDist(corpus_palabras)\n",
    "    fdist = sort_freqdist(fdist)\n",
    "    fdist = split_data(fdist, max_words)\n",
    "    return fdist\n",
    "\n",
    "\n",
    "def create_dictonary_of_index(fdist: list) -> dict:\n",
    "    \"\"\"\n",
    "    Crea un diccionario con la posición de mayor a menor frecuencia de cada palabra. La llave es la palabra a consultar\n",
    "    \"\"\"\n",
    "    # Inicializacion del diccionario\n",
    "    index = dict()\n",
    "    # Inicializacion de la posicion\n",
    "    i = 0\n",
    "    for weight, word in fdist:\n",
    "        index[word] = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "\n",
    "def build_binary_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos binarios\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = 1\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_frecuency_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    docs = 0\n",
    "    for tweet in data:\n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                bow[docs, index[word]] = tweet.count(word)\n",
    "        docs += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def create_empty_dictionary_of_words_and_documents(words: dict,\n",
    "                                                   data: list) -> dict:\n",
    "    # Inicializacion del diccionario\n",
    "    freq_word_per_document = dict()\n",
    "    word_count = dict()\n",
    "    for i, tweet in enumerate(data):\n",
    "        word_count[i] = 0\n",
    "    for word in words:\n",
    "        freq_word_per_document[word] = word_count\n",
    "    return freq_word_per_document\n",
    "\n",
    "def build_tfidf_bow(data: list, fdist: list, index: dict) -> np.array:\n",
    "    \"\"\"\n",
    "    Creacion de la BoW usando pesos basado en frecuencias\n",
    "    \"\"\"\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Inicilizacion del bow\n",
    "    bow = np.zeros((len(data), len(fdist)), dtype=float)\n",
    "    # Total de oraciones\n",
    "    n = len(data)\n",
    "    # Inicializacion del diccionario que contiene la repeticion de cada palabra\n",
    "    idf_per_word_and_document = create_empty_dictionary_of_words_and_documents(\n",
    "        index.keys(), data)\n",
    "    for docs, tweet in enumerate(data):\n",
    "        # Frecuencias \n",
    "        fdist_data = nltk.FreqDist(tokenizer.tokenize(tweet))\n",
    "        for word in fdist_data:\n",
    "            if word in index.keys():\n",
    "                # Descriptiva\n",
    "                tf = tweet.count(word)\n",
    "                idf_per_word_and_document[word][docs] += 1\n",
    "                bow[docs, index[word]] = tf\n",
    "    # Discriminativa\n",
    "    for word in index.keys():\n",
    "        idf = sum(idf_per_word_and_document[word].values())\n",
    "        idf = np.log(n / idf)\n",
    "        for docs, tweet in enumerate(data):\n",
    "            bow[docs, index[word]] = bow[docs, index[word]] * idf\n",
    "    return bow\n",
    "\n",
    "\n",
    "def create_model(bow_tr: np.array, labels_tr: np.array) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Creacion del modelo para realizar el aprendizaje\n",
    "    \"\"\"\n",
    "    parameters_model = {\"C\": [0.05, 0.12, 0.25, 0.5, 1, 2, 4]}\n",
    "    svr = svm.LinearSVC(class_weight=\"balanced\", max_iter=1200000)\n",
    "    grid = GridSearchCV(estimator=svr,\n",
    "                        param_grid=parameters_model,\n",
    "                        n_jobs=8,\n",
    "                        scoring=\"f1_macro\",\n",
    "                        cv=5)\n",
    "    grid.fit(bow_tr, labels_tr)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate_model(bow_val: np.array, labels_val: np.array,\n",
    "                   grid: GridSearchCV) -> np.array:\n",
    "    \"\"\"\n",
    "    Resultados del modelo con el dataset de validacion\n",
    "    \"\"\"\n",
    "    y_pred = grid.predict(bow_val)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels_val,\n",
    "                                                 y_pred,\n",
    "                                                 average=\"macro\",\n",
    "                                                 pos_label=1)\n",
    "    print(confusion_matrix(labels_val, y_pred))\n",
    "    print(metrics.classification_report(labels_val, y_pred))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"path data\": \"../Data/\",\n",
    "              \"train\": {\"data\": \"mex_train.txt\",\n",
    "                        \"labels\": \"mex_train_labels.txt\"},\n",
    "              \"validation\": {\"data\": \"mex_val.txt\",\n",
    "                             \"labels\": \"mex_val_labels.txt\"},\n",
    "              \"max words\": 5000,\n",
    "              }\n",
    "# Definicion de las rutas de cada archivo de datos y validacion\n",
    "path_data_tr = join_path(parameters[\"path data\"],\n",
    "                         parameters[\"train\"][\"data\"])\n",
    "path_label_tr = join_path(parameters[\"path data\"],\n",
    "                          parameters[\"train\"][\"labels\"])\n",
    "path_data_val = join_path(parameters[\"path data\"],\n",
    "                          parameters[\"validation\"][\"data\"])\n",
    "path_label_val = join_path(parameters[\"path data\"],\n",
    "                           parameters[\"validation\"][\"labels\"])\n",
    "# Lectura de las oraciones y etiquetas de los datos de entrenamiento y validacion\n",
    "data_tr, labels_tr = get_texts_from_file(path_data_tr,\n",
    "                                         path_label_tr)\n",
    "data_val, labels_val = get_texts_from_file(path_data_val,\n",
    "                                           path_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la distribucion de palabras ordenadas de mayor a menor con un maximo de 5000 palabras\n",
    "fdist_tr = obtain_fdist(data_tr,\n",
    "                       parameters[\"max words\"])\n",
    "# Creacion del diccionario con la posicion en la distribucion de palabras\n",
    "word_index = create_dictonary_of_index(fdist_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "binary_bow_tr = build_binary_bow(data_tr,\n",
    "                          fdist_tr,\n",
    "                          word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "binary_bow_val = build_binary_bow(data_val,\n",
    "                          fdist_tr,\n",
    "                          word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329  68]\n",
      " [ 47 172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       397\n",
      "           1       0.72      0.79      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.81      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid=create_model(binary_bow_tr,labels_tr)\n",
    "y_pred = evaluate_model(binary_bow_val,labels_val,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "freq_bow_tr = build_frecuency_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "freq_bow_val = build_frecuency_bow(data_val, fdist_tr, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[333  64]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85       397\n",
      "           1       0.73      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid=create_model(freq_bow_tr,labels_tr)\n",
    "y_pred = evaluate_model(freq_bow_val,labels_val,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de la BoW para los datos de entrenamiento usando pesos binarios\n",
    "tfidf_bow_tr = build_tfidf_bow(data_tr, fdist_tr, word_index)\n",
    "# Creacion de la BoW para los datos de validacion usando pesos binarios\n",
    "tfidf_bow_val = build_tfidf_bow(data_val, fdist_tr, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[327  70]\n",
      " [ 62 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       397\n",
      "           1       0.69      0.72      0.70       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.77      0.77       616\n",
      "weighted avg       0.79      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid=create_model(tfidf_bow_tr,labels_tr)\n",
    "y_pred = evaluate_model(tfidf_bow_val,labels_val,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
