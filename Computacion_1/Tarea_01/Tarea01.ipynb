{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giovanni Gamaliel López Padilla - Tarea 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import unidecode\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> [str]:\n",
    "    \"\"\"\n",
    "    Limpia un string de los caracteres \\n, \\t ,\\xa0 y espacios en blanco.\n",
    "    Input:\n",
    "     String -> Texto plano\n",
    "    \n",
    "    Output:\n",
    "     Lista de palabras en el texto sin \\n, \\t, \\xa0 y espacios en blanco\n",
    "    \"\"\"\n",
    "    text_out = text.split()\n",
    "    return text_out\n",
    "\n",
    "\n",
    "def lexical_diversity(text: [str]) -> float:\n",
    "    \"\"\"\n",
    "    Mide la riqueza lexica de un documento\n",
    "    Input: \n",
    "        String -> Texto plano\n",
    "        \n",
    "    Output:\n",
    "        Float -> riqueza lexica del documento\n",
    "    \"\"\"\n",
    "    return len(text) / len(set(text))\n",
    "\n",
    "\n",
    "def to_unidecode(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convierte el texto a unidecode\n",
    "    Input:\n",
    "        String -> Texto plano\n",
    "        \n",
    "    Output:\n",
    "        String -> Texto unidecode\n",
    "    \"\"\"\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "\n",
    "def to_lowercase(text: str, use_unidecode: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Convierte el texto a minusculas usando caracteres unidecode o no\n",
    "    Input:\n",
    "        String -> Texto plano\n",
    "        Bool -> Decision si hace uso de los caracteres unidecode\n",
    "        \n",
    "    Output:\n",
    "        String -> Texto en minusculas del texto dado\n",
    "    \"\"\"\n",
    "    if use_unidecode:\n",
    "        return to_unidecode(text.lower())\n",
    "    else:\n",
    "        return text.lower()\n",
    "\n",
    "\n",
    "def exercise_2_point_1_2_3_4(text: str, clean: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Ejecuta los puntos 1, 2 ,3 y 4 del punto 2 de la tarea\n",
    "    \"\"\"\n",
    "    # Limpia el texto de caracteres \\n, \\t y \\xa0\n",
    "    if clean:\n",
    "        text_lines = clean_text(text)\n",
    "    else:\n",
    "        text_lines = text\n",
    "    print(\"La cantidad de palabras es de {}\".format(len(text_lines)))\n",
    "    # Obtiene las palabras unicas en el texto\n",
    "    text_set = set(text_lines)\n",
    "    print(\"La longitud del vocabulario es de {}\".format(len(text_set)))\n",
    "    # Obtiene la riqueza lexica de las conferencias\n",
    "    print(\"La riqueza lexica de las conferencias es de {}\".format(\n",
    "        lexical_diversity(text_lines)))\n",
    "\n",
    "\n",
    "def obtain_only_words_with_regex_nltk(text: str) -> [str]:\n",
    "    \"\"\"\n",
    "    Obtiene las palabras usando regexp de nltk de mayusculas y minusculas\n",
    "    Input:\n",
    "        String -> Texto plano\n",
    "        \n",
    "    Output:\n",
    "        List of strings -> Lista de palabras\n",
    "    \"\"\"\n",
    "    # Definicion del tokenizador\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\n",
    "    # Lista de palabras\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def obtain_only_words_with_tweet_tokenizer(text: str) -> [str]:\n",
    "    \"\"\"\n",
    "    Obtiene las palabras usando TweetTokenizer de nltk de un texto minusculas\n",
    "    Input:\n",
    "        String -> Texto plano\n",
    "        \n",
    "    Output:\n",
    "        List of strings -> Lista de palabras\n",
    "    \"\"\"\n",
    "    # Crea una referencia para la clase TweetTokenizer\n",
    "    tk = TweetTokenizer()\n",
    "    # Creacion de los tokens\n",
    "    return tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Vistazo a los datos\n",
    "+ 1) Cargue todas las conferencias en un string y aplique la función split para generar tokens\n",
    "fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"path dataset\": \"dataset/\"}\n",
    "# Lectura de los nombres de los archivos\n",
    "files = sorted(os.listdir(parameters[\"path dataset\"]))\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    # Direccion y nombre del archivo\n",
    "    filename = \"{}{}\".format(parameters[\"path dataset\"], file)\n",
    "    # Apertura del archivo\n",
    "    file_text = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    # Concadenacion del texto\n",
    "    text += file_text.read()\n",
    "    # Cierre del texto\n",
    "    file_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 2. Contar la cantidad de palabras en todas las conferencias.\n",
    "+ 3. Extraer el vocabulario y mostrar su longitud.\n",
    "+ 4. Mida la riqueza del vocabulario de alguna forma en todos los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de palabras es de 8039596\n",
      "La longitud del vocabulario es de 170688\n",
      "La riqueza lexica de las conferencias es de 47.10112017247844\n"
     ]
    }
   ],
   "source": [
    "exercise_2_point_1_2_3_4(text)\n",
    "text_unidecode_lower = to_lowercase(text, True)\n",
    "exercise_2_point_1_2_3_4(text_unidecode_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 5. Haga lo mismo que los 4 puntos anteriores pero con todo el texto en minúsculas. Vea\n",
    "las diferencias y comente brevemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de palabras es de 8039596\n",
      "La longitud del vocabulario es de 157166\n",
      "La riqueza lexica de las conferencias es de 51.15353193438784\n"
     ]
    }
   ],
   "source": [
    "text_lower = to_lowercase(text)\n",
    "exercise_2_point_1_2_3_4(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 6. Haga lo mismo que los puntos 1, 2 y 3 usando el tokenizador RegExp de NLTK, con\n",
    "una expresión regular que trate de sacar solo tokens que pueden contener solo letras\n",
    "mayúsculas y minúsculas. Después cargue los tokens en un objeto Text de NLTK en\n",
    "lugar de una lista de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de palabras es de 7944645\n",
      "La longitud del vocabulario es de 74555\n",
      "La riqueza lexica de las conferencias es de 106.56086110924821\n"
     ]
    }
   ],
   "source": [
    "# Convierte el texto en unidecode\n",
    "text_unidecode = to_unidecode(text)\n",
    "text_nltk = obtain_only_words_with_regex_nltk(text_unidecode)\n",
    "exercise_2_point_1_2_3_4(text_nltk, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 7) Haga todo el texto minúsculas. Haga los mismo que los puntos 1, 2, y 3 usando el\n",
    "tokenizador TweetTokenizer. Haga el resto de esta tarea asumiendo haber hecho este\n",
    "punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de palabras es de 9283310\n",
      "La longitud del vocabulario es de 70357\n",
      "La riqueza lexica de las conferencias es de 131.94579075287461\n"
     ]
    }
   ],
   "source": [
    "# Convierte el texto en unidecode y en minusculas\n",
    "text_unidecode_lower = to_lowercase(text,True)\n",
    "text_nltk = obtain_only_words_with_tweet_tokenizer(text_unidecode_lower)\n",
    "exercise_2_point_1_2_3_4(text_nltk, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
