\section{Métodos}

\subsection{Método de descenso del gradiente}

La dirección que se tomada con el método del descenso del gradiente es el negativo del gradiente, esto es que la ecuación de cada paso en el método esta descrita en la ecuación \ref{eq:equation_gradient}.

\begin{equation}
    x_{k+1} = x_{k} - \alpha_k \nabla f_k \label{eq:equation_gradient}
\end{equation}

donde se dice que $\alpha_k$ es fijo si para todo $k$ el valor de $\alpha$ no cambia. Por lo que se convierte en un parametro del método.

\subsection{Método de Newton}

La direccion que es tomada en el método de Newton es obtenida al resolver el sistema matricial de la ecuación \ref{eq:direction_newton}.

\begin{equation}
    \nabla^2 f_k d_k = \nabla f_k \label{eq:direction_newton}
\end{equation}

donde $-d_k$ es la dirección de descenso que se tomaria siguiendo la ecuación \ref{eq:equation_newton}.

\begin{equation}
    x_{k+1} = x_{k} - \alpha_k d_k \label{eq:equation_newton}
\end{equation}

donde $\alpha_k$ sería obtenido en cada iteración siguiendo el algoritmo \ref{alg:alpha_algorithm}.

\subsection{Condiciones de paro \label{sec:stop}}

\subsubsection{Por posición}

Una de las condiciones tomadas para parar los métodos de Newton y descenso del gradiente es comprobando si la norma de la diferencia entre la posición $k$ y $k+1$ es menor igual a una tolerancia establecida. Esta condición es descrita en la ecuación \ref{eq:position_condition}.

\begin{equation}
    ||x_{k+1}-x_{k}|| \leq \tau \label{eq:position_condition}
\end{equation}

\subsubsection{Por función}

Otra manera de comprobar si ya no existe algún movimiendo notable en el método es revisando el valor absoluto de la diferencia de la función evaluada en $x_{k+1}$ y $x_k$. La condición esta descrita en la ecuación \ref{eq:function_condition}.

\begin{equation}
    |f(x_{k+1})-f(x_k)| \leq \tau \label{eq:function_condition}
\end{equation}

\subsubsection{Por gradiente}

Como estamos en la búsqueda de puntos estacionarios de $f$, es razonable que una condición de paro este relacionada al gradiente de la function. La condición usando al gradiente de la función esta descrita en la ecuación \ref{eq:gradient_condition}.

\begin{equation}
    ||\nabla f (x_{k+1})|| \leq \tau \label{eq:gradient_condition}
\end{equation}


\subsection{Algoritmos implementados}



\subsubsection{Método del descenso del gradiente}

El algoritmo \ref{alg:gradient} se implemento para el método del descenso del gradiente. El algoritmo hace uso de las condiciones de paro descritas en \ref{sec:stop}.

\begin{algorithm}
    \caption{Método del descenso de gradiente con $\alpha$ fija.\label{alg:gradient} }
    \KwIn{$x_0\;,f\;,\nabla f\;,\alpha$}
    \KwOut{$x_{i+1}\,\nabla f(x_{i+1})$}
    $x_j \gets x_0$\\
    \Repeat{
    $x_i \gets x_j$ \\
    $d_i \gets -\nabla f(x_i)$\\
    $x_{i+1} \gets  x_i + \alpha d_i$\\
    check stop conditions$(f,x_i,x_{i+1},\nabla f(x_{i+1}))$
    }
    \Return{$x_{i+1},\nabla f(x_{i+1})$}
\end{algorithm}

\subsubsection{Método de Newton}

El algoritmo \ref{alg:newton} se implemento para el método de Newton. El mismo hace de las condiciones de Wolfe descritas en la sección \ref{sec:wolfe} y las condiciones de paro descritas en \ref{sec:stop}.

\begin{algorithm}
    \caption{Algoritmo de Newton usando las condiciones de Wolfe.\label{alg:newton}}
    \KwIn{$x_0\;,f\;,\nabla f$}
    \KwOut{$x_{i+1}\,\nabla f(x_{i+1})$}
    $x_j \gets x_0$\\
    \Repeat{
    $x_i \gets x_j$ \\
    $d_i \gets \text{solve(}\nabla^2f(x_i) d_i = \nabla f(x_i)\text{)}$\\
    $\alpha_i \gets$ Wolfe conditions()\\
    $x_{i+1} \gets  x_i - \alpha_i d_i$\\
    check stop conditions$(f,x_i,x_{i+1},\nabla f(x_{i+1}))$
    }
    \Return{$x_{i+1},\nabla f(x_{i+1})$}
\end{algorithm}

\subsection{Función de Rosembrock}

La función de Rosembrock se define en la ecuación \ref{eq:rosembrock}.

\begin{equation}
    f(x) = \sum_{i=1}^{n-1}  100(x_{i+1}-x_{i}^2)^2 +(1-x_i)
    \label{eq:rosembrock}
\end{equation}

donde $x\in \Real^n$

Con la función de Rosembrock definida, se tiene que su gradiente es calculado con en la ecuación \ref{eq:rosembrock_gradient}.

\begin{equation}
    \nabla f (x) =\begin{cases}
        -400x_i(x_{i+1}-x_{i}^2)                              & \text{para } i=1   \\[0.25cm]
        200(x_{i}-x_{i-1}^2-400x_i(x_{i+1}-x_{i}^2) -2(1-x_i) & \text{para } 1<i<n \\[0.25cm]
        200(x_{i}-x_{i-1}                                     & \text{para } i=n
    \end{cases} \label{eq:rosembrock_gradient}
\end{equation}

Con la función de Rosembrock definda, se tiene que su hessiano es calculado con la ecuación \ref{eq:rosembrock_hessian}.

\begin{equation}
    \nabla^2 f(x)  = \begin{cases}
        \ddpartial{f}{x_i} = -200                    & \text{para } x=1         \\[0.25cm]
        \ddpartial{f}{x_i} =1200x_i^2-400x_{i+1}+202 & \text{para } 1<i<n       \\[0.25cm]
        \dpartiald{f}{x_i}{x_{i+1}}  = -400x_i       & \text{para } 0\leq i < n \\[0.25cm]
        \ddpartial{f}{x_i} = 200                     & \text{para } i=n
    \end{cases}
    \label{eq:rosembrock_hessian}
\end{equation}

El vector inicial predefinido que se usará con la función de Rosembrock esta definido en la ecuación \ref{eq:rosembrock_vector}.

\begin{equation}
    x=\begin{bmatrix}
        -1.2 & 1 & 1 & \dots & -1.2 & 1
    \end{bmatrix}^T \label{eq:rosembrock_vector}
\end{equation}

\subsection{Función de Wood}

La función de Wood se define en la ecuación \ref{eq:wood}.

\begin{equation}
    f(x) = 100(x_1^2-x_2)^2+(x_1-1)^2+(x_3-1)^2+90(x_3-x_4)^2 +10.1((x_2-1)^2+(x_4-1)^2)+19.8(x_2-1)(x_4-1) \label{eq:wood}
\end{equation}

Donde $x\in \Real^4$.

Con la función de Wood definida, podemos obtener le gradiente de la función de Wood. El resultado del gradiente de la función de Wood se encuentra en la ecuación \ref{eq:wood_gradient}.

\begin{equation}
    \nabla f(x) = \begin{cases}
        \dpartial{f}{x_1}=400(x_1^2-x_2)x_1 +2(x_1-1)             \\[0.25cm]
        \dpartial{f}{x_2}=-200(x_1^2-x_2)+20.2(x_2-1)+19.8(x_4-1) \\[0.25cm]
        \dpartial{f}{x_3}=2(x_3-1)+360(x_3^2-x_4)x_3              \\[0.25cm]
        \dpartial{f}{x_4}=-180(x_3^2-x_4)+20.2(x_4-1)+19.8(x_2-1)
    \end{cases} \label{eq:wood_gradient}
\end{equation}

De igual forma, se puede obtener el hessiano de la función de Wood. El resultado del Hessiano se encuentra en la ecuación \ref{eq:wood_hessian}.

\begin{equation}
    \nabla^2 f(x) = \begin{cases}
        \ddpartial{f}{x_1} = 400(x_1^2-x_2)+800x_1^2+2             \\[0.25cm]
        \dpartiald{f}{x_1}{x_2} =\dpartiald{f}{x_2}{x_1} = -400x_1 \\[0.25cm]
        \ddpartial{f}{x_2} = 220.2                                 \\[0.25cm]
        \dpartiald{f}{x_4}{x_2} = \dpartiald{f}{x_2}{x_4} = 19.8   \\[0.25cm]
        \ddpartial{f}{x_3} = 720x_3^2+360(x_3^2-x_4)+2             \\[0.25cm]
        \dpartiald{f}{x_4}{x_3}=\dpartiald{f}{x_3}{x_4} = -360x_3  \\[0.25cm]
        \ddpartial{f}{x_4} = 200.2
    \end{cases} \label{eq:wood_hessian}
\end{equation}

El vector inicial predefinido para el problema de la función de Wood se encuentra definido en la ecuación \ref{eq:wood_vector}.

\begin{equation}
    x = \begin{bmatrix}
        -3 & -1 & -3 & -1
    \end{bmatrix}^T \label{eq:wood_vector}
\end{equation}

\subsection{Función de suavizado}


La función de suavizado se define en la ecuación \ref{eq:lambda}.

\begin{equation}
    f(x) = \sum_{i=1}^n (x_i-y_i)^2 + \lambda \sum_{i=1}^{n-1} (x_{i+1}-x_i)^2 \label{eq:lambda}
\end{equation}


donde $x,y\in \Real^n$, $\lambda >0$. El vector $y$ se propuso obtenerse conforme a la ecuación .

\begin{equation}
    y = t_i^2 + \eta \qquad t_i = \frac{2i}{n-1} \qquad i=0,1,\dots,n-1 \qquad \eta \in \mathcal{N}(0,\sigma)
\end{equation}

Calculando el gradiente de la función de suavizado se obtiene lo mostrado en la ecuación \ref{eq:lambda_gradient}.

\begin{equation}
    \nabla f(x) = \begin{cases}
        \dpartial{f}{x_1} = 2(x_1-y_1)-2\lambda (x_1-x_0)                                 \\[0.25cm]
        \dpartial{f}{x_i} = 2(x_i-y_i)+2\lambda(2x_i-x_{i+1}-x{i-1}) \;\text{para } 1<i<n \\[0.25cm]
        \dpartial{f}{x_n} = 2(x_n-y_n) + 2\lambda(x_n-x_{n-1})
    \end{cases} \label{eq:lambda_gradient}
\end{equation}

Para la prueba del

\subsection{Vectores iniciales aleatorios}

Para realizar una evaluación a los métodos se implemento la creación de vectores aleatorios. Cada elemento del vector sigue una distribución normal con media 0 y sigma 0.5 $(\mathcal{N}(0,0.5))$.

\begin{equation}
    x = \{ x_i \in \mathcal{N}(0,0.5)\} \label{eq:random_vector}
\end{equation}