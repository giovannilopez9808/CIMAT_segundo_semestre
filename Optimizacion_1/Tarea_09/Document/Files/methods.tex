\section{Métodos}

\subsection{Gradiente Conjugado}

De manera general, el algortimo para el método de Gradiente Conjugado se encuentra descrito en .

\begin{algorithm}[H]
    \KwIn{$x_0$, $d_0$}
    \KwOut{$x^*$}
    $d_0 \gets -\nabla f(x_0)$\\
    \While{$||g(x_k)||<\tau$}{
        $\alpha_k \gets$ line search\\
        Calcular $\nabla f(x_{k+1})$\\
        Calcular $\beta_{k+1}$\\
        $d_{k+1}=\beta_{k+1}d_k-\nabla f(x_{k+1})$
    }
\end{algorithm}

La razón por la que $\alpha_k$ es calculado usando una búsqueda en linea es debido a que asi se asegura que la dirección $d_{k+1}$ es de descenso. Esto es debido a que $d_{k+1}^Td_k=0$, entonces

\begin{align*}
    d_{k+1}^Td_{k+1} & = - ||g_{k+1}||^2+\beta_{k+1}^{FR}g_{k+1}^Td_k \\
                     & = -||g_{k+1}||^2                               \\
                     & <0
\end{align*}

Si $\alpha_{k+1}$ no es un tamaño de paso exacto. El término $beta_{k+1}^{FR}g_{k+1}^Td_k $ podria dominar provocando que no se garantize el descenso. Para el caso cuando $\alpha_{k+1}$ no es un tamaño de paso exacto, se puede garantizar el descenso si se usan las condiciones fuertes de Wolfe. Esto es

\begin{align*}
    f(x_{k+1})     & \leq f(x_{k})+c_1 \alpha_k g_k^Td_k \\
    |g_{k+1}^Td_k| & \leq -c_2g_k^Td_k
\end{align*}

con $0<c_1<c_2<1$. Para el caso del algoritmo de gradiente Conjugado, el valor de $c_2=0.1$ garantiza la convergencia.

\subsection{Fletcher-Reeves}

El algortimo de Fletcher-Reeves\cite{fletcher_1964} se basa en encontrar una secuencia de $\beta_k$ descritas en la ecuación \ref{eq:FR_beta}.

\begin{equation}
    \beta_{k+1}^{FR} = \frac{\nabla^T f(x_{k+1})\nabla f(x_{k+1})}{\nabla^T f(x_{k})\nabla f(x_{k})} \label{eq:FR_beta}
\end{equation}

\subsection{Polak-Ribiere}

El algortimo de Polak-Ribiere\cite{polak_1969} se basa en encontrar una secuencia de $\beta_k$ descritas en la ecuación \ref{eq:PR_beta}.

\begin{equation}
    \beta_{k+1}^{PR} = \frac{\nabla^T f(x_{k+1})(\nabla f(x_{k+1})-\nabla f(x_{k}))}{\nabla^T f(x_{k})\nabla f(x_{k})} \label{eq:PR_beta}
\end{equation}

Es conveniente elegir el valor máximo entre $\beta_{k+1}^{PR}$ y 0. Esto es debido a que existen funciones las cuales pueden ciclarse de forma indefinida.

\subsection{Hestenes-Stiefel}

El algortimo de Hestenes-Stiefel\cite{hestenes_1978} se basa en encontrar una secuencia de $\beta_k$ descritas en la ecuación \ref{eq:HS_beta}.

\begin{equation}
    \beta_{k+1}^{HS} = \frac{\nabla^T f(x_{k+1})(\nabla f(x_{k+1})-\nabla f(x_{k}))}{(\nabla f(x_{k+1})-\nabla f(x_{k}))^Td_{k}} \label{eq:HS_beta}
\end{equation}


\subsection{Fletcher-Reeves Polak-Ribiere}

El algortimo de Fletcher-Reeves en combinación con Polak-Ribiere\cite{Babaie_2014} se basa en encontrar una secuencia de $\beta_k$ descritas en la ecuación \ref{eq:FR_PR_beta}.

\begin{equation}
    \beta_{k+1}^{FR-PR} = \begin{cases}
        -\beta_k^{FR} & \text{si } \beta_k^{PF} < -\beta_k^{FR}     \\
        \beta_k^{PR}  & \text{si } |\beta_k^{PF}| \leq \beta_k^{FR} \\
        \beta_k^{FR}  & \text{si } \beta_k^{PF} >\beta_k^{FR}
    \end{cases} \label{eq:FR_PR_beta}
\end{equation}

La secuencia $\beta_{k+1}^{FR-PR}$ garantiza la convergencia, debido a que $\beta_k \leq \beta_k^{FR}$ para $k>2$.

\subsection{Función de suavizado}

En este trabajo utilizaremos como función de costo la ecuación \ref{eq:cost_function}.

\begin{equation}
    f(x) =\sum_{ij} \left[(x_{i,j}-g_{i,j})^2 + \lambda \sum_{(l,m)\in \Omega_{i,j}}\sqrt{(x_{i,j}-x_{l,m})^2+\mu} \right] \label{eq:cost_function}
\end{equation}

donde $g$ es la función a suavizar, en este caso será la imagen \file{lena.png}, $\lambda$ es un parámetro positivo a elegir, $\mu$ es un parámetro fijo con valor de 0.01 y el conjunto $\Omega_{i,j}$ esta definido de la siguiente manera:

\begin{equation*}
    \Omega_{i,j} = \{ (i+1,j), (i-1,j), (i,j+1), (i,j-1)\}
\end{equation*}

El gradiente de la ecuación \ref{eq:cost_function} se encuentra descrito en la ecuación \ref{eq:gradient}.

\begin{equation}
    \frac{\partial f(x)}{\partial x_{i,j}} = 2(x_{i,j} -g_{i,j}) + \lambda \sum_{(l,m) \in \Omega_{i,j}} \frac{x_{i,j}-x_{l,m}}{\sqrt{(x_{i,j}-x_{l,m})^2+\mu}} \label{eq:gradient}
\end{equation}

