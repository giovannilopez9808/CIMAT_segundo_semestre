\begin{center}
    \begin{minipage}{0.85\linewidth}
        \begin{center}
            Resumen
        \end{center}
        \vspace{-0.5cm}
        El método de Newton se centra en encontrar el mínimo global de una función a partir de un punto cercano al óptimo. Esto si se tiene un hessiano de al función positivo definido y no singular. Si se tiene otro caso el método de Newton no ofrece una convergencia. Es por ello que se emplean distintas estrategias para obtener el mejor resultado posible. Algunas de estas estrategias es usar una región de confianza en la que se encontrará un punto óptimo dentro de la región. Otra alternativa es transformar la matriz H para que sea una matriz positiva definida. Además del método de Newton se puede usar un método llamado Dogleg, el cual combina las estrategias de descenso de gradiente, región de confianza y punto de Cauchy para obtener un mínimo global de la función. En este trabajo se usaron las funciones Wood, Rosembrock y Branin como funciones de benchmarck. Llegando a que los métodos que involucran al paso de Newton tienden a tener un mayor número de iteraciones a comparación de los métodos que usan el punto de Cauchy. El método de Cauchy-Newton y Cauchy obtienen como resultado el mínimo global con una frecuencia mayor.
    \end{minipage}
\end{center}