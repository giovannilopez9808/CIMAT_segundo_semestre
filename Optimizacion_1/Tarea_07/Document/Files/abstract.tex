\begin{center}
    \begin{minipage}{0.85\linewidth}
        \begin{center}
            Resumen
        \end{center}
        \vspace{-0.5cm}
        Los métodos del descenso del gradiente y de Newton son los métodos más simples en la rama de la optimización. Esto debido a que se basan en principios matemáticos los cuales indican que existe un mínimo global o local en una función con dominio convexo. En este trabajo se analizo la eficiencia de estos dos métodos usando las funciones de Rosembrock, de Wood y de suavizado. Llegando así a que el método del descenso del gradiente tiende a obtener mínimos locales cercanos al mínimo global en cuestión del valor de la función a analizar. Por otro lado el método de Newton obtiene un mínimo local cercano al punto inicial con un número menor de iteraciones a comparación del método del descenso del gradiente.
    \end{minipage}
\end{center}