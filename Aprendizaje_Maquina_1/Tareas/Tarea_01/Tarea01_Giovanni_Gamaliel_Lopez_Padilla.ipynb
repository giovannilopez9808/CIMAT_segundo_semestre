{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción al aprendizaje profundo\n",
    "#### Giovanni Gamaliel López Padilla\n",
    "##### Tarea 01\n",
    "\n",
    "###### Versiones utilizadas\n",
    "```bash\n",
    "numpy == 1.22.3\n",
    "tabulate == 0.8.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_all_params() -> dict:\n",
    "    \"\"\"\n",
    "    Funcion que reune los parámtros de la función y el gradiente. Devuelve estos dos parámetros en forma de diccionarios.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"models\": [\"SGD\",\n",
    "                   \"NAG\",\n",
    "                   \"ADAM\",\n",
    "                   \"ADADELTA\"],\n",
    "        \"max iteration\": 100,\n",
    "        \"n\": 100,\n",
    "        \"sigma\": 1,\n",
    "        \"epsilon\": 0.01\n",
    "    }\n",
    "\n",
    "    # parámetros del algoritmo\n",
    "    gd_params = {\n",
    "        'alpha': 0.95,\n",
    "        'alphaADADELTA': 0.95,\n",
    "        'alphaADAM': 0.95,\n",
    "        'nIter': 300,\n",
    "        'batch_size': 10,\n",
    "        'eta': 0.9,\n",
    "        'eta1': 0.9,\n",
    "        'eta2': 0.999\n",
    "    }\n",
    "    return params, gd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, ones, mean, ones_like, array\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def print_results(params: dict, results: array) -> None:\n",
    "    table = []\n",
    "    for model_name in params[\"models\"]:\n",
    "        time = results[model_name][\"time\"]\n",
    "        error = results[model_name][\"error\"]\n",
    "        table += [[model_name, time, error]]\n",
    "    print(tabulate(table,\n",
    "                   headers=[\"Model\",\n",
    "                            \"Time\",\n",
    "                            \"Error\"]))\n",
    "\n",
    "\n",
    "class function_class:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def phi(self, y: array, mu: array, sigma: array) -> array:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Parámetros\n",
    "        -----------\n",
    "        y -> Patrones a Aproximar\n",
    "        mu -> Array de medias\n",
    "        sigma -> Vector de Desviaciones\n",
    "        Output\n",
    "        -----------\n",
    "        phi          : matriz de kerneles\n",
    "        \"\"\"\n",
    "        mu_aux = mu.reshape(-1, 1)\n",
    "        phi = exp(-(y-mu_aux)**2/(2*sigma**2))\n",
    "        return phi\n",
    "\n",
    "    def gradient_gaussian_mu(self, theta: None, f_params: dict) -> array:\n",
    "        \"\"\"\n",
    "        Calcula el gradiente respecto a mu\n",
    "        Parámetros\n",
    "        -----------\n",
    "        theta\n",
    "        f_params -> lista de parametros para la funcion objetivo,\n",
    "        X = f_params['X'] Variable independiente\n",
    "        y = f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "            Array gradiente\n",
    "        \"\"\"\n",
    "        # Obtengo Parámetros\n",
    "        phi = f_params['X']\n",
    "        alpha = f_params['Alpha']\n",
    "        n = f_params['n']\n",
    "        y = f_params['y']\n",
    "        mu = f_params['mu']\n",
    "        alpha = alpha.reshape((-1, 1))\n",
    "        mu = mu.reshape((-1, 1))\n",
    "        y = y.reshape((-1, 1))\n",
    "        gradient = (phi @ alpha - y) @ alpha.T * \\\n",
    "            (y @ ones((1, n)) - ones_like(y) @ mu.T)\n",
    "        return mean(gradient, axis=0)\n",
    "\n",
    "    def gradient_gaussian_alpha(self, theta: None, f_params: dict) -> array:\n",
    "        \"\"\"\n",
    "        Calcula el gradiente respecto a alpha\n",
    "        Parámetros\n",
    "        -----------\n",
    "            theta\n",
    "            f_params : lista de parametros para la funcion objetivo,\n",
    "                        X -> f_params['X'] Variable independiente\n",
    "                        y -> f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "            Array gradiente\n",
    "        \"\"\"\n",
    "        # Obtengo Parámetros\n",
    "        phi = f_params['X']\n",
    "        y = f_params['y']\n",
    "        alpha = f_params['Alpha']\n",
    "        gradient = phi.T @ (phi @ alpha - y)\n",
    "        return mean(gradient, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, zeros, sqrt\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Modelo que reune los metodos de \n",
    "        + Descenso de gradiente estocástico.\n",
    "        + Descenso de gradiente estoc ́astico accelerado de tipo Nesterov.\n",
    "        + AdaDelta\n",
    "        + ADAM\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def select_method(self, method_name: str):\n",
    "        if method_name == \"SGD\":\n",
    "            self.method = self.SGD\n",
    "        if method_name == \"NAG\":\n",
    "            self.method = self.NAG\n",
    "        if method_name == \"ADADELTA\":\n",
    "            self.method = self.ADADELTA\n",
    "        if method_name == \"ADAM\":\n",
    "            self.method = self.ADAM\n",
    "\n",
    "    def SGD(self, theta: list, grad, gd_params: dict, f_params: dict,) -> array:\n",
    "        \"\"\"\n",
    "        Descenso de gradiente estocástico\n",
    "\n",
    "        Parámetros\n",
    "        -----------\n",
    "        theta     :   condicion inicial\n",
    "        grad      :   funcion que calcula el gradiente\n",
    "\n",
    "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
    "                        nIter = gd_params['nIter'] número de iteraciones\n",
    "                        alpha = gd_params['alpha'] tamaño de paso alpha\n",
    "                        batch_size = gd_params['batch_size'] tamaño de la muestra\n",
    "\n",
    "        f_params  :   lista de parametros para la funcion objetivo,\n",
    "                        X     = f_params['X'] Variable independiente\n",
    "                        y     = f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "        Theta     :   trayectoria de los parametros\n",
    "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "        \"\"\"\n",
    "        (high, dim) = f_params['X'].shape\n",
    "        batch_size = gd_params['batch_size']\n",
    "        nIter = gd_params['nIter']\n",
    "        alpha = gd_params['alpha']\n",
    "        Theta = []\n",
    "        for t in range(nIter):\n",
    "            # Set of sampled indices\n",
    "            smpIdx = randint(low=0,\n",
    "                             high=high,\n",
    "                             size=batch_size,\n",
    "                             dtype='int32')\n",
    "            # sample\n",
    "            smpX = f_params['X'][smpIdx]\n",
    "            smpy = f_params['y'][smpIdx]\n",
    "            # parametros de la funcion objetivo\n",
    "            smpf_params = {\"Alpha\": f_params[\"Alpha\"],\n",
    "                           \"mu\": f_params[\"mu\"],\n",
    "                           \"n\": f_params[\"n\"],\n",
    "                           'X': smpX,\n",
    "                           'y': smpy}\n",
    "            p = grad(theta,\n",
    "                     f_params=smpf_params)\n",
    "            theta = theta - alpha*p\n",
    "            Theta.append(theta)\n",
    "        return array(Theta)\n",
    "\n",
    "    def NAG(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
    "        \"\"\"\n",
    "        Descenso acelerado de Nesterov\n",
    "\n",
    "        Parámetros\n",
    "        -----------\n",
    "        theta     :   condicion inicial\n",
    "        grad      :   funcion que calcula el gradiente\n",
    "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
    "                        nIter = gd_params['nIter'] número de iteraciones\n",
    "                        alpha = gd_params['alpha'] tamaño de paso alpha\n",
    "                        eta   = gd_params['eta']  parametro de inercia (0,1]\n",
    "        f_params  :   lista de parametros para la funcion objetivo,\n",
    "                        X     = f_params['X'] Variable independiente\n",
    "                        y     = f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "        Theta     :   trayectoria de los parametros\n",
    "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "        \"\"\"\n",
    "        nIter = gd_params['nIter']\n",
    "        alpha = gd_params['alpha']\n",
    "        eta = gd_params['eta']\n",
    "        p = zeros(theta.shape)\n",
    "        Theta = []\n",
    "        for t in range(nIter):\n",
    "            pre_theta = theta - 2.0*alpha*p\n",
    "            g = grad(pre_theta,\n",
    "                     f_params=f_params)\n",
    "            p = g + eta*p\n",
    "            theta = theta - alpha*p\n",
    "            Theta.append(theta)\n",
    "        return array(Theta)\n",
    "\n",
    "    def ADADELTA(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
    "        \"\"\"\n",
    "        Descenso de Gradiente Adaptable (ADADELTA)\n",
    "\n",
    "        Parámetros\n",
    "        -----------\n",
    "        theta     :   condicion inicial\n",
    "        grad      :   funcion que calcula el gradiente\n",
    "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
    "                        nIter    = gd_params['nIter'] número de iteraciones\n",
    "                        alphaADA = gd_params['alphaADADELTA'] tamaño de paso alpha\n",
    "                        eta      = gd_params['eta']  parametro adaptación del alpha\n",
    "        f_params  :   lista de parametros para la funcion objetivo,\n",
    "                        X     = f_params['X'] Variable independiente\n",
    "                        y     = f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "        Theta     :   trayectoria de los parametros\n",
    "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        nIter = gd_params['nIter']\n",
    "        alpha = gd_params['alphaADADELTA']\n",
    "        eta = gd_params['eta']\n",
    "        G = zeros(theta.shape)\n",
    "        g = zeros(theta.shape)\n",
    "        Theta = []\n",
    "        for t in range(nIter):\n",
    "            g = grad(theta,\n",
    "                     f_params=f_params)\n",
    "            G = eta*g**2 + (1-eta)*G\n",
    "            p = 1.0/(sqrt(G)+epsilon)*g\n",
    "            theta = theta - alpha * p\n",
    "            Theta.append(theta)\n",
    "        return array(Theta)\n",
    "\n",
    "    def ADAM(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
    "        \"\"\"\n",
    "        Descenso de Gradiente Adaptable con Momentum(A DAM)\n",
    "\n",
    "        Parámetros\n",
    "        -----------\n",
    "        theta     :   condicion inicial\n",
    "        grad      :   funcion que calcula el gradiente\n",
    "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
    "                        nIter    = gd_params['nIter'] número de iteraciones\n",
    "                        alphaADA = gd_params['alphaADAM'] tamaño de paso alpha\n",
    "                        eta1     = gd_params['eta1'] factor de momentum para la direccion\n",
    "                                    de descenso (0,1)\n",
    "                        eta2     = gd_params['eta2'] factor de momentum para la el\n",
    "                                    tamaño de paso (0,1)\n",
    "        f_params  :   lista de parametros para la funcion objetivo,\n",
    "                        kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
    "                        X     = f_params['X'] Variable independiente\n",
    "                        y     = f_params['y'] Variable dependiente\n",
    "\n",
    "        Output\n",
    "        -----------\n",
    "        Theta     :   trayectoria de los parametros\n",
    "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        nIter = gd_params['nIter']\n",
    "        alpha = gd_params['alphaADAM']\n",
    "        eta1 = gd_params['eta1']\n",
    "        eta2 = gd_params['eta2']\n",
    "        p = zeros(theta.shape)\n",
    "        v = 0.0\n",
    "        Theta = []\n",
    "        eta1_t = eta1\n",
    "        eta2_t = eta2\n",
    "        for t in range(nIter):\n",
    "            g = grad(theta,\n",
    "                     f_params=f_params)\n",
    "            p = eta1*p + (1.0-eta1)*g\n",
    "            v = eta2*v + (1.0-eta2)*(g**2)\n",
    "            theta = theta - alpha * p / (sqrt(v)+epsilon)\n",
    "            eta1_t *= eta1\n",
    "            eta2_t *= eta2\n",
    "            Theta.append(theta)\n",
    "        return array(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform\n",
    "from numpy.linalg import norm\n",
    "from numpy import linspace\n",
    "import time\n",
    "\n",
    "\n",
    "def solver(models: model_class, y: list, params: dict, gd_params: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Funcion que ejecuta un algoritmo para realizar la optimización de la función dado un diccionario de parametros\n",
    "\n",
    "    Parámetros\n",
    "    -----------------------\n",
    "    models -> modelo que contiene los métodos de optimización de parámetros\n",
    "    y -> patrones a aproximar\n",
    "    params -> diccionario que contiene los parametros de las iteraciones\n",
    "    gd_params -> diccionario que contiene los parametros del modelo\n",
    "    \"\"\"\n",
    "    max_iteration = params[\"max iteration\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    sigma = params[\"sigma\"]\n",
    "    n = params[\"n\"]\n",
    "    functions = function_class()\n",
    "    t_init = time.clock_gettime(0)\n",
    "    # Valores Iniciales\n",
    "    mu = linspace(0, 100, n)\n",
    "    phi = functions.phi(y, mu, sigma)\n",
    "    alpha = uniform(0, sigma, n)\n",
    "    # Parámetros para el gradiente\n",
    "    f_params = {\n",
    "        'mu': mu,\n",
    "        'X': phi,\n",
    "        'y': y,\n",
    "        'Alpha': alpha,\n",
    "        'n': n\n",
    "    }\n",
    "    iteration = 0\n",
    "    while iteration < max_iteration:\n",
    "        # descenso para alpha\n",
    "        alpha = models.method(alpha,\n",
    "                              grad=functions.gradient_gaussian_alpha,\n",
    "                              gd_params=gd_params,\n",
    "                              f_params=f_params)[-1]\n",
    "        if norm(phi @ alpha - y) < epsilon:\n",
    "            break\n",
    "        # descenso para mu\n",
    "        mu_old = mu\n",
    "        mu = models.method(mu,\n",
    "                           grad=functions.gradient_gaussian_mu,\n",
    "                           gd_params=gd_params,\n",
    "                           f_params=f_params)[-1]\n",
    "        # actualizacion\n",
    "        phi = functions.phi(y, mu, sigma)\n",
    "        # Criterio de parada\n",
    "        if norm(mu - mu_old) < epsilon:\n",
    "            break\n",
    "        # Número máximo de iteraciones si no hay convergencia\n",
    "        iteration += 1\n",
    "    t_end = time.clock_gettime(0)\n",
    "    total_time = t_end - t_init\n",
    "    return phi, alpha, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3LObA4eC5As",
    "outputId": "130acf38-8bab-4fc6-c1f9-352509db8f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolviendo por medio de SGD\n",
      "Resolviendo por medio de NAG\n",
      "Resolviendo por medio de ADAM\n",
      "Resolviendo por medio de ADADELTA\n",
      "Model         Time     Error\n",
      "--------  --------  --------\n",
      "SGD        8.25008  0.335291\n",
      "NAG       11.3339   0.335291\n",
      "ADAM      14.4546   0.335291\n",
      "ADADELTA  14.6655   0.335291\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import uniform\n",
    "results = {}\n",
    "params, gd_params = obtain_all_params()\n",
    "models = model_class()\n",
    "y = uniform(0, 1, params[\"n\"])\n",
    "for model_name in params[\"models\"]:\n",
    "    print(\"Resolviendo por medio de {}\".format(model_name))\n",
    "    results[model_name] = {}\n",
    "    models.select_method(model_name)\n",
    "    phi, alpha, time_solver = solver(models, y, params, gd_params)\n",
    "    error = round(((phi @ alpha - y)**2).mean(), 8)\n",
    "    results[model_name][\"time\"] = time_solver\n",
    "    results[model_name][\"error\"] = error\n",
    "print_results(params, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
