{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from numpy import exp, zeros, ones, mean, ones_like,square,linspace,sqrt,zeros,array\n",
        "from numpy.random import randint,uniform,normal\n",
        "from numpy.linalg import norm\n",
        "from tabulate import tabulate\n",
        "import time"
      ],
      "metadata": {
        "id": "A1WQWJegDrWy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ft5LmAHBRXU"
      },
      "outputs": [],
      "source": [
        "def obtain_all_params() -> dict:\n",
        "    \"\"\"\n",
        "    Funcion que reune los parametros de la función, su gradiente y de los métodos a utilizar\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"max_iter\": 100,\n",
        "        \"n\": 100,\n",
        "        \"m\": 2000,\n",
        "        \"sigma\": 1,\n",
        "        \"epsilon\": 0.01,\n",
        "        \"theta\": 10 * normal(size=2)\n",
        "    }\n",
        "\n",
        "    # parámetros del algoritmo\n",
        "    gd_params = {\n",
        "        'alpha': 0.95,\n",
        "        'alphaADADELTA': 0.7,\n",
        "        'alphaADAM': 0.95,\n",
        "        'nIter': 300,\n",
        "        'batch_size': 100,\n",
        "        'eta': 0.9,\n",
        "        'eta1': 0.9,\n",
        "        'eta2': 0.999\n",
        "    }\n",
        "    return params, gd_params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class function_class:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def update_phi(self, Y, mu, sigma, n):\n",
        "        '''\n",
        "        Construye  Matriz de Kerneles Phi\n",
        "\n",
        "        Parámetros\n",
        "        -----------\n",
        "        + Y: Patrones a Aproximar\n",
        "        + mu: Array de medias\n",
        "        + sigma: Vector de Desviaciones\n",
        "        + n: Número de funciones radiales usadas\n",
        "        Regresa\n",
        "        -----------\n",
        "            phi          : matriz de kerneles\n",
        "        '''\n",
        "        phi = zeros((Y.shape[0], n))\n",
        "        for i in range(n):\n",
        "            phi[:, i] = exp(- 1.0 / (2*sigma**2) * (Y - mu[i])**2)\n",
        "        return phi\n",
        "\n",
        "    def grad_gaussian_radial_mu(self, theta, f_params):\n",
        "        '''\n",
        "        Calcula el gradiente respecto a mu\n",
        "        Parámetros\n",
        "        -----------\n",
        "        + theta\n",
        "        + f_params: lista de parametros para la funcion objetivo,\n",
        "        + kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "        + X = f_params['X'] Variable independiente\n",
        "        + y = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "            Array gradiente\n",
        "        '''\n",
        "        # Obtengo Parámetros\n",
        "        phi = f_params['X']\n",
        "        alpha = f_params['Alpha']\n",
        "        n = f_params['n']\n",
        "        Y = f_params['y']\n",
        "        mu = f_params['mu']\n",
        "        alpha = alpha.reshape((-1, 1))\n",
        "        mu = mu.reshape((-1, 1))\n",
        "        Y = Y.reshape((-1, 1))\n",
        "        gradient = (phi @ alpha - Y) @ alpha.T * \\\n",
        "            (Y @ ones((1, n)) - ones_like(Y) @ mu.T)\n",
        "        return mean(gradient, axis=0)\n",
        "\n",
        "    def grad_gaussian_radial_alpha(self, theta, f_params):\n",
        "        '''\n",
        "        Calcula el gradiente respecto a alpha\n",
        "        Parámetros\n",
        "        -----------\n",
        "        + theta\n",
        "        + f_params: lista de parametros para la funcion objetivo,\n",
        "        + kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "        + X = f_params['X'] Variable independiente\n",
        "        + y = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "            Array gradiente\n",
        "        '''\n",
        "        # Obtengo Parámetros\n",
        "        phi = f_params['X']\n",
        "        Y = f_params['y']\n",
        "        alpha = f_params['Alpha']\n",
        "        gradient = phi.T @ (phi @ alpha - Y)\n",
        "        return mean(gradient, axis=0)"
      ],
      "metadata": {
        "id": "5Mb4lADgBmR3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model_class:\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        Modelo que reune los metodos de \n",
        "        + Descenso de gradiente estocástico.\n",
        "        + Descenso de gradiente estoc ́astico accelerado de tipo Nesterov.\n",
        "        + AdaDelta\n",
        "        + ADAM\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def select_method(self, method_name: str):\n",
        "        if method_name == \"SGD\":\n",
        "            self.method = self.SGD\n",
        "        if method_name == \"NAG\":\n",
        "            self.method = self.NAG\n",
        "        if method_name == \"ADADELTA\":\n",
        "            self.method = self.ADADELTA\n",
        "        if method_name == \"ADAM\":\n",
        "            self.method = self.ADAM\n",
        "\n",
        "    def SGD(self, theta: list, grad, gd_params: dict, f_params: dict,) -> array:\n",
        "        \"\"\"\n",
        "        Descenso de gradiente estocástico\n",
        "\n",
        "        Parámetros\n",
        "        -----------\n",
        "        theta     :   condicion inicial\n",
        "        grad      :   funcion que calcula el gradiente\n",
        "\n",
        "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
        "                        nIter = gd_params['nIter'] número de iteraciones\n",
        "                        alpha = gd_params['alpha'] tamaño de paso alpha\n",
        "                        batch_size = gd_params['batch_size'] tamaño de la muestra\n",
        "\n",
        "        f_params  :   lista de parametros para la funcion objetivo,\n",
        "                        kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "                        X     = f_params['X'] Variable independiente\n",
        "                        y     = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "        Theta     :   trayectoria de los parametros\n",
        "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
        "        \"\"\"\n",
        "        (high, dim) = f_params['X'].shape\n",
        "        batch_size = gd_params['batch_size']\n",
        "        nIter = gd_params['nIter']\n",
        "        alpha = gd_params['alpha']\n",
        "        Theta = []\n",
        "        for t in range(nIter):\n",
        "            # Set of sampled indices\n",
        "            smpIdx = randint(low=0,\n",
        "                             high=high,\n",
        "                             size=batch_size,\n",
        "                             dtype='int32')\n",
        "            # sample\n",
        "            smpX = f_params['X'][smpIdx]\n",
        "            smpy = f_params['y'][smpIdx]\n",
        "            # parametros de la funcion objetivo\n",
        "            smpf_params = {'kappa': f_params['kappa'],\n",
        "                           \"Alpha\": f_params[\"Alpha\"],\n",
        "                           \"mu\": f_params[\"mu\"],\n",
        "                           \"n\": f_params[\"n\"],\n",
        "                           'X': smpX,\n",
        "                           'y': smpy}\n",
        "            p = grad(theta,\n",
        "                     f_params=smpf_params)\n",
        "            theta = theta - alpha*p\n",
        "            Theta.append(theta)\n",
        "        return array(Theta)\n",
        "\n",
        "    def NAG(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
        "        \"\"\"\n",
        "        Descenso acelerado de Nesterov\n",
        "\n",
        "        Parámetros\n",
        "        -----------\n",
        "        theta     :   condicion inicial\n",
        "        grad      :   funcion que calcula el gradiente\n",
        "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
        "                        nIter = gd_params['nIter'] número de iteraciones\n",
        "                        alpha = gd_params['alpha'] tamaño de paso alpha\n",
        "                        eta   = gd_params['eta']  parametro de inercia (0,1]\n",
        "        f_params  :   lista de parametros para la funcion objetivo,\n",
        "                        kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "                        X     = f_params['X'] Variable independiente\n",
        "                        y     = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "        Theta     :   trayectoria de los parametros\n",
        "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
        "        \"\"\"\n",
        "        nIter = gd_params['nIter']\n",
        "        alpha = gd_params['alpha']\n",
        "        eta = gd_params['eta']\n",
        "        p = zeros(theta.shape)\n",
        "        Theta = []\n",
        "        for t in range(nIter):\n",
        "            pre_theta = theta - 2.0*alpha*p\n",
        "            g = grad(pre_theta,\n",
        "                     f_params=f_params)\n",
        "            p = g + eta*p\n",
        "            theta = theta - alpha*p\n",
        "            Theta.append(theta)\n",
        "        return array(Theta)\n",
        "\n",
        "    def ADADELTA(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
        "        \"\"\"\n",
        "        Descenso de Gradiente Adaptable (ADADELTA)\n",
        "\n",
        "        Parámetros\n",
        "        -----------\n",
        "        theta     :   condicion inicial\n",
        "        grad      :   funcion que calcula el gradiente\n",
        "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
        "                        nIter    = gd_params['nIter'] número de iteraciones\n",
        "                        alphaADA = gd_params['alphaADADELTA'] tamaño de paso alpha\n",
        "                        eta      = gd_params['eta']  parametro adaptación del alpha\n",
        "        f_params  :   lista de parametros para la funcion objetivo,\n",
        "                        kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "                        X     = f_params['X'] Variable independiente\n",
        "                        y     = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "        Theta     :   trayectoria de los parametros\n",
        "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
        "        \"\"\"\n",
        "        epsilon = 1e-8\n",
        "        nIter = gd_params['nIter']\n",
        "        alpha = gd_params['alphaADADELTA']\n",
        "        eta = gd_params['eta']\n",
        "        G = zeros(theta.shape)\n",
        "        g = zeros(theta.shape)\n",
        "        Theta = []\n",
        "        for t in range(nIter):\n",
        "            g = grad(theta,\n",
        "                     f_params=f_params)\n",
        "            G = eta*g**2 + (1-eta)*G\n",
        "            p = 1.0/(sqrt(G)+epsilon)*g\n",
        "            theta = theta - alpha * p\n",
        "            Theta.append(theta)\n",
        "        return array(Theta)\n",
        "\n",
        "    def ADAM(self, theta: list, grad, gd_params: dict, f_params: dict,):\n",
        "        \"\"\"\n",
        "        Descenso de Gradiente Adaptable con Momentum(A DAM)\n",
        "\n",
        "        Parámetros\n",
        "        -----------\n",
        "        theta     :   condicion inicial\n",
        "        grad      :   funcion que calcula el gradiente\n",
        "        gd_params :   lista de parametros para el algoritmo de descenso,\n",
        "                        nIter    = gd_params['nIter'] número de iteraciones\n",
        "                        alphaADA = gd_params['alphaADAM'] tamaño de paso alpha\n",
        "                        eta1     = gd_params['eta1'] factor de momentum para la direccion\n",
        "                                    de descenso (0,1)\n",
        "                        eta2     = gd_params['eta2'] factor de momentum para la el\n",
        "                                    tamaño de paso (0,1)\n",
        "        f_params  :   lista de parametros para la funcion objetivo,\n",
        "                        kappa = f_params['kappa'] parametro de escala (rechazo de outliers)\n",
        "                        X     = f_params['X'] Variable independiente\n",
        "                        y     = f_params['y'] Variable dependiente\n",
        "\n",
        "        Regresa\n",
        "        -----------\n",
        "        Theta     :   trayectoria de los parametros\n",
        "                        Theta[-1] es el valor alcanzado en la ultima iteracion\n",
        "        \"\"\"\n",
        "        epsilon = 1e-8\n",
        "        nIter = gd_params['nIter']\n",
        "        alpha = gd_params['alphaADAM']\n",
        "        eta1 = gd_params['eta1']\n",
        "        eta2 = gd_params['eta2']\n",
        "        p = zeros(theta.shape)\n",
        "        v = 0.0\n",
        "        Theta = []\n",
        "        eta1_t = eta1\n",
        "        eta2_t = eta2\n",
        "        for t in range(nIter):\n",
        "            g = grad(theta,\n",
        "                     f_params=f_params)\n",
        "            p = eta1*p + (1.0-eta1)*g\n",
        "            v = eta2*v + (1.0-eta2)*(g**2)\n",
        "            theta = theta - alpha * p / (sqrt(v)+epsilon)\n",
        "            eta1_t *= eta1\n",
        "            eta2_t *= eta2\n",
        "            Theta.append(theta)\n",
        "        return array(Theta)"
      ],
      "metadata": {
        "id": "jrvSZaHSCtnL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solver(models: model_class, Y: list, params: dict, gd_params: dict):\n",
        "    \"\"\"\"\n",
        "    Modelo que resuelve el problema dado con una serie de metodos\n",
        "\n",
        "    Parámetros\n",
        "    --------------------------\n",
        "    + models: clase que reune a todos los metodos para obtener la solucion del problema\n",
        "    + Y: datos a aproximar\n",
        "    + params: parametros del modelo a utilizar\n",
        "    + gd_params: parametros del metodo a utilizar\n",
        "\n",
        "    Regresa\n",
        "    ---------------------------\n",
        "        phi, alpha y tiempo de ejeccion del metodo\n",
        "    \"\"\"\"\n",
        "    max_iter = params[\"max_iter\"]\n",
        "    epsilon = params[\"epsilon\"]\n",
        "    sigma = params[\"sigma\"]\n",
        "    n = params[\"n\"]\n",
        "    functions = function_class()\n",
        "    t_init = time.clock_gettime(0)\n",
        "    # Valores Iniciales\n",
        "    mu = linspace(0, 100, n)\n",
        "    phi = functions.update_phi(Y, mu, sigma, n)\n",
        "    alpha = uniform(0, sigma, n)\n",
        "    # Parámetros para el gradiente\n",
        "    f_params = {\n",
        "        'kappa': 0.01,\n",
        "        'mu': mu,\n",
        "        'X': phi,\n",
        "        'y': Y,\n",
        "        'Alpha': alpha,\n",
        "        'n': n\n",
        "    }\n",
        "    num_iter = 0\n",
        "    while num_iter < max_iter:\n",
        "        # descenso para alpha\n",
        "        alpha = models.method(alpha,\n",
        "                              grad=functions.grad_gaussian_radial_alpha,\n",
        "                              gd_params=gd_params,\n",
        "                              f_params=f_params)[-1]\n",
        "        if norm(phi @ alpha - Y) < epsilon:\n",
        "            break\n",
        "        # DESCENSO  PARA  MU\n",
        "        mu_old = mu\n",
        "        mu = models.method(mu,\n",
        "                           grad=functions.grad_gaussian_radial_mu,\n",
        "                           gd_params=gd_params,\n",
        "                           f_params=f_params)[-1]\n",
        "        # actualizacion\n",
        "        phi = functions.update_phi(Y, mu, sigma, n)\n",
        "        # Criterio de parada\n",
        "        if norm(mu - mu_old) < epsilon:\n",
        "            break\n",
        "        # Número máximo de iteraciones si no hay convergencia\n",
        "        num_iter += 1\n",
        "    t_end = time.clock_gettime(0)\n",
        "    total_time = t_end - t_init\n",
        "    return phi, alpha, total_time"
      ],
      "metadata": {
        "id": "ZSapOpjCCyaQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\"SGD\",\n",
        "               \"NAG\",\n",
        "               \"ADAM\",\n",
        "               \"ADADELTA\"]\n",
        "# inicializacion del guardado de los resultados\n",
        "results = {}\n",
        "# Lecutura de los parametros\n",
        "params, gd_params = obtain_all_params()\n",
        "# Inicializacion de los metodos\n",
        "models = model_class()\n",
        "y = uniform(0, 1, params[\"m\"])\n",
        "# recorrido por todos los modelos\n",
        "for model_name in model_names:\n",
        "    print(\"Resolviendo por medio de {}\".format(model_name))\n",
        "    # Inicializacion del guardado de cada metodo\n",
        "    results[model_name] = {}\n",
        "    # Seleccion del metodo\n",
        "    models.select_method(model_name)\n",
        "    # Solver del problema\n",
        "    phi, alpha, total_time = solver(models, y, params, gd_params)\n",
        "    # Error\n",
        "    error = round(square(phi @ alpha - y).mean(), 8)\n",
        "    # Guardado de los resultados\n",
        "    results[model_name][\"time\"] = total_time\n",
        "    results[model_name][\"error\"] = error\n",
        "# Organizacion para el guardado de los resultados\n",
        "table = []\n",
        "for model_name in model_names:\n",
        "    total_time = results[model_name][\"time\"]\n",
        "    error = results[model_name][\"error\"]\n",
        "    table += [[model_name, total_time, error]]\n",
        "print(tabulate(table,\n",
        "               headers=[\"Model\",\n",
        "                        \"Time\",\n",
        "                        \"Error\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3LObA4eC5As",
        "outputId": "130acf38-8bab-4fc6-c1f9-352509db8f23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolviendo por medio de SGD\n",
            "Resolviendo por medio de NAG\n",
            "Resolviendo por medio de ADAM\n",
            "Resolviendo por medio de ADADELTA\n",
            "Model         Time     Error\n",
            "--------  --------  --------\n",
            "SGD        16.9738  0.342642\n",
            "NAG       128.874   0.342642\n",
            "ADAM      129.253   0.342642\n",
            "ADADELTA  126.732   0.342642\n"
          ]
        }
      ]
    }
  ]
}