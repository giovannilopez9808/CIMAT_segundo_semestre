{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo1o-tLu1iNX"
   },
   "source": [
    "## datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M2wTp6YV1hDR"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_params() -> dict:\n",
    "    params = {\"path data\": \"../Data\",\n",
    "              \"train data\": \"mex_train.txt\",\n",
    "              \"train labels\": \"mex_train_labels.txt\",\n",
    "              \"validation data\": \"mex_val.txt\",\n",
    "              \"validation labels\": \"mex_val_labels.txt\",\n",
    "              \"path model\":\"../Data/Practica\",\n",
    "              \"file model\":\"model_best.pt\",\n",
    "              \"stadistics file\":\"stadistics.csv\",\n",
    "              }\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_args() -> Namespace:\n",
    "    args = Namespace()\n",
    "    args.batch_size = 64\n",
    "    args.num_workers = 2\n",
    "    args.N = 4\n",
    "    # Dimension of word Embeddings\n",
    "    args.d = 100\n",
    "    # Dimension for Hidden Layer\n",
    "    args.d_h = 200\n",
    "    args.dropout = 0.1\n",
    "    # Training hyperparameters\n",
    "    args.lr = 2.3e-1\n",
    "    args.num_epochs = 100\n",
    "    args.patience = 20\n",
    "    # Scheduler hyperparameters\n",
    "    args.lr_patience = 10\n",
    "    args.lr_factor = 0.5\n",
    "    # Save directory\n",
    "    args.savedir = \"model\"\n",
    "    makedirs(args.savedir,\n",
    "             exist_ok=True)\n",
    "    return args\n",
    "\n",
    "\n",
    "def init_seeds() -> None:\n",
    "    seed = 1111\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7A_xys1ihw"
   },
   "source": [
    "### ngram_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XWztOREC1i2L"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from nltk import FreqDist, ngrams\n",
    "from numpy import array, empty\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "class ngram_model:\n",
    "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer = None, embeddings_model=None) -> None:\n",
    "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '!',\n",
    "                         '¡', '¿', '?', '\"', '\\'', '...', '<url>',\n",
    "                          '*', '@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.unk = '<unk>'\n",
    "        self.sos = '<s>'\n",
    "        self.eos = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def default_tokenize(self, doc: str) -> list:\n",
    "        return doc.split(\"  \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def sortFreqDisct(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "\n",
    "    def get_vocabulary(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([word.lower()\n",
    "                              for sentence in corpus\n",
    "                              for word in self.tokenize(sentence)\n",
    "                              if not self.remove_word(word)])\n",
    "        sorted_words = self.sortFreqDisct(freq_dist)\n",
    "        sorted_words = sorted_words[:self.vocab_max-3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocabulary = self.get_vocabulary(corpus)\n",
    "        self.vocabulary.add(self.unk)\n",
    "        self.vocabulary.add(self.sos)\n",
    "        self.vocabulary.add(self.eos)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embeddings_matrix = empty([self.get_vocabulary_size,\n",
    "                                            self.embeddings_model.vector_size])\n",
    "        self.make_data(corpus)\n",
    "\n",
    "    def make_data(self, corpus: str) -> tuple:\n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenize(doc):\n",
    "                word = word.lower()\n",
    "                if word in self.vocabulary and not word in self.word_index:\n",
    "                    self.word_index[word] = id\n",
    "                    self.index_word[id] = word\n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word in self.embeddings_model:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model[word]\n",
    "                        else:\n",
    "                            self.embeddings_matrix[id] = rand(\n",
    "                                self.embeddings_model.vector_size)\n",
    "                    id += 1\n",
    "        # Always add special tokens\n",
    "        self.word_index.update({\n",
    "            self.unk: id,\n",
    "            self.sos: id + 1,\n",
    "            self.eos: id + 2\n",
    "        })\n",
    "        self.index_word.update({\n",
    "            id: self.unk,\n",
    "            id + 1: self.sos,\n",
    "            id + 2: self.eos\n",
    "        })\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenize(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [word.lower() for word in doc_tokens]\n",
    "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocabulary:\n",
    "                doc_tokens[i] = self.unk\n",
    "        return doc_tokens\n",
    "\n",
    "    def transform(self, corpus: list) -> tuple:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.word_index[word]\n",
    "                                    for word in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return array(X_ngrams), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNT_w8m81jF2"
   },
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xVgI-FgD1joQ"
   },
   "outputs": [],
   "source": [
    "from numpy import array, mean, asanyarray, sum, exp, argmax,log\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import read_csv,DataFrame\n",
    "from numpy.random import multinomial\n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "from tabulate import tabulate\n",
    "from shutil import copyfile\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class Mex_data_class:\n",
    "    def __init__(self, params: dict, args: Namespace) -> None:\n",
    "        self.params = params\n",
    "        self.args = args\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"\n",
    "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
    "        \"\"\"\n",
    "        train_filename = join(self.params[\"path data\"],\n",
    "                              self.params[\"train data\"])\n",
    "        validation_filename = join(self.params[\"path data\"],\n",
    "                                   self.params[\"train data\"])\n",
    "        self.train_text = self.read_file(train_filename)\n",
    "        self.validation_text = self.read_file(validation_filename)\n",
    "\n",
    "    def read_file(self, filename: str) -> list:\n",
    "        data = read_csv(filename,\n",
    "                        engine=\"python\",\n",
    "                        sep=\"\\r\\n\",\n",
    "                        header=None)\n",
    "        data = list(data[0])\n",
    "        return data\n",
    "\n",
    "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
    "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
    "        self.validation_data, self.validation_labels = ngram.transform(\n",
    "            self.validation_text)\n",
    "\n",
    "    def obtain_loaders(self) -> None:\n",
    "        self.train_loader = obtain_loader(self.train_data,\n",
    "                                          self.train_labels,\n",
    "                                          self.args)\n",
    "        self.validation_loader = obtain_loader(self.validation_data,\n",
    "                                               self.validation_labels,\n",
    "                                               self.args)\n",
    "\n",
    "\n",
    "class neural_language_model(nn.Module):\n",
    "    def __init__(self, args, embeddings=None) -> None:\n",
    "        super(neural_language_model, self).__init__()\n",
    "        self.window_size = args.N-1\n",
    "        self.embeding_size = args.d\n",
    "        self.emb = nn.Embedding(args.vocabulary_size,\n",
    "                                args.d)\n",
    "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
    "                             args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h,\n",
    "                             args.vocabulary_size,\n",
    "                             bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size*self.embeding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "    def read_model(self, path: str, name: str) -> None:\n",
    "        filename = join(path, name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(filename,\n",
    "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "        self.train(False)\n",
    "\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
    "        self.validation_loader = validation_loader\n",
    "        self.train_loader = train_loader\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def get_pred(self, raw_logits):\n",
    "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "        y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def model_eval(self, data):\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            tgts = []\n",
    "            for window_words, labels in data:\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                outputs = self.model(window_words)\n",
    "                # Get prediction\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.numpy()\n",
    "                tgts.append(tgt)\n",
    "                preds.append(y_pred)\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        return accuracy_score(tgts, preds)\n",
    "\n",
    "    def save_checkpoint(self, state,\n",
    "                        is_best: bool,\n",
    "                        checkpoint_path: str,\n",
    "                        filename: str = 'checkpoint.pt',\n",
    "                        best_model_name: str = 'model_best.pt') -> None:\n",
    "        print(checkpoint_path, filename)\n",
    "        name = join(checkpoint_path,\n",
    "                    filename)\n",
    "        torch.save(state,\n",
    "                   name)\n",
    "        if is_best:\n",
    "            filename_best = join(checkpoint_path,\n",
    "                                 best_model_name)\n",
    "            copyfile(name,\n",
    "                     filename_best)\n",
    "\n",
    "    def run(self):\n",
    "        stadistics = DataFrame(\n",
    "            columns=[\"Train acc\", \"Loss\", \"Val acc\", \"Time\"])\n",
    "        start_time = time.time()\n",
    "        best_metric = 0\n",
    "        metric_history = []\n",
    "        train_metric_history = []\n",
    "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
    "                                                                 self.args)\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            loss_epoch = []\n",
    "            training_metric = []\n",
    "            self.model.train()\n",
    "            for window_words, labels in self.train_loader:\n",
    "                # If GPU available\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                # Forward pass\n",
    "                outputs = self.model(window_words)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_epoch.append(loss.item())\n",
    "                # Get Trainning Metrics\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.cpu().numpy()\n",
    "                training_metric.append(accuracy_score(tgt, y_pred))\n",
    "                # Backward and Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # Get Metric in Trainning Dataset\n",
    "            mean_epoch_metric = mean(training_metric)\n",
    "            train_metric_history.append(mean_epoch_metric)\n",
    "            # Get Metric in Validation Dataset\n",
    "            self.model.eval()\n",
    "            tuning_metric = self.model_eval(self.validation_loader)\n",
    "            metric_history.append(mean_epoch_metric)\n",
    "            # Update Scheduler\n",
    "            scheduler.step(tuning_metric)\n",
    "            # Check for Metric Improvement\n",
    "            is_improvement = tuning_metric > best_metric\n",
    "            if is_improvement:\n",
    "                best_metric = tuning_metric\n",
    "                n_no_improve = 0\n",
    "            else:\n",
    "                n_no_improve += 1\n",
    "            # Save best model if metric improved\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric, }\n",
    "            self.save_checkpoint(\n",
    "                state,\n",
    "                is_improvement,\n",
    "                self.args.savedir,\n",
    "            )\n",
    "            # Early stopping\n",
    "            if n_no_improve >= self.args.patience:\n",
    "                print('No improvement. Breaking out of loop')\n",
    "                break\n",
    "            finish_time = time.time() - epoch_start_time\n",
    "            stadistics.loc[epoch + 1] = [\n",
    "                mean_epoch_metric,\n",
    "                mean(loss_epoch), tuning_metric, finish_time\n",
    "            ]\n",
    "            print('Train acc: {}'.format(mean_epoch_metric))\n",
    "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
    "                epoch + 1,\n",
    "                self.args.num_epochs,\n",
    "                mean(loss_epoch),\n",
    "                tuning_metric,\n",
    "                finish_time))\n",
    "            print('--- %s seconds ---' % (time.time() - start_time))\n",
    "        return stadistics\n",
    "\n",
    "\n",
    "class generate_text_class:\n",
    "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
    "        self.ngram_data = ngram_data\n",
    "        self.tokenize = tokenize\n",
    "        self.model = model\n",
    "\n",
    "    def parse_text(self, text: str) -> tuple:\n",
    "        tokens = self.tokenize(text)\n",
    "        all_tokens = [word.lower()\n",
    "                      if word in self.ngram_data.word_index else self.ngram_data.unk\n",
    "                      for word in tokens]\n",
    "        tokens_id = [self.ngram_data.word_index[word.lower()]\n",
    "                     for word in all_tokens]\n",
    "        return tokens, tokens_id\n",
    "\n",
    "    def sample_next_word(self, logits, temperature: float):\n",
    "        logits = asanyarray(logits).astype(\"float64\")\n",
    "        preds = logits/temperature\n",
    "        exp_preds = exp(preds)\n",
    "        preds = exp_preds/sum(exp_preds)\n",
    "        probability = multinomial(1, preds)\n",
    "        return argmax(probability)\n",
    "\n",
    "    def predict_next_token(self, tokens_id):\n",
    "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
    "        y_raw_predict = self.model(\n",
    "            word_index_tensor).squeeze(0).detach().numpy()\n",
    "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
    "        return y_pred\n",
    "\n",
    "    def run(self, initial_text: str):\n",
    "        tokens, window_word_index = self.parse_text(initial_text)\n",
    "        for i in range(100):\n",
    "            y_pred = self.predict_next_token(window_word_index)\n",
    "            next_word = self.ngram_data.index_word[y_pred]\n",
    "            tokens.append(next_word)\n",
    "            if next_word == self.ngram_data.eos:\n",
    "                break\n",
    "            else:\n",
    "                window_word_index.pop(0)\n",
    "                window_word_index.append(y_pred)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
    "    args.use_gpu = torch.cuda.is_available()\n",
    "    if args.use_gpu:\n",
    "        model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           \"min\",\n",
    "                                                           patience=args.lr_patience,\n",
    "                                                           verbose=True,\n",
    "                                                           factor=args.lr_factor)\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "def print_closet_words(embeddings, ngram_data, word, n):\n",
    "    word_id = torch.LongTensor([ngram_data.word_index[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    # Compute distances to all words\n",
    "    dist = torch.norm(embeddings.weight-word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dist.numpy()),\n",
    "                 key=lambda x: x[1])\n",
    "    table = []\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        table += [[ngram_data.index_word[idx],\n",
    "                   difference]]\n",
    "    print(tabulate(table,\n",
    "                   headers=[\"Word\", \"Difference\"]))\n",
    "\n",
    "\n",
    "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
    "    dataset = TensorDataset(torch.tensor(data,\n",
    "                                         dtype=torch.int64),\n",
    "                            torch.tensor(labels,\n",
    "                                         dtype=torch.int64))\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
    "    x, y = ngram_data.transform(text)\n",
    "    x, y = x[2:], y[2:]\n",
    "    x = torch.LongTensor(x).unsqueeze(0)\n",
    "    logits = model(x).detach()\n",
    "    probability = F.softmax(logits, dim=1).numpy()\n",
    "    return sum(log([probability[i][w]\n",
    "                    for i, w in enumerate(y)]))\n",
    "\n",
    "def perplexity(model:neural_language_model, text:str, ngram_data:ngram_model) ->float:\n",
    "    perplexity_value = log_likelihood(model,text,ngram_data)\n",
    "    perplexity_value = - perplexity_value / len(text) \n",
    "    return perplexity_value\n",
    "  \n",
    "def save_stadistics(params: dict, stadistics: DataFrame) -> None:\n",
    "    filename = join(params[\"path data\"], params[\"stadistics file\"])\n",
    "    stadistics.index.name = \"Epoch\"\n",
    "    stadistics.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFjuqOpM1xvH",
    "outputId": "0bf72771-e425-4268-cc31-250090ff89ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura de archivos\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "\n",
    "init_seeds()\n",
    "params = get_params()\n",
    "args = get_args()\n",
    "tokenize = tokenizer().tokenize\n",
    "print(\"Lectura de archivos\")\n",
    "mex_data = Mex_data_class(params, args)\n",
    "ngram = ngram_model(args.N, tokenize=tokenize)\n",
    "ngram.fit(mex_data.train_text)\n",
    "args.vocabulary_size = ngram.get_vocabulary_size()\n",
    "mex_data.obtain_data_and_labels(ngram)\n",
    "mex_data.obtain_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9YDq0a33_i4",
    "outputId": "9bf8ba17-5589-4c9a-c0b0-b3faed4a55d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init neural model\n",
      "Train neural model\n"
     ]
    }
   ],
   "source": [
    "print(\"Init neural model\")\n",
    "neural_model = neural_language_model(args)\n",
    "model = model_class(neural_model,\n",
    "                    args,\n",
    "                    mex_data.train_loader,\n",
    "                    mex_data.validation_loader)\n",
    "print(\"Train neural model\")\n",
    "# stadistics=model.run()\n",
    "# save_stadistics(params,stadistics)\n",
    "neural_model.read_model(params[\"path model\"],\n",
    "                        params[\"file model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "P3V022_x4Ubk",
    "outputId": "05e06a55-7aaa-41fb-a60b-763d43f783ee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<s> <s> <s> pinche arbitro vete alv que no soy putos <unk> </s>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text=generate_text_class(ngram,\n",
    "                                  neural_model,\n",
    "                                  tokenize)\n",
    "generate_text.run(\"<s> <s> <s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9-TMSBdmjNL",
    "outputId": "4a222dd4-0cb3-4695-a2b8-35c0f0362fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -956.1721\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\",log_likelihood(neural_model,\n",
    "                                      \"Estamos en la clase de procesamiento de lenguaje\",\n",
    "                                      ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5A5FNtCxprNy",
    "outputId": "65b1359c-52d8-4ab0-a327-62778112c21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -1183.4691\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\",log_likelihood(neural_model,\n",
    "                                      \"Estamos procesamiento clase en la clase de natural lenguaje\",\n",
    "                                      ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4xrYIP2p4bf",
    "outputId": "daba0c97-7b34-48d4-d875-c35a1a7f2a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -1159.6027\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\",log_likelihood(neural_model,\n",
    "                                      \"la natural Estamos clase en de de lenaguuaje procesamiento\",\n",
    "                                      ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4r9DC2yp8a-",
    "outputId": "43c48345-b6e7-4c31-daf6-e0f8aaebeeec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "-555.5158 gano voy a la me sino chingada\n",
      "-555.5158 gano sino voy me la chingada a\n",
      "-555.5158 gano sino la voy me chingada a\n",
      "-555.5158 gano sino a voy me la chingada\n",
      "-555.5158 gano me voy sino chingada la a\n",
      "----------------------------------------\n",
      "-563.668 chingada a gano me la sino voy\n",
      "-563.668 chingada a gano la voy sino me\n",
      "-563.668 chingada a gano la sino voy me\n",
      "-563.668 chingada a gano la sino me voy\n",
      "-563.668 chingada a gano la me sino voy\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "word_list=\"sino gano me voy a la chingada\".split(\" \")\n",
    "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
    "best_log_likelihood=[(log_likelihood(neural_model,pharse,ngram),pharse)\n",
    "                        for pharse in perms]\n",
    "best_log_likelihood=sorted(best_log_likelihood,reverse=True)\n",
    "print(\"-\"*40)\n",
    "for p,i in best_log_likelihood[:5]:\n",
    "    print(p,i)\n",
    "print(\"-\"*40)\n",
    "for p,i in best_log_likelihood[-5:]:\n",
    "    print(p,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYRZVtCSqF-A",
    "outputId": "5f5969b5-2697-4609-f5aa-ba02bb90b0f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.21337538329726"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(neural_model,\n",
    "           mex_data.validation_text,\n",
    "           ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTDOyQ6SViRP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practica05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
