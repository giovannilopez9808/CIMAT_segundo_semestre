{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tymlRibfzFJA"
   },
   "source": [
    "## Tarea 05 - Giovanni Gamaliel López Padilla\n",
    "### Procesamiento de lenguaje natural\n",
    "#### Ejercicio 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo1o-tLu1iNX"
   },
   "source": [
    "## datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6354,
     "status": "ok",
     "timestamp": 1648331424204,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "M2wTp6YV1hDR"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_params() -> dict:\n",
    "    params = {\n",
    "        \"path data\": \"../Data\",\n",
    "        \"word2vec path\": \"../Data/word2vec\",\n",
    "        \"word2vec file\": \"word2vec_col.txt\",\n",
    "        \"train data\": \"mex_train.txt\",\n",
    "        \"train labels\": \"mex_train_labels.txt\",\n",
    "        \"validation data\": \"mex_val.txt\",\n",
    "        \"validation labels\": \"mex_val_labels.txt\",\n",
    "        \"path model\": \"../Data/Model_02\",\n",
    "        \"file model\": \"model_best.pt\",\n",
    "        \"stadistics  file\": \"stadistics.csv\",\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_args() -> Namespace:\n",
    "    args = Namespace()\n",
    "    args.batch_size = 64\n",
    "    args.num_workers = 2\n",
    "    args.N = 4\n",
    "    # Dimension of word Embeddings\n",
    "    args.d = 100\n",
    "    # Dimension for Hidden Layer\n",
    "    args.d_h = 200\n",
    "    args.dropout = 0.1\n",
    "    # Training hyperparameters\n",
    "    args.lr = 2.3e-1\n",
    "    args.num_epochs = 100\n",
    "    args.patience = 20\n",
    "    # Scheduler hyperparameters\n",
    "    args.lr_patience = 10\n",
    "    args.lr_factor = 0.5\n",
    "    # Save directory\n",
    "    args.savedir = \"model\"\n",
    "    makedirs(args.savedir,\n",
    "             exist_ok=True)\n",
    "    return args\n",
    "\n",
    "\n",
    "def init_seeds() -> None:\n",
    "    seed = 1111\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqCkLYu2noeK"
   },
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1648331424206,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "i91pQrJ-KV5i"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "class word2vec_class:\n",
    "    def __init__(self, params: dict) -> None:\n",
    "        self.params = params\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        filename = join(self.params[\"word2vec path\"],\n",
    "                        self.params[\"word2vec file\"])\n",
    "        data = read_csv(filename,\n",
    "                        sep=\" \",\n",
    "                        skiprows=1,\n",
    "                        header=0,\n",
    "                        index_col=0)\n",
    "        data = data.T\n",
    "        self.vector_size = len(data)\n",
    "        self.data = self.to_dict(data)\n",
    "\n",
    "    def to_dict(self, data: DataFrame) -> dict:\n",
    "        return data.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7A_xys1ihw"
   },
   "source": [
    "### ngram_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1346,
     "status": "ok",
     "timestamp": 1648331425540,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "XWztOREC1i2L"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from nltk import FreqDist, ngrams\n",
    "from numpy import array, empty\n",
    "\n",
    "\n",
    "class ngram_model:\n",
    "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer = None, embeddings_model: word2vec_class = None) -> None:\n",
    "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '!',\n",
    "                         '¡', '¿', '?', '\"', '\\'', '...', '<url>',\n",
    "                          '*', '@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.unk = '<unk>'\n",
    "        self.sos = '<s>'\n",
    "        self.eos = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def default_tokenize(self, doc: str) -> list:\n",
    "        return doc.split(\"  \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def sortFreqDisct(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "\n",
    "    def get_vocabulary(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([word.lower()\n",
    "                              for sentence in corpus\n",
    "                              for word in self.tokenize(sentence)\n",
    "                              if not self.remove_word(word)])\n",
    "        sorted_words = self.sortFreqDisct(freq_dist)[:self.vocab_max - 3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocabulary = self.get_vocabulary(corpus)\n",
    "        self.vocabulary.add(self.unk)\n",
    "        self.vocabulary.add(self.sos)\n",
    "        self.vocabulary.add(self.eos)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = empty([self.get_vocabulary_size(),\n",
    "                                           self.embeddings_model.vector_size])\n",
    "        self.make_data(corpus)\n",
    "\n",
    "    def make_data(self, corpus: str) -> tuple:\n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenize(doc):\n",
    "                word = word.lower()\n",
    "                if word in self.vocabulary and not word in self.word_index:\n",
    "                    self.word_index[word] = id\n",
    "                    self.index_word[id] = word\n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word in self.embeddings_model.data:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model.data[word]\n",
    "                    id += 1\n",
    "        # Always add special tokens\n",
    "        self.word_index.update({\n",
    "            self.unk: id,\n",
    "            self.sos: id + 1,\n",
    "            self.eos: id + 2\n",
    "        })\n",
    "        self.index_word.update({\n",
    "            id: self.unk,\n",
    "            id + 1: self.sos,\n",
    "            id + 2: self.eos\n",
    "        })\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenize(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [word.lower() for word in doc_tokens]\n",
    "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocabulary:\n",
    "                doc_tokens[i] = self.unk\n",
    "        return doc_tokens\n",
    "\n",
    "    def transform(self, corpus: list) -> tuple:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.word_index[word]\n",
    "                                    for word in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return array(X_ngrams), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNT_w8m81jF2"
   },
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1648331426650,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "xVgI-FgD1joQ"
   },
   "outputs": [],
   "source": [
    "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import DataFrame, read_csv\n",
    "from numpy.random import multinomial\n",
    "from itertools import permutations\n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "from tabulate import tabulate\n",
    "from shutil import copyfile\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class Mex_data_class:\n",
    "    def __init__(self, params: dict, args: Namespace) -> None:\n",
    "        self.params = params\n",
    "        self.args = args\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"\n",
    "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
    "        \"\"\"\n",
    "        train_filename = join(self.params[\"path data\"],\n",
    "                              self.params[\"train data\"])\n",
    "        validation_filename = join(self.params[\"path data\"],\n",
    "                                   self.params[\"train data\"])\n",
    "        self.train_text = self.read_file(train_filename)\n",
    "        self.validation_text = self.read_file(validation_filename)\n",
    "\n",
    "    def read_file(self, filename: str) -> list:\n",
    "        data = read_csv(filename,\n",
    "                        engine=\"python\",\n",
    "                        sep=\"\\r\\n\",\n",
    "                        header=None)\n",
    "        data = list(data[0])\n",
    "        return data\n",
    "\n",
    "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
    "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
    "        self.validation_data, self.validation_labels = ngram.transform(\n",
    "            self.validation_text)\n",
    "\n",
    "    def obtain_loaders(self) -> None:\n",
    "        self.train_loader = obtain_loader(self.train_data,\n",
    "                                          self.train_labels,\n",
    "                                          self.args)\n",
    "        self.validation_loader = obtain_loader(self.validation_data,\n",
    "                                               self.validation_labels,\n",
    "                                               self.args)\n",
    "\n",
    "\n",
    "class neural_language_model(nn.Module):\n",
    "    def __init__(self, args, embeddings: array = None) -> None:\n",
    "        super(neural_language_model, self).__init__()\n",
    "        self.window_size = args.N-1\n",
    "        self.embeding_size = args.d\n",
    "        self.emb = nn.Embedding(args.vocabulary_size,\n",
    "                                args.d)\n",
    "        if embeddings is not None:\n",
    "            for i in range(embeddings.shape[0]):\n",
    "                for j in range(embeddings.shape[1]):\n",
    "                    self.emb.weight.data[i][j] = embeddings[i][j]\n",
    "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
    "                             args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h,\n",
    "                             args.vocabulary_size,\n",
    "                             bias=False)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size*self.embeding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "    def read_model(self, path: str, name: str) -> None:\n",
    "        filename = join(path, name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(filename,\n",
    "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "        self.train(False)\n",
    "\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
    "        self.validation_loader = validation_loader\n",
    "        self.train_loader = train_loader\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def get_pred(self, raw_logits):\n",
    "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "        y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def model_eval(self, data):\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            tgts = []\n",
    "            for window_words, labels in data:\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                outputs = self.model(window_words)\n",
    "                # Get prediction\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.numpy()\n",
    "                tgts.append(tgt)\n",
    "                preds.append(y_pred)\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        return accuracy_score(tgts, preds)\n",
    "\n",
    "    def save_checkpoint(self, state,\n",
    "                        is_best: bool,\n",
    "                        checkpoint_path: str,\n",
    "                        filename: str = 'checkpoint.pt',\n",
    "                        best_model_name: str = 'model_best.pt') -> None:\n",
    "        print(checkpoint_path, filename)\n",
    "        name = join(checkpoint_path,\n",
    "                    filename)\n",
    "        torch.save(state,\n",
    "                   name)\n",
    "        if is_best:\n",
    "            filename_best = join(checkpoint_path,\n",
    "                                 best_model_name)\n",
    "            copyfile(name,\n",
    "                     filename_best)\n",
    "\n",
    "    def run(self):\n",
    "        stadistics = DataFrame(columns=[\"Train acc\",\n",
    "                                        \"Loss\",\n",
    "                                        \"Val acc\",\n",
    "                                        \"Time\"])\n",
    "        start_time = time.time()\n",
    "        best_metric = 0\n",
    "        metric_history = []\n",
    "        train_metric_history = []\n",
    "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
    "                                                                 self.args)\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            loss_epoch = []\n",
    "            training_metric = []\n",
    "            self.model.train()\n",
    "            for window_words, labels in self.train_loader:\n",
    "                # If GPU available\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                # Forward pass\n",
    "                outputs = self.model(window_words)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_epoch.append(loss.item())\n",
    "                # Get Trainning Metrics\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.cpu().numpy()\n",
    "                training_metric.append(accuracy_score(tgt, y_pred))\n",
    "                # Backward and Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # Get Metric in Trainning Dataset\n",
    "            mean_epoch_metric = mean(training_metric)\n",
    "            train_metric_history.append(mean_epoch_metric)\n",
    "            # Get Metric in Validation Dataset\n",
    "            self.model.eval()\n",
    "            tuning_metric = self.model_eval(self.validation_loader)\n",
    "            metric_history.append(mean_epoch_metric)\n",
    "            # Update Scheduler\n",
    "            scheduler.step(tuning_metric)\n",
    "            # Check for Metric Improvement\n",
    "            is_improvement = tuning_metric > best_metric\n",
    "            if is_improvement:\n",
    "                best_metric = tuning_metric\n",
    "                n_no_improve = 0\n",
    "            else:\n",
    "                n_no_improve += 1\n",
    "            # Save best model if metric improved\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric, }\n",
    "            self.save_checkpoint(\n",
    "                state,\n",
    "                is_improvement,\n",
    "                self.args.savedir,\n",
    "            )\n",
    "            # Early stopping\n",
    "            if n_no_improve >= self.args.patience:\n",
    "                print('No improvement. Breaking out of loop')\n",
    "                break\n",
    "            finish_time = time.time()-epoch_start_time\n",
    "            stadistics.loc[epoch+1] = [mean_epoch_metric,\n",
    "                                       mean(loss_epoch),\n",
    "                                       tuning_metric,\n",
    "                                       finish_time]\n",
    "            print('Train acc: {}'.format(mean_epoch_metric))\n",
    "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
    "                epoch + 1,\n",
    "                self.args.num_epochs,\n",
    "                mean(loss_epoch),\n",
    "                tuning_metric,\n",
    "                finish_time))\n",
    "            print('--- %s seconds ---' % (time.time() - start_time))\n",
    "        return stadistics\n",
    "\n",
    "\n",
    "class generate_text_class:\n",
    "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
    "        self.ngram_data = ngram_data\n",
    "        self.tokenize = tokenize\n",
    "        self.model = model\n",
    "\n",
    "    def parse_text(self, text: str) -> tuple:\n",
    "        all_tokens = [word.lower()\n",
    "                      if word in self.ngram_data.word_index else self.ngram_data.eos\n",
    "                      for word in self.tokenize(text)]\n",
    "        tokens_id = [self.ngram_data.word_index[word]\n",
    "                     for word in all_tokens]\n",
    "        return all_tokens, tokens_id\n",
    "\n",
    "    def sample_next_word(self, logits: array, temperature: float) -> int:\n",
    "        logits = asanyarray(logits).astype(\"float64\")\n",
    "        preds = logits/temperature\n",
    "        exp_preds = exp(preds)\n",
    "        preds = exp_preds/sum(exp_preds)\n",
    "        probability = multinomial(1, preds)\n",
    "        return argmax(probability)\n",
    "\n",
    "    def predict_next_token(self, tokens_id: list) -> int:\n",
    "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
    "        y_raw_predict = self.model(\n",
    "            word_index_tensor).squeeze(0).detach().numpy()\n",
    "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
    "        return y_pred\n",
    "\n",
    "    def run(self, initial_text: str):\n",
    "        tokens, window_word_index = self.parse_text(initial_text)\n",
    "        for i in range(300):\n",
    "            y_pred = self.predict_next_token(window_word_index)\n",
    "            next_word = self.ngram_data.index_word[y_pred]\n",
    "            tokens.append(next_word)\n",
    "            if next_word == self.ngram_data.eos:\n",
    "                break\n",
    "            else:\n",
    "                window_word_index.pop(0)\n",
    "                window_word_index.append(y_pred)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def obtain_closet_words(self, word: str, n: int) -> None:\n",
    "        print(\"Palabras cercanas a {}\".format(word))\n",
    "        word_id = torch.LongTensor([self.ngram_data.word_index[word]])\n",
    "        word_embed = self.model.emb(word_id)\n",
    "        # Compute distances to all words\n",
    "        dist = torch.norm(self.model.emb.weight-word_embed, dim=1).detach()\n",
    "        lst = sorted(enumerate(dist.numpy()),\n",
    "                     key=lambda x: x[1])\n",
    "        table = []\n",
    "        for idx, difference in lst[1:n+1]:\n",
    "            table += [[self.ngram_data.index_word[idx],\n",
    "                       difference]]\n",
    "        print(tabulate(table,\n",
    "                       headers=[\"Word\", \"Difference\"]))\n",
    "        print()\n",
    "\n",
    "\n",
    "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
    "    args.use_gpu = torch.cuda.is_available()\n",
    "    if args.use_gpu:\n",
    "        model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           \"min\",\n",
    "                                                           patience=args.lr_patience,\n",
    "                                                           verbose=True,\n",
    "                                                           factor=args.lr_factor)\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
    "    dataset = TensorDataset(torch.tensor(data,\n",
    "                                         dtype=torch.int64),\n",
    "                            torch.tensor(labels,\n",
    "                                         dtype=torch.int64))\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
    "    x, y = ngram_data.transform(text)\n",
    "    x, y = x[2:], y[2:]\n",
    "    x = torch.LongTensor(x).unsqueeze(0)\n",
    "    logits = model(x).detach()\n",
    "    probability = F.softmax(logits, dim=1).numpy()\n",
    "    return sum(log([probability[i][w]\n",
    "                    for i, w in enumerate(y)]))\n",
    "\n",
    "\n",
    "def perplexity(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
    "    perplexity_value = log_likelihood(model, text, ngram_data)\n",
    "    perplexity_value = - perplexity_value / len(text)\n",
    "    return perplexity_value\n",
    "\n",
    "\n",
    "def syntax_structure(model: neural_language_model, ngram_data: ngram_model, word: str, tokenize: tokenizer) -> None:\n",
    "    words = tokenize(word)\n",
    "    perms = [\" \".join(perm) for perm in permutations(words)]\n",
    "    best_log_likelihood = [(log_likelihood(model,\n",
    "                                           pharse,\n",
    "                                           ngram_data),\n",
    "                            pharse)\n",
    "                           for pharse in perms]\n",
    "    best_log_likelihood = sorted(best_log_likelihood, reverse=True)\n",
    "    headers = [\"Palabra\", \"Perplejidad\"]\n",
    "    print(\"-\"*40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[:5]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results,\n",
    "                   headers=headers))\n",
    "    print(\"-\"*40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[-5:]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results,\n",
    "                   headers=headers))\n",
    "\n",
    "\n",
    "def save_stadistics(params: dict, stadistics: DataFrame) -> None:\n",
    "    filename = join(params[\"path data\"],\n",
    "                    params[\"stadistics  file\"])\n",
    "    stadistics.index.name = \"Epoch\"\n",
    "    stadistics.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbgmQWnjx1lE"
   },
   "source": [
    "### Inicialización de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73496,
     "status": "ok",
     "timestamp": 1648331529771,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "oFjuqOpM1xvH",
    "outputId": "b5cef7bb-7633-4779-f573-d968342e1c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura de archivos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "# Semillas de las funciones aleatorias\n",
    "init_seeds()\n",
    "# Recoleccion de los parametros y argumentos\n",
    "params = get_params()\n",
    "args = get_args()\n",
    "# Definicion del tokenizer\n",
    "tokenize = tokenizer().tokenize\n",
    "print(\"Lectura de archivos\")\n",
    "# Lectura de los datos\n",
    "mex_data = Mex_data_class(params, args)\n",
    "# Lectura de word2vec embeddings\n",
    "word2vec = word2vec_class(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3561,
     "status": "ok",
     "timestamp": 1648331533324,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "x27lyw6GIYSG"
   },
   "outputs": [],
   "source": [
    "# Inicializacion del modelo de ngramas\n",
    "ngram = ngram_model(args.N,\n",
    "                    tokenize=tokenize,\n",
    "                    embeddings_model=word2vec)\n",
    "ngram.fit(mex_data.train_text)\n",
    "# Argumento del tamaño del vocabulario\n",
    "args.vocabulary_size = ngram.get_vocabulary_size()\n",
    "# # Estructuración de los datos para la red neuronal\n",
    "mex_data.obtain_data_and_labels(ngram)\n",
    "mex_data.obtain_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1648331534094,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "K9YDq0a33_i4"
   },
   "outputs": [],
   "source": [
    "# Inicializacion de la red neuronal\n",
    "neural_model = neural_language_model(args)\n",
    "# Inicializacion del modelo de prediccion\n",
    "model = model_class(neural_model,\n",
    "                    args,\n",
    "                    mex_data.train_loader,\n",
    "                    mex_data.validation_loader)\n",
    "# Entrenamiento de la neurona\n",
    "# stadistics=model.run()\n",
    "# save_stadistics(params,stadistics)\n",
    "# Lectura de los parametros de la red neuronal\n",
    "neural_model.read_model(params[\"path model\"],\n",
    "                        params[\"file model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8flRw3ISitfv"
   },
   "source": [
    "### Punto 1\n",
    "Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tomé en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto. Después de haber entrenado el modelo, recupere las 10 palabras más similares a tres palabras de su gusto dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1648331534096,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "tNXUaOBHitQC",
    "outputId": "2c3d2c84-50e8-4723-fd2c-490050200b50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras cercanas a pinche\n",
      "Word          Difference\n",
      "----------  ------------\n",
      "clave            11.1125\n",
      "paraguayos       11.1371\n",
      "atención         11.1394\n",
      "destino          11.156\n",
      "<unk>            11.2876\n",
      "lamentable       11.31\n",
      "malas            11.3124\n",
      "groserías        11.4708\n",
      "sano             11.4834\n",
      "han              11.4888\n",
      "\n",
      "Palabras cercanas a saludo\n",
      "Word           Difference\n",
      "-----------  ------------\n",
      "<unk>             11.555\n",
      "daca              11.6047\n",
      "estaríamos        11.6133\n",
      "celosa            11.8526\n",
      "tirarse           11.8577\n",
      "policía           11.8887\n",
      "diputado          11.9451\n",
      "vídeos            12.0213\n",
      "bastardo          12.0952\n",
      "despertaron       12.1416\n",
      "\n",
      "Palabras cercanas a verga\n",
      "Word        Difference\n",
      "--------  ------------\n",
      "valí           10.3502\n",
      "<unk>          10.4064\n",
      "entramos       10.5384\n",
      "déjense        10.5931\n",
      "borrar         10.7274\n",
      "cambiar        10.7583\n",
      "<s>            10.8519\n",
      "reventar       10.9437\n",
      "exista         10.9773\n",
      "putas          10.9777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text = generate_text_class(ngram,\n",
    "                                    neural_model,\n",
    "                                    tokenize)\n",
    "generate_text.obtain_closet_words(\"pinche\", 10)\n",
    "generate_text.obtain_closet_words(\"saludo\", 10)\n",
    "generate_text.obtain_closet_words(\"verga\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7jeTUQFypit"
   },
   "source": [
    "## Punto 2\n",
    "Ponga al modelo a generar texto a partir\n",
    "de tres secuencias de inicio de su gusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1648331534298,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "P3V022_x4Ubk",
    "outputId": "374625e2-896b-4a49-a85b-003f467dd419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Primer palabra\n",
      "hola como estas putos <unk> de mierda <unk> <unk> <unk> </s>\n",
      "----------------------------------------\n",
      "Segunda palabra\n",
      "espero me sigan la verga bebés denle <unk> nos vale verga <unk> no los dije solo les <unk> putos solo saben ganar seguidores hdp <unk> </s>\n",
      "----------------------------------------\n",
      "Tercera palabra\n",
      "<s> <s> me supera toda su ex con esas <unk> de <unk> ya desayuno <unk> de verga pero todos a mucho y <unk> o feminazi <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Primer palabra\")\n",
    "print(generate_text.run(\"hola como estas\"))\n",
    "print(\"-\"*40)\n",
    "print(\"Segunda palabra\")\n",
    "print(generate_text.run(\"espero me sigan\"))\n",
    "print(\"-\"*40)\n",
    "print(\"Tercera palabra\")\n",
    "print(generate_text.run(\"<s> <s> me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4JRuUEFytMJ"
   },
   "source": [
    "## Punto 3\n",
    "Escriba 5 ejemplos de oraciones y mídales el likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1648331534299,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "D9-TMSBdmjNL",
    "outputId": "515ae661-9350-4f25-c76a-b408724a8ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -344.99414\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"Dejalo que termine\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1648331534301,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "5A5FNtCxprNy",
    "outputId": "71ddd5ba-9e46-4208-a8a1-66e0d649152e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -846.5855\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"esperate a que tenga servicios, ya completos\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1648331534303,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "x4xrYIP2p4bf",
    "outputId": "b02b6454-b5d3-48b8-9a81-233f087ee3e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -624.24255\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"asi te ganas un chingo de gente\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1648331534304,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "KpJbHMQVo4OG",
    "outputId": "8ad1d5f3-15ae-45be-e777-18f3b4fb05e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -760.2761\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"eso que esten en redes con sus criticas\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1648331534487,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "gG9MqZDapBvl",
    "outputId": "2dc02c45-5a2f-4f15-f75b-1973e170fb6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -721.4949\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"unas tlayudas no le hacen daño a nadie\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfIvM8zny2ti"
   },
   "source": [
    "## Punto 4\n",
    "Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1648331534489,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "L4r9DC2yp8a-",
    "outputId": "85271f8f-0d35-40f2-ab66-14284effce68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Palabra                 Perplejidad\n",
      "--------------------  -------------\n",
      "lleva me la chingada       -385.685\n",
      "lleva la me chingada       -385.685\n",
      "lleva la chingada me       -385.685\n",
      "la me lleva chingada       -385.685\n",
      "la lleva me chingada       -385.685\n",
      "----------------------------------------\n",
      "Palabra                 Perplejidad\n",
      "--------------------  -------------\n",
      "chingada me la lleva       -393.073\n",
      "chingada lleva me la       -393.073\n",
      "chingada lleva la me       -393.073\n",
      "chingada la me lleva       -393.073\n",
      "chingada la lleva me       -393.073\n"
     ]
    }
   ],
   "source": [
    "word = \"me lleva la chingada\"\n",
    "syntax_structure(neural_model, ngram, word, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sERbz-ry_SF"
   },
   "source": [
    "## Punto 4\n",
    "Calcule la perplejidad del modelo sobre los datos val. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7595,
     "status": "ok",
     "timestamp": 1648331542076,
     "user": {
      "displayName": "Giovanni Gamaliel López Padilla",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjpT78ErFeRcEUuVuDTBmiHhfgx4VxadeE8s31p=s64",
      "userId": "00801951827653287620"
     },
     "user_tz": 360
    },
    "id": "MYRZVtCSqF-A",
    "outputId": "8a678924-5e27-4a70-dde7-f769b17b6672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.21337538329726"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(neural_model,\n",
    "           mex_data.validation_text,\n",
    "           ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xovw8vJTzJ1r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Problema_02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
