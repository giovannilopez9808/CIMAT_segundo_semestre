{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tarea 05 - Giovanni Gamaliel López Padilla\n",
        "### Procesamiento de lenguaje natural\n",
        "#### Ejercicio 01"
      ],
      "metadata": {
        "id": "tymlRibfzFJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datsets"
      ],
      "metadata": {
        "id": "uo1o-tLu1iNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from os import makedirs\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_params() -> dict:\n",
        "    params = {\n",
        "        \"path data\": \"/content/drive/MyDrive/data\",\n",
        "        \"word2vec path\": \"/content/drive/MyDrive/data\",\n",
        "        \"word2vec file\": \"word2vec_col.txt\",\n",
        "        \"train data\": \"mex_train.txt\",\n",
        "        \"train labels\": \"mex_train_labels.txt\",\n",
        "        \"validation data\": \"mex_val.txt\",\n",
        "        \"validation labels\": \"mex_val_labels.txt\",\n",
        "        \"model path\": \"/content/drive/MyDrive/data/model_2\",\n",
        "        \"file model\": \"model_best.pt\",\n",
        "    }\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_args() -> Namespace:\n",
        "    args = Namespace()\n",
        "    args.batch_size = 64\n",
        "    args.num_workers = 2\n",
        "    args.N = 4\n",
        "    # Dimension of word Embeddings\n",
        "    args.d = 100\n",
        "    # Dimension for Hidden Layer\n",
        "    args.d_h = 200\n",
        "    args.dropout = 0.1\n",
        "    # Training hyperparameters\n",
        "    args.lr = 2.3e-1\n",
        "    args.num_epochs = 100\n",
        "    args.patience = 20\n",
        "    # Scheduler hyperparameters\n",
        "    args.lr_patience = 10\n",
        "    args.lr_factor = 0.5\n",
        "    # Save directory\n",
        "    args.savedir = \"model\"\n",
        "    makedirs(args.savedir,\n",
        "             exist_ok=True)\n",
        "    return args\n",
        "\n",
        "\n",
        "def init_seeds() -> None:\n",
        "    seed = 1111\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "M2wTp6YV1hDR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec_model"
      ],
      "metadata": {
        "id": "MqCkLYu2noeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "class word2vec_class:\n",
        "    def __init__(self, params: dict) -> None:\n",
        "        self.params = params\n",
        "        self.read()\n",
        "\n",
        "    def read(self) -> None:\n",
        "        filename = join(self.params[\"word2vec path\"],\n",
        "                        self.params[\"word2vec file\"])\n",
        "        data = read_csv(filename,\n",
        "                        sep=\" \",\n",
        "                        skiprows=1,\n",
        "                        header=0,\n",
        "                        index_col=0)\n",
        "        data = data.T\n",
        "        self.vector_size=len(data)\n",
        "        self.data = self.to_dict(data)\n",
        "\n",
        "    def to_dict(self, data: DataFrame) -> dict:\n",
        "        return data.to_dict(\"list\")"
      ],
      "metadata": {
        "id": "i91pQrJ-KV5i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ngram_class"
      ],
      "metadata": {
        "id": "Tq7A_xys1ihw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from nltk import FreqDist, ngrams\n",
        "from numpy import array, empty\n",
        "\n",
        "\n",
        "class ngram_model:\n",
        "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer=None, embeddings_model:word2vec_class=None) -> None:\n",
        "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
        "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '!',\n",
        "                         '¡', '¿', '?', '\"', '\\'', '...', '<url>',\n",
        "                          '*', '@usuario'])\n",
        "        self.N = N\n",
        "        self.vocab_max = vocab_max\n",
        "        self.unk = '<unk>'\n",
        "        self.sos = '<s>'\n",
        "        self.eos = '</s>'\n",
        "        self.embeddings_model = embeddings_model\n",
        "\n",
        "    def get_vocabulary_size(self) -> int:\n",
        "        return len(self.vocabulary)\n",
        "\n",
        "    def default_tokenize(self, doc: str) -> list:\n",
        "        return doc.split(\"  \")\n",
        "\n",
        "    def remove_word(self, word: str) -> bool:\n",
        "        word = word.lower()\n",
        "        is_punct = word in self.punct\n",
        "        is_digit = word.isnumeric()\n",
        "        return is_punct or is_digit\n",
        "\n",
        "    def sortFreqDisct(self, freq_dist) -> list:\n",
        "        freq_dict = dict(freq_dist)\n",
        "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
        "\n",
        "    def get_vocabulary(self, corpus: list) -> set:\n",
        "        freq_dist = FreqDist([word.lower()\n",
        "                              for sentence in corpus\n",
        "                              for word in self.tokenize(sentence)\n",
        "                              if not self.remove_word(word)])\n",
        "        sorted_words = self.sortFreqDisct(freq_dist)[:self.vocab_max - 3]\n",
        "        return set(sorted_words)\n",
        "\n",
        "    def fit(self, corpus: list) -> None:\n",
        "        self.vocabulary = self.get_vocabulary(corpus)\n",
        "        self.vocabulary.add(self.unk)\n",
        "        self.vocabulary.add(self.sos)\n",
        "        self.vocabulary.add(self.eos)\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        if self.embeddings_model is not None:\n",
        "            self.embedding_matrix = empty([self.get_vocabulary_size(),\n",
        "                                            self.embeddings_model.vector_size])\n",
        "        self.make_data(corpus)\n",
        "\n",
        "    def make_data(self, corpus: str) -> tuple:\n",
        "        id = 0\n",
        "        for doc in corpus:\n",
        "            for word in self.tokenize(doc):\n",
        "                word = word.lower()\n",
        "                if word in self.vocabulary and not word in self.word_index:\n",
        "                    self.word_index[word] = id\n",
        "                    self.index_word[id] = word\n",
        "                    if self.embeddings_model is not None:\n",
        "                        if word in self.embeddings_model.data:\n",
        "                            self.embedding_matrix[id] = self.embeddings_model.data[word]\n",
        "                    id += 1\n",
        "        # Always add special tokens\n",
        "        self.word_index.update({\n",
        "            self.unk: id,\n",
        "            self.sos: id + 1,\n",
        "            self.eos: id + 2\n",
        "        })\n",
        "        self.index_word.update({\n",
        "            id: self.unk,\n",
        "            id + 1: self.sos,\n",
        "            id + 2: self.eos\n",
        "        })\n",
        "\n",
        "    def get_ngram_doc(self, doc: str) -> list:\n",
        "        doc_tokens = self.tokenize(doc)\n",
        "        doc_tokens = self.replace_unk(doc_tokens)\n",
        "        doc_tokens = [word.lower() for word in doc_tokens]\n",
        "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
        "        return list(ngrams(doc_tokens, self.N))\n",
        "\n",
        "    def replace_unk(self, doc_tokens: list) -> list:\n",
        "        for i, token in enumerate(doc_tokens):\n",
        "            if token.lower() not in self.vocabulary:\n",
        "                doc_tokens[i] = self.unk\n",
        "        return doc_tokens\n",
        "\n",
        "    def transform(self, corpus: list) -> tuple:\n",
        "        X_ngrams = []\n",
        "        y = []\n",
        "        for doc in corpus:\n",
        "            doc_ngram = self.get_ngram_doc(doc)\n",
        "            for words_window in doc_ngram:\n",
        "                words_window_ids = [self.word_index[word]\n",
        "                                    for word in words_window]\n",
        "                X_ngrams.append(list(words_window_ids[:-1]))\n",
        "                y.append(words_window_ids[-1])\n",
        "        return array(X_ngrams), array(y)"
      ],
      "metadata": {
        "id": "XWztOREC1i2L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "QNT_w8m81jF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy.random import multinomial\n",
        "from itertools import permutations\n",
        "import torch.nn.functional as F\n",
        "from argparse import Namespace\n",
        "from tabulate import tabulate\n",
        "from pandas import DataFrame, read_csv\n",
        "from shutil import copyfile\n",
        "from os.path import join\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "class Mex_data_class:\n",
        "    def __init__(self, params: dict, args: Namespace) -> None:\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.read()\n",
        "\n",
        "    def read(self) -> None:\n",
        "        \"\"\"\n",
        "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
        "        \"\"\"\n",
        "        train_filename = join(self.params[\"path data\"],\n",
        "                              self.params[\"train data\"])\n",
        "        validation_filename = join(self.params[\"path data\"],\n",
        "                                   self.params[\"train data\"])\n",
        "        self.train_text = self.read_file(train_filename)\n",
        "        self.validation_text = self.read_file(validation_filename)\n",
        "\n",
        "    def read_file(self, filename: str) -> list:\n",
        "        data = read_csv(filename,\n",
        "                        engine=\"python\",\n",
        "                        sep=\"\\r\\n\",\n",
        "                        header=None)\n",
        "        data = list(data[0])\n",
        "        return data\n",
        "\n",
        "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
        "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
        "        self.validation_data, self.validation_labels = ngram.transform(\n",
        "            self.validation_text)\n",
        "\n",
        "    def obtain_loaders(self) -> None:\n",
        "        self.train_loader = obtain_loader(self.train_data,\n",
        "                                          self.train_labels,\n",
        "                                          self.args)\n",
        "        self.validation_loader = obtain_loader(self.validation_data,\n",
        "                                               self.validation_labels,\n",
        "                                               self.args)\n",
        "\n",
        "\n",
        "class neural_language_model(nn.Module):\n",
        "    def __init__(self, args, embeddings: array = None) -> None:\n",
        "        super(neural_language_model, self).__init__()\n",
        "        self.window_size = args.N-1\n",
        "        self.embeding_size = args.d\n",
        "        self.emb = nn.Embedding(args.vocabulary_size,\n",
        "                                args.d)\n",
        "        if embeddings is not None:\n",
        "            for i in range(embeddings.shape[0]):\n",
        "                for j in range(embeddings.shape[1]):\n",
        "                    self.emb.weight.data[i][j] = embeddings[i][j]\n",
        "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
        "                             args.d_h)\n",
        "        self.drop1 = nn.Dropout(p=args.dropout)\n",
        "        self.fc2 = nn.Linear(args.d_h,\n",
        "                             args.vocabulary_size,\n",
        "                             bias=False)\n",
        "        self.args = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(-1, self.window_size*self.embeding_size)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = self.drop1(h)\n",
        "        return self.fc2(h)\n",
        "\n",
        "    def read_model(self, path: str, name: str) -> None:\n",
        "        filename = join(path, name)\n",
        "        if torch.cuda.is_available():\n",
        "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
        "        else:\n",
        "            self.load_state_dict(torch.load(filename,\n",
        "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
        "        self.train(False)\n",
        "\n",
        "\n",
        "class model_class:\n",
        "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
        "        self.validation_loader = validation_loader\n",
        "        self.train_loader = train_loader\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def get_pred(self, raw_logits):\n",
        "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
        "        y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "        return y_pred\n",
        "\n",
        "    def model_eval(self, data):\n",
        "        with torch.no_grad():\n",
        "            preds = []\n",
        "            tgts = []\n",
        "            for window_words, labels in data:\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                outputs = self.model(window_words)\n",
        "                # Get prediction\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.numpy()\n",
        "                tgts.append(tgt)\n",
        "                preds.append(y_pred)\n",
        "        tgts = [e for l in tgts for e in l]\n",
        "        preds = [e for l in preds for e in l]\n",
        "        return accuracy_score(tgts, preds)\n",
        "\n",
        "    def save_checkpoint(self, state,\n",
        "                        is_best: bool,\n",
        "                        checkpoint_path: str,\n",
        "                        filename: str = 'checkpoint.pt',\n",
        "                        best_model_name: str = 'model_best.pt') -> None:\n",
        "        print(checkpoint_path, filename)\n",
        "        name = join(checkpoint_path,\n",
        "                    filename)\n",
        "        torch.save(state,\n",
        "                   name)\n",
        "        if is_best:\n",
        "            filename_best = join(checkpoint_path,\n",
        "                                 best_model_name)\n",
        "            copyfile(name,\n",
        "                     filename_best)\n",
        "\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        best_metric = 0\n",
        "        metric_history = []\n",
        "        train_metric_history = []\n",
        "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
        "                                                                 self.args)\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            loss_epoch = []\n",
        "            training_metric = []\n",
        "            self.model.train()\n",
        "            for window_words, labels in self.train_loader:\n",
        "                # If GPU available\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                    labels = labels.cuda()\n",
        "                # Forward pass\n",
        "                outputs = self.model(window_words)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss_epoch.append(loss.item())\n",
        "                # Get Trainning Metrics\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.cpu().numpy()\n",
        "                training_metric.append(accuracy_score(tgt, y_pred))\n",
        "                # Backward and Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Get Metric in Trainning Dataset\n",
        "            mean_epoch_metric = mean(training_metric)\n",
        "            train_metric_history.append(mean_epoch_metric)\n",
        "            # Get Metric in Validation Dataset\n",
        "            self.model.eval()\n",
        "            tuning_metric = self.model_eval(self.validation_loader)\n",
        "            metric_history.append(mean_epoch_metric)\n",
        "            # Update Scheduler\n",
        "            scheduler.step(tuning_metric)\n",
        "            # Check for Metric Improvement\n",
        "            is_improvement = tuning_metric > best_metric\n",
        "            if is_improvement:\n",
        "                best_metric = tuning_metric\n",
        "                n_no_improve = 0\n",
        "            else:\n",
        "                n_no_improve += 1\n",
        "            # Save best model if metric improved\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': self.model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'best_metric': best_metric, }\n",
        "            self.save_checkpoint(\n",
        "                state,\n",
        "                is_improvement,\n",
        "                self.args.savedir,\n",
        "            )\n",
        "            # Early stopping\n",
        "            if n_no_improve >= self.args.patience:\n",
        "                print('No improvement. Breaking out of loop')\n",
        "                break\n",
        "            print('Train acc: {}'.format(mean_epoch_metric))\n",
        "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
        "                epoch + 1,\n",
        "                self.args.num_epochs,\n",
        "                mean(loss_epoch),\n",
        "                tuning_metric,\n",
        "                time.time() - epoch_start_time))\n",
        "            print('--- %s seconds ---' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "class generate_text_class:\n",
        "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
        "        self.ngram_data = ngram_data\n",
        "        self.tokenize = tokenize\n",
        "        self.model = model\n",
        "\n",
        "    def parse_text(self, text: str) -> tuple:\n",
        "        all_tokens = [word.lower()\n",
        "                      if word in self.ngram_data.word_index else self.ngram_data.eos\n",
        "                      for word in self.tokenize(text)]\n",
        "        tokens_id = [self.ngram_data.word_index[word]\n",
        "                     for word in all_tokens]\n",
        "        return all_tokens, tokens_id\n",
        "\n",
        "    def sample_next_word(self, logits: array, temperature: float) -> int:\n",
        "        logits = asanyarray(logits).astype(\"float64\")\n",
        "        preds = logits/temperature\n",
        "        exp_preds = exp(preds)\n",
        "        preds = exp_preds/sum(exp_preds)\n",
        "        probability = multinomial(1, preds)\n",
        "        return argmax(probability)\n",
        "\n",
        "    def predict_next_token(self, tokens_id: list) -> int:\n",
        "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
        "        y_raw_predict = self.model(\n",
        "            word_index_tensor).squeeze(0).detach().numpy()\n",
        "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
        "        return y_pred\n",
        "\n",
        "    def run(self, initial_text: str):\n",
        "        tokens, window_word_index = self.parse_text(initial_text)\n",
        "        for i in range(300):\n",
        "            y_pred = self.predict_next_token(window_word_index)\n",
        "            next_word = self.ngram_data.index_word[y_pred]\n",
        "            tokens.append(next_word)\n",
        "            if next_word == self.ngram_data.eos:\n",
        "                break\n",
        "            else:\n",
        "                window_word_index.pop(0)\n",
        "                window_word_index.append(y_pred)\n",
        "        return \" \".join(tokens)\n",
        "    \n",
        "    def obtain_closet_words(self,word:str, n:int) -> None:\n",
        "        print(\"Palabras cercanas a {}\".format(word))\n",
        "        word_id = torch.LongTensor([self.ngram_data.word_index[word]])\n",
        "        word_embed = self.model.emb(word_id)\n",
        "        # Compute distances to all words\n",
        "        dist = torch.norm(self.model.emb.weight-word_embed, dim=1).detach()\n",
        "        lst = sorted(enumerate(dist.numpy()),\n",
        "                    key=lambda x: x[1])\n",
        "        table = []\n",
        "        for idx, difference in lst[1:n+1]:\n",
        "            table += [[self.ngram_data.index_word[idx],\n",
        "                    difference]]\n",
        "        print(tabulate(table,\n",
        "                    headers=[\"Word\", \"Difference\"]))\n",
        "        print()\n",
        "\n",
        "\n",
        "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
        "    args.use_gpu = torch.cuda.is_available()\n",
        "    if args.use_gpu:\n",
        "        model.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                lr=args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           \"min\",\n",
        "                                                           patience=args.lr_patience,\n",
        "                                                           verbose=True,\n",
        "                                                           factor=args.lr_factor)\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "\n",
        "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
        "    dataset = TensorDataset(torch.tensor(data,\n",
        "                                         dtype=torch.int64),\n",
        "                            torch.tensor(labels,\n",
        "                                         dtype=torch.int64))\n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.num_workers,\n",
        "                        shuffle=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
        "    x, y = ngram_data.transform(text)\n",
        "    x, y = x[2:], y[2:]\n",
        "    x = torch.LongTensor(x).unsqueeze(0)\n",
        "    logits = model(x).detach()\n",
        "    probability = F.softmax(logits, dim=1).numpy()\n",
        "    return sum(log([probability[i][w]\n",
        "                    for i, w in enumerate(y)]))\n",
        "\n",
        "\n",
        "def perplexity(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
        "    perplexity_value = log_likelihood(model, text, ngram_data)\n",
        "    perplexity_value = - perplexity_value / len(text)\n",
        "    return perplexity_value\n",
        "\n",
        "\n",
        "def syntax_structure(model: neural_language_model, ngram_data: ngram_model, word: str, tokenize: tokenizer) -> None:\n",
        "    words = tokenize(word)\n",
        "    perms = [\" \".join(perm) for perm in permutations(words)]\n",
        "    best_log_likelihood = [(log_likelihood(model,\n",
        "                                           pharse,\n",
        "                                           ngram_data),\n",
        "                            pharse)\n",
        "                           for pharse in perms]\n",
        "    best_log_likelihood = sorted(best_log_likelihood, reverse=True)\n",
        "    headers = [\"Palabra\", \"Perplejidad\"]\n",
        "    print(\"-\"*40)\n",
        "    results = []\n",
        "    for p, i in best_log_likelihood[:5]:\n",
        "        results += [[i, p]]\n",
        "    print(tabulate(results,\n",
        "                   headers=headers))\n",
        "    print(\"-\"*40)\n",
        "    results = []\n",
        "    for p, i in best_log_likelihood[-5:]:\n",
        "        results += [[i, p]]\n",
        "    print(tabulate(results,\n",
        "                   headers=headers))"
      ],
      "metadata": {
        "id": "xVgI-FgD1joQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicialización de los modelos"
      ],
      "metadata": {
        "id": "DbgmQWnjx1lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "# Semillas de las funciones aleatorias\n",
        "init_seeds()\n",
        "# Recoleccion de los parametros y argumentos\n",
        "params = get_params()\n",
        "args = get_args()\n",
        "# Definicion del tokenizer\n",
        "tokenize = tokenizer().tokenize\n",
        "print(\"Lectura de archivos\")\n",
        "# Lectura de los datos\n",
        "mex_data = Mex_data_class(params, args)\n",
        "# Lectura de word2vec embeddings\n",
        "word2vec = word2vec_class(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFjuqOpM1xvH",
        "outputId": "85ad9986-4d04-4881-ffed-6064808472ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lectura de archivos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializacion del modelo de ngramas\n",
        "ngram = ngram_model(args.N, \n",
        "                    tokenize=tokenize,\n",
        "                    embeddings_model=word2vec)\n",
        "ngram.fit(mex_data.train_text)\n",
        "# Argumento del tamaño del vocabulario\n",
        "args.vocabulary_size = ngram.get_vocabulary_size()\n",
        "# # Estructuración de los datos para la red neuronal\n",
        "mex_data.obtain_data_and_labels(ngram)\n",
        "mex_data.obtain_loaders()"
      ],
      "metadata": {
        "id": "x27lyw6GIYSG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializacion de la red neuronal\n",
        "neural_model = neural_language_model(args)\n",
        "# Inicializacion del modelo de prediccion\n",
        "model = model_class(neural_model,\n",
        "                    args,\n",
        "                    mex_data.train_loader,\n",
        "                    mex_data.validation_loader)\n",
        "# Entrenamiento de la neurona\n",
        "# model.run()\n",
        "# Lectura de los parametros de la red neuronal\n",
        "neural_model.read_model(params[\"model path\"],\n",
        "                        params[\"file model\"])"
      ],
      "metadata": {
        "id": "K9YDq0a33_i4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 1\n",
        "Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tomé en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto. Después de haber entrenado el modelo, recupere las 10 palabras más similares a tres palabras de su gusto dadas."
      ],
      "metadata": {
        "id": "8flRw3ISitfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text=generate_text_class(ngram,\n",
        "                                  neural_model,\n",
        "                                  tokenize)\n",
        "generate_text.obtain_closet_words(\"pinche\",10)\n",
        "generate_text.obtain_closet_words(\"saludo\",10)\n",
        "generate_text.obtain_closet_words(\"verga\",10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNXUaOBHitQC",
        "outputId": "9aabfc68-4d93-4ad2-a7d2-2e2a5e6e368f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras cercanas a pinche\n",
            "Word          Difference\n",
            "----------  ------------\n",
            "clave            11.1125\n",
            "paraguayos       11.1371\n",
            "atención         11.1394\n",
            "destino          11.156\n",
            "<unk>            11.2876\n",
            "lamentable       11.31\n",
            "malas            11.3124\n",
            "groserías        11.4708\n",
            "sano             11.4834\n",
            "han              11.4888\n",
            "\n",
            "Palabras cercanas a saludo\n",
            "Word           Difference\n",
            "-----------  ------------\n",
            "<unk>             11.555\n",
            "daca              11.6047\n",
            "estaríamos        11.6133\n",
            "celosa            11.8526\n",
            "tirarse           11.8577\n",
            "policía           11.8887\n",
            "diputado          11.9451\n",
            "vídeos            12.0213\n",
            "bastardo          12.0952\n",
            "despertaron       12.1416\n",
            "\n",
            "Palabras cercanas a verga\n",
            "Word        Difference\n",
            "--------  ------------\n",
            "valí           10.3502\n",
            "<unk>          10.4064\n",
            "entramos       10.5384\n",
            "déjense        10.5931\n",
            "borrar         10.7274\n",
            "cambiar        10.7583\n",
            "<s>            10.8519\n",
            "reventar       10.9437\n",
            "exista         10.9773\n",
            "putas          10.9777\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 2\n",
        "Ponga al modelo a generar texto a partir\n",
        "de tres secuencias de inicio de su gusto."
      ],
      "metadata": {
        "id": "t7jeTUQFypit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\"*40)\n",
        "print(\"Primer palabra\")\n",
        "print(generate_text.run(\"hola como estas\"))\n",
        "print(\"-\"*40)\n",
        "print(\"Segunda palabra\")\n",
        "print(generate_text.run(\"espero me sigan\"))\n",
        "print(\"-\"*40)\n",
        "print(\"Tercera palabra\")\n",
        "print(generate_text.run(\"<s> <s> me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3V022_x4Ubk",
        "outputId": "8ea056b8-6d17-4879-913b-9e5bbc6fe15e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Primer palabra\n",
            "hola como estas y <unk> verga y los pizza hijos de sus putas madres </s>\n",
            "----------------------------------------\n",
            "Segunda palabra\n",
            "espero me sigan muy <unk> es </s>\n",
            "----------------------------------------\n",
            "Tercera palabra\n",
            "<s> <s> me encanta sentirme contigo <unk> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 3\n",
        "Escriba 5 ejemplos de oraciones y mídales el likelihood."
      ],
      "metadata": {
        "id": "c4JRuUEFytMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"Dejalo que termine\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9-TMSBdmjNL",
        "outputId": "e37496a9-f79a-4401-cc3f-1378ea9dc19f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -344.99414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"esperate a que tenga servicios, ya completos\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A5FNtCxprNy",
        "outputId": "c5bd43bd-bcac-4b39-bdd6-b5a562b3ff6c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -846.5855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"asi te ganas un chingo de gente\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4xrYIP2p4bf",
        "outputId": "e6a345cf-bb6c-4a17-ffb9-14d5dc704d77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -624.24255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"eso que esten en redes con sus criticas\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpJbHMQVo4OG",
        "outputId": "1a124c3d-7b57-459d-abc6-9488399c3e4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -760.2761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"unas tlayudas no le hacen daño a nadie\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9MqZDapBvl",
        "outputId": "0cd2b622-e682-478a-cb7f-8513188df3e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -721.4949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 4\n",
        "Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga."
      ],
      "metadata": {
        "id": "dfIvM8zny2ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"me lleva la chingada\"\n",
        "syntax_structure(neural_model,ngram,word,tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4r9DC2yp8a-",
        "outputId": "b620dba4-d486-4e62-e514-3150c6fcb2c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Palabra                 Perplejidad\n",
            "--------------------  -------------\n",
            "lleva me la chingada       -385.685\n",
            "lleva la me chingada       -385.685\n",
            "lleva la chingada me       -385.685\n",
            "la me lleva chingada       -385.685\n",
            "la lleva me chingada       -385.685\n",
            "----------------------------------------\n",
            "Palabra                 Perplejidad\n",
            "--------------------  -------------\n",
            "chingada me la lleva       -393.073\n",
            "chingada lleva me la       -393.073\n",
            "chingada lleva la me       -393.073\n",
            "chingada la me lleva       -393.073\n",
            "chingada la lleva me       -393.073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 4\n",
        "Calcule la perplejidad del modelo sobre los datos val. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados"
      ],
      "metadata": {
        "id": "5sERbz-ry_SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(neural_model,\n",
        "           mex_data.validation_text,\n",
        "           ngram)"
      ],
      "metadata": {
        "id": "MYRZVtCSqF-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6e6bae-a7d6-4116-86f2-0f2a1be53b7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42.21337538329726"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xovw8vJTzJ1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}