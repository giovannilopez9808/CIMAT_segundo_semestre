{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tymlRibfzFJA"
   },
   "source": [
    "## Tarea 05 - Giovanni Gamaliel López Padilla\n",
    "### Procesamiento de lenguaje natural\n",
    "#### Ejercicio 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo1o-tLu1iNX"
   },
   "source": [
    "## datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M2wTp6YV1hDR"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_params() -> dict:\n",
    "    params = {\n",
    "        \"path data\": \"../Data\",\n",
    "        \"train data\": \"mex_train.txt\",\n",
    "        \"train labels\": \"mex_train_labels.txt\",\n",
    "        \"validation data\": \"mex_val.txt\",\n",
    "        \"validation labels\": \"mex_val_labels.txt\",\n",
    "        \"path model\": \"../Data/Model_01\",\n",
    "        \"file model\": \"model_best.pt\",\n",
    "        \"stadistics  file\": \"stadistics.csv\",\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_args() -> Namespace:\n",
    "    args = Namespace()\n",
    "    args.batch_size = 64\n",
    "    args.num_workers = 2\n",
    "    args.N = 6\n",
    "    # Dimension of word Embeddings\n",
    "    args.d = 100\n",
    "    # Dimension for Hidden Layer\n",
    "    args.d_h = 200\n",
    "    args.dropout = 0.1\n",
    "    # Training hyperparameters\n",
    "    args.lr = 2.3e-1\n",
    "    args.num_epochs = 100\n",
    "    args.patience = 20\n",
    "    # Scheduler hyperparameters\n",
    "    args.lr_patience = 10\n",
    "    args.lr_factor = 0.5\n",
    "    # Save directory\n",
    "    args.savedir = \"model\"\n",
    "    makedirs(args.savedir, exist_ok=True)\n",
    "    return args\n",
    "\n",
    "\n",
    "def init_seeds() -> None:\n",
    "    seed = 1111\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7A_xys1ihw"
   },
   "source": [
    "### ngram_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XWztOREC1i2L"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from nltk import FreqDist, ngrams\n",
    "from numpy import array, empty\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "class ngram_model:\n",
    "\n",
    "    def __init__(self,\n",
    "                 N: int,\n",
    "                 vocab_max: int = 5000,\n",
    "                 tokenize: tokenizer = None,\n",
    "                 embeddings_model=None) -> None:\n",
    "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
    "        self.punct = set([\n",
    "            '.', ',', ';', ':', '-', '^', '»', '!', '¡', '¿', '?', '\"', '\\'',\n",
    "            '...', '<url>', '*', '@usuario'\n",
    "        ])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.unk = '<unk>'\n",
    "        self.sos = '<s>'\n",
    "        self.eos = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def default_tokenize(self, doc: str) -> list:\n",
    "        return doc.split(\"  \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def sortFreqDisct(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "\n",
    "    def get_vocabulary(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([\n",
    "            letter.lower() for sentence in corpus for letter in sentence\n",
    "            if not self.remove_word(letter)\n",
    "        ])\n",
    "        sorted_words = self.sortFreqDisct(freq_dist)\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocabulary = self.get_vocabulary(corpus)\n",
    "        self.vocabulary.add(self.unk)\n",
    "        self.vocabulary.add(self.sos)\n",
    "        self.vocabulary.add(self.eos)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embeddings_matrix = empty(\n",
    "                [self.get_vocabulary_size, self.embeddings_model.vector_size])\n",
    "        self.make_data(corpus)\n",
    "\n",
    "    def make_data(self, corpus: str) -> tuple:\n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in doc:\n",
    "                word = word.lower()\n",
    "                if word in self.vocabulary and not word in self.word_index:\n",
    "                    self.word_index[word] = id\n",
    "                    self.index_word[id] = word\n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word in self.embeddings_model:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model[\n",
    "                                word]\n",
    "                    id += 1\n",
    "        # Always add special tokens\n",
    "        self.word_index.update({\n",
    "            self.unk: id,\n",
    "            self.sos: id + 1,\n",
    "            self.eos: id + 2\n",
    "        })\n",
    "        self.index_word.update({\n",
    "            id: self.unk,\n",
    "            id + 1: self.sos,\n",
    "            id + 2: self.eos\n",
    "        })\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = list(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [word.lower() for word in doc_tokens]\n",
    "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocabulary:\n",
    "                doc_tokens[i] = self.unk\n",
    "        return doc_tokens\n",
    "\n",
    "    def transform(self, corpus: list) -> tuple:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [\n",
    "                    self.word_index[word] for word in words_window\n",
    "                ]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return array(X_ngrams), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNT_w8m81jF2"
   },
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xVgI-FgD1joQ"
   },
   "outputs": [],
   "source": [
    "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import read_csv, DataFrame\n",
    "from numpy.random import multinomial\n",
    "from itertools import permutations\n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "from tabulate import tabulate\n",
    "from shutil import copyfile\n",
    "from random import shuffle\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class Mex_data_class:\n",
    "\n",
    "    def __init__(self, params: dict, args: Namespace) -> None:\n",
    "        self.params = params\n",
    "        self.args = args\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"\n",
    "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
    "        \"\"\"\n",
    "        train_filename = join(self.params[\"path data\"],\n",
    "                              self.params[\"train data\"])\n",
    "        validation_filename = join(self.params[\"path data\"],\n",
    "                                   self.params[\"train data\"])\n",
    "        self.train_text = self.read_file(train_filename)\n",
    "        self.validation_text = self.read_file(validation_filename)\n",
    "\n",
    "    def read_file(self, filename: str) -> list:\n",
    "        data = read_csv(filename, engine=\"python\", sep=\"\\r\\n\", header=None)\n",
    "        data = list(data[0])\n",
    "        return data\n",
    "\n",
    "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
    "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
    "        self.validation_data, self.validation_labels = ngram.transform(\n",
    "            self.validation_text)\n",
    "\n",
    "    def obtain_loaders(self) -> None:\n",
    "        self.train_loader = obtain_loader(self.train_data, self.train_labels,\n",
    "                                          self.args)\n",
    "        self.validation_loader = obtain_loader(self.validation_data,\n",
    "                                               self.validation_labels,\n",
    "                                               self.args)\n",
    "\n",
    "\n",
    "class neural_language_model(nn.Module):\n",
    "\n",
    "    def __init__(self, args, embeddings=None) -> None:\n",
    "        super(neural_language_model, self).__init__()\n",
    "        self.window_size = args.N - 1\n",
    "        self.embeding_size = args.d\n",
    "        self.emb = nn.Embedding(args.vocabulary_size, args.d)\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocabulary_size, bias=False)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embeding_size)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "    def read_model(self, path: str, name: str) -> None:\n",
    "        filename = join(path, name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
    "        else:\n",
    "            self.load_state_dict(\n",
    "                torch.load(filename,\n",
    "                           map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "        self.train(False)\n",
    "\n",
    "\n",
    "class model_class:\n",
    "\n",
    "    def __init__(self, model: neural_language_model, args: Namespace,\n",
    "                 train_loader, validation_loader):\n",
    "        self.validation_loader = validation_loader\n",
    "        self.train_loader = train_loader\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def get_pred(self, raw_logits):\n",
    "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "        y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def model_eval(self, data):\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            tgts = []\n",
    "            for window_words, labels in data:\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                outputs = self.model(window_words)\n",
    "                # Get prediction\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.numpy()\n",
    "                tgts.append(tgt)\n",
    "                preds.append(y_pred)\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        return accuracy_score(tgts, preds)\n",
    "\n",
    "    def save_checkpoint(self,\n",
    "                        state,\n",
    "                        is_best: bool,\n",
    "                        checkpoint_path: str,\n",
    "                        filename: str = 'checkpoint.pt',\n",
    "                        best_model_name: str = 'model_best.pt') -> DataFrame:\n",
    "        print(checkpoint_path, filename)\n",
    "        name = join(checkpoint_path, filename)\n",
    "        torch.save(state, name)\n",
    "        if is_best:\n",
    "            filename_best = join(checkpoint_path, best_model_name)\n",
    "            copyfile(name, filename_best)\n",
    "\n",
    "    def run(self):\n",
    "        stadistics = DataFrame(\n",
    "            columns=[\"Train acc\", \"Loss\", \"Val acc\", \"Time\"])\n",
    "        start_time = time.time()\n",
    "        best_metric = 0\n",
    "        metric_history = []\n",
    "        train_metric_history = []\n",
    "        criterion, optimizer, scheduler = init_models_parameters(\n",
    "            self.model, self.args)\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            loss_epoch = []\n",
    "            training_metric = []\n",
    "            self.model.train()\n",
    "            for window_words, labels in self.train_loader:\n",
    "                # If GPU available\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                # Forward pass\n",
    "                outputs = self.model(window_words)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_epoch.append(loss.item())\n",
    "                # Get Trainning Metrics\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.cpu().numpy()\n",
    "                training_metric.append(accuracy_score(tgt, y_pred))\n",
    "                # Backward and Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # Get Metric in Trainning Dataset\n",
    "            mean_epoch_metric = mean(training_metric)\n",
    "            train_metric_history.append(mean_epoch_metric)\n",
    "            # Get Metric in Validation Dataset\n",
    "            self.model.eval()\n",
    "            tuning_metric = self.model_eval(self.validation_loader)\n",
    "            metric_history.append(mean_epoch_metric)\n",
    "            # Update Scheduler\n",
    "            scheduler.step(tuning_metric)\n",
    "            # Check for Metric Improvement\n",
    "            is_improvement = tuning_metric > best_metric\n",
    "            if is_improvement:\n",
    "                best_metric = tuning_metric\n",
    "                n_no_improve = 0\n",
    "            else:\n",
    "                n_no_improve += 1\n",
    "            # Save best model if metric improved\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric,\n",
    "            }\n",
    "            self.save_checkpoint(\n",
    "                state,\n",
    "                is_improvement,\n",
    "                self.args.savedir,\n",
    "            )\n",
    "            # Early stopping\n",
    "            if n_no_improve >= self.args.patience:\n",
    "                print('No improvement. Breaking out of loop')\n",
    "                break\n",
    "            finish_time = time.time() - epoch_start_time\n",
    "            stadistics.loc[epoch + 1] = [\n",
    "                mean_epoch_metric,\n",
    "                mean(loss_epoch), tuning_metric, finish_time\n",
    "            ]\n",
    "            print('Train acc: {}'.format(mean_epoch_metric))\n",
    "            print(\n",
    "                'Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'\n",
    "                .format(epoch + 1, self.args.num_epochs, mean(loss_epoch),\n",
    "                        tuning_metric, finish_time))\n",
    "            print('--- %s seconds ---' % (time.time() - start_time))\n",
    "        return stadistics\n",
    "\n",
    "\n",
    "class generate_text_class:\n",
    "\n",
    "    def __init__(self, ngram_data: ngram_model, model: neural_language_model,\n",
    "                 tokenize: tokenizer) -> None:\n",
    "        self.ngram_data = ngram_data\n",
    "        self.tokenize = tokenize\n",
    "        self.model = model\n",
    "\n",
    "    def parse_text(self, text: str) -> tuple:\n",
    "        tokens = self.tokenize(text)\n",
    "        all_tokens = []\n",
    "        for word in tokens:\n",
    "            if word == self.ngram_data.sos:\n",
    "                all_tokens += [word]\n",
    "                all_tokens += [\" \"]\n",
    "        # División entre dos  porque se estan añadiendo dos elementos por <s> encontrado\n",
    "        n = len(all_tokens) // 2\n",
    "        sentence = \" \".join(tokens[n:])\n",
    "        all_tokens += [\n",
    "            letter.lower()\n",
    "            if letter in self.ngram_data.word_index else self.ngram_data.unk\n",
    "            for letter in sentence\n",
    "        ]\n",
    "        tokens_id = [\n",
    "            self.ngram_data.word_index[letter] for letter in all_tokens\n",
    "        ]\n",
    "        return all_tokens, tokens_id\n",
    "\n",
    "    def sample_next_word(self, logits: array, temperature: float) -> int:\n",
    "        logits = asanyarray(logits).astype(\"float64\")\n",
    "        preds = logits / temperature\n",
    "        exp_preds = exp(preds)\n",
    "        preds = exp_preds / sum(exp_preds)\n",
    "        probability = multinomial(1, preds)\n",
    "        return argmax(probability)\n",
    "\n",
    "    def predict_next_token(self, tokens_id: list) -> int:\n",
    "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
    "        y_raw_predict = self.model(word_index_tensor).squeeze(\n",
    "            0).detach().numpy()\n",
    "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
    "        return y_pred\n",
    "\n",
    "    def run(self, initial_text: str):\n",
    "        tokens, window_word_index = self.parse_text(initial_text)\n",
    "        for i in range(300):\n",
    "            y_pred = self.predict_next_token(window_word_index)\n",
    "            next_word = self.ngram_data.index_word[y_pred]\n",
    "            tokens.append(next_word)\n",
    "            if next_word == self.ngram_data.eos:\n",
    "                break\n",
    "            else:\n",
    "                window_word_index.pop(0)\n",
    "                window_word_index.append(y_pred)\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "\n",
    "def init_models_parameters(model: neural_language_model,\n",
    "                           args: Namespace) -> tuple:\n",
    "    args.use_gpu = torch.cuda.is_available()\n",
    "    if args.use_gpu:\n",
    "        model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        \"min\",\n",
    "        patience=args.lr_patience,\n",
    "        verbose=True,\n",
    "        factor=args.lr_factor)\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "def print_closet_words(embeddings, ngram_data, word, n) -> None:\n",
    "    word_id = torch.LongTensor([ngram_data.word_index[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    # Compute distances to all words\n",
    "    dist = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dist.numpy()), key=lambda x: x[1])\n",
    "    table = []\n",
    "    for idx, difference in lst[1:n + 1]:\n",
    "        table += [[ngram_data.index_word[idx], difference]]\n",
    "    print(tabulate(table, headers=[\"Word\", \"Difference\"]))\n",
    "\n",
    "\n",
    "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
    "    dataset = TensorDataset(torch.tensor(data, dtype=torch.int64),\n",
    "                            torch.tensor(labels, dtype=torch.int64))\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def log_likelihood(model: neural_language_model, text: str,\n",
    "                   ngram_data: ngram_model) -> float:\n",
    "    x, y = ngram_data.transform(text)\n",
    "    x, y = x[2:], y[2:]\n",
    "    x = torch.LongTensor(x).unsqueeze(0)\n",
    "    logits = model(x).detach()\n",
    "    probability = F.softmax(logits, dim=1).numpy()\n",
    "    return sum(log([probability[i][w] for i, w in enumerate(y)]))\n",
    "\n",
    "\n",
    "def perplexity(model: neural_language_model, text: str,\n",
    "               ngram_data: ngram_model) -> float:\n",
    "    perplexity_value = log_likelihood(model, text, ngram_data)\n",
    "    perplexity_value = -perplexity_value / len(text)\n",
    "    return perplexity_value\n",
    "\n",
    "\n",
    "def syntax_structure(model: neural_language_model, ngram_data: ngram_model,\n",
    "                     word: str) -> None:\n",
    "    perms = [\"\".join(perm) for perm in permutations(word)]\n",
    "    best_log_likelihood = [(log_likelihood(model, pharse, ngram_data), pharse)\n",
    "                           for pharse in perms]\n",
    "    best_log_likelihood = sorted(best_log_likelihood, reverse=True)\n",
    "    headers = [\"Palabra\", \"Perplejidad\"]\n",
    "    print(\"-\" * 40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[:5]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results, headers=headers))\n",
    "    print(\"-\" * 40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[-5:]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results, headers=headers))\n",
    "\n",
    "\n",
    "def save_stadistics(params: dict, stadistics: DataFrame) -> None:\n",
    "    filename = join(params[\"path data\"], params[\"stadistics  file\"])\n",
    "    stadistics.index.name = \"Epoch\"\n",
    "    stadistics.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbgmQWnjx1lE"
   },
   "source": [
    "### Inicialización de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFjuqOpM1xvH",
    "outputId": "e8d76a2a-7a28-4563-fb0a-808144fea23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura de archivos\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "# Semillas de las funciones aleatorias\n",
    "init_seeds()\n",
    "# Recoleccion de los parametros y argumentos\n",
    "params = get_params()\n",
    "args = get_args()\n",
    "# Definicion del tokenizer\n",
    "tokenize = tokenizer().tokenize\n",
    "print(\"Lectura de archivos\")\n",
    "# Lectura de los datos\n",
    "mex_data = Mex_data_class(params, args)\n",
    "# Inicializacion del modelo de ngramas\n",
    "ngram = ngram_model(args.N, tokenize=tokenize)\n",
    "ngram.fit(mex_data.train_text)\n",
    "# Argumento del tamaño del vocabulario\n",
    "args.vocabulary_size = ngram.get_vocabulary_size()\n",
    "# Estructuración de los datos para la red neuronal\n",
    "mex_data.obtain_data_and_labels(ngram)\n",
    "mex_data.obtain_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K9YDq0a33_i4"
   },
   "outputs": [],
   "source": [
    "# Inicializacion de la red neuronal\n",
    "neural_model = neural_language_model(args)\n",
    "# Inicializacion del modelo de prediccion\n",
    "model = model_class(neural_model, args, mex_data.train_loader,\n",
    "                    mex_data.validation_loader)\n",
    "# Entrenamiento de la neurona\n",
    "# stadistics=model.run()\n",
    "# Guardado de las estadisticas de entrenamiento\n",
    "# save_stadistics(params,stadistics)\n",
    "# Lectura de los parametros de la red neuronal\n",
    "neural_model.read_model(params[\"path model\"], params[\"file model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3V022_x4Ubk",
    "outputId": "f49c42d0-dfab-495d-b26f-a3ac20193072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Primer palabra\n",
      "<s> holas puta pendeja en lo profertero de la verga<unk> cuando decirá paco caras <unk> conocubir a su madre #tuxglo</s>\n",
      "----------------------------------------\n",
      "Segunda palabra\n",
      "correna la primeran de mi capaz valer 😭💞osa loca de la mierda sus respeta estan</s>\n",
      "----------------------------------------\n",
      "Tercera palabra\n",
      "<s> <s> como me tamper pues que jajajajajajajaja loca<unk></s>\n"
     ]
    }
   ],
   "source": [
    "generate_text = generate_text_class(ngram, neural_model, tokenize)\n",
    "print(\"-\" * 40)\n",
    "print(\"Primer palabra\")\n",
    "print(generate_text.run(\"<s> hol\"))\n",
    "print(\"-\" * 40)\n",
    "print(\"Segunda palabra\")\n",
    "print(generate_text.run(\"corre\"))\n",
    "print(\"-\" * 40)\n",
    "print(\"Tercera palabra\")\n",
    "print(generate_text.run(\"<s> <s> c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4JRuUEFytMJ"
   },
   "source": [
    "## Punto 2\n",
    "Escriba 5 ejemplos de oraciones y mídales el likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9-TMSBdmjNL",
    "outputId": "caa655f7-8c0a-420f-f164-1c2dc4d9e6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -230.42316\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\",\n",
    "      log_likelihood(neural_model, \"Dejalo que termine\", ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5A5FNtCxprNy",
    "outputId": "ef229cca-6bf9-4301-e46c-f83972209960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -567.6274\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"log likelihood\",\n",
    "    log_likelihood(neural_model,\n",
    "                   \"esperate a que tenga servicios, ya completos\", ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4xrYIP2p4bf",
    "outputId": "46436f5c-9519-4780-f993-6e2f48b18a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -412.32114\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\",\n",
    "      log_likelihood(neural_model, \"asi te ganas un chingo de gente\", ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpJbHMQVo4OG",
    "outputId": "879edcf3-ea6d-4416-ce14-4f12e93023bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -527.38086\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"log likelihood\",\n",
    "    log_likelihood(neural_model, \"eso que esten en redes con sus criticas\",\n",
    "                   ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gG9MqZDapBvl",
    "outputId": "36807c67-9458-4c2c-e78c-fe21cf5bd883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -492.78888\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"log likelihood\",\n",
    "    log_likelihood(neural_model, \"unas tlayudas no le hacen daño a nadie\",\n",
    "                   ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfIvM8zny2ti"
   },
   "source": [
    "## Punto 3\n",
    "Escriba un ejemplo de estructura morfológica (permutaciones con caracteres) similar al de estructura sintáctica del profesor con 5 o más caracteres de su gusto (e.g., \"ando \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4r9DC2yp8a-",
    "outputId": "9eb6cd93-c16f-4a42-8071-5b4879623142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Palabra      Perplejidad\n",
      "---------  -------------\n",
      "jonedaa          -66.821\n",
      "jonedaa          -66.821\n",
      "joneada          -66.821\n",
      "joneada          -66.821\n",
      "joneaad          -66.821\n",
      "----------------------------------------\n",
      "Palabra      Perplejidad\n",
      "---------  -------------\n",
      "aadjoen         -70.3344\n",
      "aadjnoe         -70.3344\n",
      "aadjnoe         -70.3344\n",
      "aadjneo         -70.3344\n",
      "aadjneo         -70.3344\n"
     ]
    }
   ],
   "source": [
    "word = \"enojada\"\n",
    "syntax_structure(neural_model, ngram, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sERbz-ry_SF"
   },
   "source": [
    "## Punto 4\n",
    "Calcule la perplejidad del modelo sobre los datos val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYRZVtCSqF-A",
    "outputId": "d35d100c-c9a1-411d-e5fb-68c2ffd7a1ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.86564303751803"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(neural_model, mex_data.validation_text, ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Problema_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
