{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tarea 05 - Giovanni Gamaliel L√≥pez Padilla\n",
        "### Procesamiento de lenguaje natural\n",
        "#### Ejercicio 01"
      ],
      "metadata": {
        "id": "tymlRibfzFJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## datsets"
      ],
      "metadata": {
        "id": "uo1o-tLu1iNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from os import makedirs\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_params() -> dict:\n",
        "    params = {\"path data\": \"/content/drive/MyDrive/data\",\n",
        "              \"train data\": \"mex_train.txt\",\n",
        "              \"train labels\": \"mex_train_labels.txt\",\n",
        "              \"validation data\": \"mex_val.txt\",\n",
        "              \"validation labels\": \"mex_val_labels.txt\",\n",
        "              \"model path\":\"/content/drive/MyDrive/data/model_2\",\n",
        "              \"file model\":\"model_best.pt\",\n",
        "              }\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_args() -> Namespace:\n",
        "    args = Namespace()\n",
        "    args.batch_size = 64\n",
        "    args.num_workers = 2\n",
        "    args.N = 6\n",
        "    # Dimension of word Embeddings\n",
        "    args.d = 100\n",
        "    # Dimension for Hidden Layer\n",
        "    args.d_h = 200\n",
        "    args.dropout = 0.1\n",
        "    # Training hyperparameters\n",
        "    args.lr = 2.3e-1\n",
        "    args.num_epochs = 100\n",
        "    args.patience = 20\n",
        "    # Scheduler hyperparameters\n",
        "    args.lr_patience = 10\n",
        "    args.lr_factor = 0.5\n",
        "    # Save directory\n",
        "    args.savedir = \"model\"\n",
        "    makedirs(args.savedir,\n",
        "             exist_ok=True)\n",
        "    return args\n",
        "\n",
        "\n",
        "def init_seeds() -> None:\n",
        "    seed = 1111\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "M2wTp6YV1hDR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ngram_class"
      ],
      "metadata": {
        "id": "Tq7A_xys1ihw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from nltk import FreqDist, ngrams\n",
        "from numpy import array, empty\n",
        "from numpy.random import rand\n",
        "\n",
        "\n",
        "class ngram_model:\n",
        "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer = None, embeddings_model=None) -> None:\n",
        "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
        "        self.punct = set(['.', ',', ';', ':', '-', '^', '¬ª', '!',\n",
        "                         '¬°', '¬ø', '?', '\"', '\\'', '...', '<url>',\n",
        "                          '*', '@usuario'])\n",
        "        self.N = N\n",
        "        self.vocab_max = vocab_max\n",
        "        self.unk = '<unk>'\n",
        "        self.sos = '<s>'\n",
        "        self.eos = '</s>'\n",
        "        self.embeddings_model = embeddings_model\n",
        "\n",
        "    def get_vocabulary_size(self) -> int:\n",
        "        return len(self.vocabulary)\n",
        "\n",
        "    def default_tokenize(self, doc: str) -> list:\n",
        "        return doc.split(\"  \")\n",
        "\n",
        "    def remove_word(self, word: str) -> bool:\n",
        "        word = word.lower()\n",
        "        is_punct = word in self.punct\n",
        "        is_digit = word.isnumeric()\n",
        "        return is_punct or is_digit\n",
        "    \n",
        "    def sortFreqDisct(self, freq_dist) -> list:\n",
        "        freq_dict = dict(freq_dist)\n",
        "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
        "\n",
        "    def get_vocabulary(self, corpus: list) -> set:\n",
        "        freq_dist = FreqDist([letter.lower()\n",
        "                              for sentence in corpus\n",
        "                              for letter in sentence\n",
        "                              if not self.remove_word(letter)])\n",
        "        sorted_words = self.sortFreqDisct(freq_dist)\n",
        "        return set(sorted_words)\n",
        "\n",
        "    def fit(self, corpus: list) -> None:\n",
        "        self.vocabulary = self.get_vocabulary(corpus)\n",
        "        self.vocabulary.add(self.unk)\n",
        "        self.vocabulary.add(self.sos)\n",
        "        self.vocabulary.add(self.eos)\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        if self.embeddings_model is not None:\n",
        "            self.embeddings_matrix = empty([self.get_vocabulary_size,\n",
        "                                            self.embeddings_model.vector_size])\n",
        "        self.make_data(corpus)\n",
        "\n",
        "    def make_data(self, corpus: str) -> tuple:\n",
        "        id = 0\n",
        "        for doc in corpus:\n",
        "            for word in doc:\n",
        "                word = word.lower()\n",
        "                if word in self.vocabulary and not word in self.word_index:\n",
        "                    self.word_index[word] = id\n",
        "                    self.index_word[id] = word\n",
        "                    if self.embeddings_model is not None:\n",
        "                        if word in self.embeddings_model:\n",
        "                            self.embedding_matrix[id] = self.embeddings_model[word]\n",
        "                    id += 1\n",
        "        # Always add special tokens\n",
        "        self.word_index.update({\n",
        "            self.unk: id,\n",
        "            self.sos: id + 1,\n",
        "            self.eos: id + 2\n",
        "        })\n",
        "        self.index_word.update({\n",
        "            id: self.unk,\n",
        "            id + 1: self.sos,\n",
        "            id + 2: self.eos\n",
        "        })\n",
        "\n",
        "    def get_ngram_doc(self, doc: str) -> list:\n",
        "        doc_tokens = list(doc)\n",
        "        doc_tokens = self.replace_unk(doc_tokens)\n",
        "        doc_tokens = [word.lower() for word in doc_tokens]\n",
        "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
        "        return list(ngrams(doc_tokens, self.N))\n",
        "\n",
        "    def replace_unk(self, doc_tokens: list) -> list:\n",
        "        for i, token in enumerate(doc_tokens):\n",
        "            if token.lower() not in self.vocabulary:\n",
        "                doc_tokens[i] = self.unk\n",
        "        return doc_tokens\n",
        "\n",
        "    def transform(self, corpus: list) -> tuple:\n",
        "        X_ngrams = []\n",
        "        y = []\n",
        "        for doc in corpus:\n",
        "            doc_ngram = self.get_ngram_doc(doc)\n",
        "            for words_window in doc_ngram:\n",
        "                words_window_ids = [self.word_index[word]\n",
        "                                    for word in words_window]\n",
        "                X_ngrams.append(list(words_window_ids[:-1]))\n",
        "                y.append(words_window_ids[-1])\n",
        "        return array(X_ngrams), array(y)"
      ],
      "metadata": {
        "id": "XWztOREC1i2L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "QNT_w8m81jF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy.random import multinomial\n",
        "from itertools import permutations\n",
        "import torch.nn.functional as F\n",
        "from argparse import Namespace\n",
        "from tabulate import tabulate\n",
        "from pandas import read_csv\n",
        "from shutil import copyfile\n",
        "from random import shuffle\n",
        "from os.path import join\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "class Mex_data_class:\n",
        "    def __init__(self, params: dict, args: Namespace) -> None:\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.read()\n",
        "\n",
        "    def read(self) -> None:\n",
        "        \"\"\"\n",
        "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
        "        \"\"\"\n",
        "        train_filename = join(self.params[\"path data\"],\n",
        "                              self.params[\"train data\"])\n",
        "        validation_filename = join(self.params[\"path data\"],\n",
        "                                   self.params[\"train data\"])\n",
        "        self.train_text = self.read_file(train_filename)\n",
        "        self.validation_text = self.read_file(validation_filename)\n",
        "\n",
        "    def read_file(self, filename: str) -> list:\n",
        "        data = read_csv(filename,\n",
        "                        engine=\"python\",\n",
        "                        sep=\"\\r\\n\",\n",
        "                        header=None)\n",
        "        data = list(data[0])\n",
        "        return data\n",
        "\n",
        "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
        "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
        "        self.validation_data, self.validation_labels = ngram.transform(\n",
        "            self.validation_text)\n",
        "\n",
        "    def obtain_loaders(self) -> None:\n",
        "        self.train_loader = obtain_loader(self.train_data,\n",
        "                                          self.train_labels,\n",
        "                                          self.args)\n",
        "        self.validation_loader = obtain_loader(self.validation_data,\n",
        "                                               self.validation_labels,\n",
        "                                               self.args)\n",
        "\n",
        "\n",
        "class neural_language_model(nn.Module):\n",
        "    def __init__(self, args, embeddings=None) -> None:\n",
        "        super(neural_language_model, self).__init__()\n",
        "        self.window_size = args.N-1\n",
        "        self.embeding_size = args.d\n",
        "        self.emb = nn.Embedding(args.vocabulary_size,\n",
        "                                args.d)\n",
        "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
        "                             args.d_h)\n",
        "        self.drop1 = nn.Dropout(p=args.dropout)\n",
        "        self.fc2 = nn.Linear(args.d_h,\n",
        "                             args.vocabulary_size,\n",
        "                             bias=False)\n",
        "        self.args=args\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(-1, self.window_size*self.embeding_size)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = self.drop1(h)\n",
        "        return self.fc2(h)\n",
        "\n",
        "    def read_model(self, path: str, name: str) -> None:\n",
        "        filename = join(path, name)\n",
        "        if torch.cuda.is_available():\n",
        "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
        "        else:\n",
        "            self.load_state_dict(torch.load(filename,\n",
        "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
        "        self.train(False)\n",
        "\n",
        "\n",
        "class model_class:\n",
        "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
        "        self.validation_loader = validation_loader\n",
        "        self.train_loader = train_loader\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def get_pred(self, raw_logits):\n",
        "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
        "        y_pred = torch.argmax(probs, dim=1).numpy()\n",
        "        return y_pred\n",
        "\n",
        "    def model_eval(self, data):\n",
        "        with torch.no_grad():\n",
        "            preds = []\n",
        "            tgts = []\n",
        "            for window_words, labels in data:\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                outputs = self.model(window_words)\n",
        "                # Get prediction\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.numpy()\n",
        "                tgts.append(tgt)\n",
        "                preds.append(y_pred)\n",
        "        tgts = [e for l in tgts for e in l]\n",
        "        preds = [e for l in preds for e in l]\n",
        "        return accuracy_score(tgts, preds)\n",
        "\n",
        "    def save_checkpoint(self, state,\n",
        "                        is_best: bool,\n",
        "                        checkpoint_path: str,\n",
        "                        filename: str = 'checkpoint.pt',\n",
        "                        best_model_name: str = 'model_best.pt') -> None:\n",
        "        print(checkpoint_path, filename)\n",
        "        name = join(checkpoint_path,\n",
        "                    filename)\n",
        "        torch.save(state,\n",
        "                   name)\n",
        "        if is_best:\n",
        "            filename_best = join(checkpoint_path,\n",
        "                                 best_model_name)\n",
        "            copyfile(name,\n",
        "                     filename_best)\n",
        "\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        best_metric = 0\n",
        "        metric_history = []\n",
        "        train_metric_history = []\n",
        "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
        "                                                                 self.args)\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            loss_epoch = []\n",
        "            training_metric = []\n",
        "            self.model.train()\n",
        "            for window_words, labels in self.train_loader:\n",
        "                # If GPU available\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                    labels = labels.cuda()\n",
        "                # Forward pass\n",
        "                outputs = self.model(window_words)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss_epoch.append(loss.item())\n",
        "                # Get Trainning Metrics\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.cpu().numpy()\n",
        "                training_metric.append(accuracy_score(tgt, y_pred))\n",
        "                # Backward and Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Get Metric in Trainning Dataset\n",
        "            mean_epoch_metric = mean(training_metric)\n",
        "            train_metric_history.append(mean_epoch_metric)\n",
        "            # Get Metric in Validation Dataset\n",
        "            self.model.eval()\n",
        "            tuning_metric = self.model_eval(self.validation_loader)\n",
        "            metric_history.append(mean_epoch_metric)\n",
        "            # Update Scheduler\n",
        "            scheduler.step(tuning_metric)\n",
        "            # Check for Metric Improvement\n",
        "            is_improvement = tuning_metric > best_metric\n",
        "            if is_improvement:\n",
        "                best_metric = tuning_metric\n",
        "                n_no_improve = 0\n",
        "            else:\n",
        "                n_no_improve += 1\n",
        "            # Save best model if metric improved\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': self.model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'best_metric': best_metric, }\n",
        "            self.save_checkpoint(\n",
        "                state,\n",
        "                is_improvement,\n",
        "                self.args.savedir,\n",
        "            )\n",
        "            # Early stopping\n",
        "            if n_no_improve >= self.args.patience:\n",
        "                print('No improvement. Breaking out of loop')\n",
        "                break\n",
        "            print('Train acc: {}'.format(mean_epoch_metric))\n",
        "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
        "                epoch + 1,\n",
        "                self.args.num_epochs,\n",
        "                mean(loss_epoch),\n",
        "                tuning_metric,\n",
        "                time.time() - epoch_start_time))\n",
        "            print('--- %s seconds ---' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "class generate_text_class:\n",
        "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
        "        self.ngram_data = ngram_data\n",
        "        self.tokenize = tokenize\n",
        "        self.model = model\n",
        "\n",
        "    def parse_text(self, text: str) -> tuple:\n",
        "        tokens = self.tokenize(text)\n",
        "        all_tokens = []\n",
        "        for word in tokens:\n",
        "            if word == self.ngram_data.sos:\n",
        "                all_tokens += [word]\n",
        "                all_tokens += [\" \"]\n",
        "        # Divisi√≥n entre dos  porque se estan a√±adiendo dos elementos por <s> encontrado\n",
        "        n = len(all_tokens)//2\n",
        "        sentence = \" \".join(tokens[n:])\n",
        "        all_tokens += [letter.lower()\n",
        "                       if letter in self.ngram_data.word_index else self.ngram_data.unk\n",
        "                       for letter in sentence]\n",
        "        tokens_id = [self.ngram_data.word_index[letter]\n",
        "                     for letter in all_tokens]\n",
        "        return all_tokens, tokens_id\n",
        "\n",
        "    def sample_next_word(self, logits:array, temperature: float)->int:\n",
        "        logits = asanyarray(logits).astype(\"float64\")\n",
        "        preds = logits/temperature\n",
        "        exp_preds = exp(preds)\n",
        "        preds = exp_preds/sum(exp_preds)\n",
        "        probability = multinomial(1, preds)\n",
        "        return argmax(probability)\n",
        "\n",
        "    def predict_next_token(self, tokens_id:list)->int:\n",
        "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
        "        y_raw_predict = self.model(\n",
        "            word_index_tensor).squeeze(0).detach().numpy()\n",
        "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
        "        return y_pred\n",
        "\n",
        "    def run(self, initial_text: str):\n",
        "        tokens, window_word_index = self.parse_text(initial_text)\n",
        "        for i in range(300):\n",
        "            y_pred = self.predict_next_token(window_word_index)\n",
        "            next_word = self.ngram_data.index_word[y_pred]\n",
        "            tokens.append(next_word)\n",
        "            if next_word == self.ngram_data.eos:\n",
        "                break\n",
        "            else:\n",
        "                window_word_index.pop(0)\n",
        "                window_word_index.append(y_pred)\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "\n",
        "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
        "    args.use_gpu = torch.cuda.is_available()\n",
        "    if args.use_gpu:\n",
        "        model.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                lr=args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           \"min\",\n",
        "                                                           patience=args.lr_patience,\n",
        "                                                           verbose=True,\n",
        "                                                           factor=args.lr_factor)\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "\n",
        "def print_closet_words(embeddings, ngram_data, word, n)->None:\n",
        "    word_id = torch.LongTensor([ngram_data.word_index[word]])\n",
        "    word_embed = embeddings(word_id)\n",
        "    # Compute distances to all words\n",
        "    dist = torch.norm(embeddings.weight-word_embed, dim=1).detach()\n",
        "    lst = sorted(enumerate(dist.numpy()),\n",
        "                 key=lambda x: x[1])\n",
        "    table = []\n",
        "    for idx, difference in lst[1:n+1]:\n",
        "        table += [[ngram_data.index_word[idx],\n",
        "                   difference]]\n",
        "    print(tabulate(table,\n",
        "                   headers=[\"Word\", \"Difference\"]))\n",
        "\n",
        "\n",
        "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
        "    dataset = TensorDataset(torch.tensor(data,\n",
        "                                         dtype=torch.int64),\n",
        "                            torch.tensor(labels,\n",
        "                                         dtype=torch.int64))\n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.num_workers,\n",
        "                        shuffle=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
        "    x, y = ngram_data.transform(text)\n",
        "    x, y = x[2:], y[2:]\n",
        "    x = torch.LongTensor(x).unsqueeze(0)\n",
        "    logits = model(x).detach()\n",
        "    probability = F.softmax(logits, dim=1).numpy()\n",
        "    return sum(log([probability[i][w]\n",
        "                    for i, w in enumerate(y)]))\n",
        "\n",
        "def perplexity(model:neural_language_model, text:str, ngram_data:ngram_model) ->float:\n",
        "    perplexity_value = log_likelihood(model,text,ngram_data)\n",
        "    perplexity_value = - perplexity_value / len(text) \n",
        "    return perplexity_value\n",
        "\n",
        "def syntax_structure(model: neural_language_model, ngram_data: ngram_model, word: str)->None:\n",
        "    perms = [\"\".join(perm) for perm in permutations(word)]\n",
        "    best_log_likelihood=[(log_likelihood(model,pharse,ngram_data),pharse)\n",
        "                            for pharse in perms]\n",
        "    best_log_likelihood=sorted(best_log_likelihood,reverse=True)\n",
        "    headers=[\"Palabra\",\"Perplejidad\"]\n",
        "    print(\"-\"*40)\n",
        "    results=[]\n",
        "    for p,i in best_log_likelihood[:5]:\n",
        "        results+=[[i,p]]\n",
        "    print(tabulate(results,\n",
        "                   headers=headers))\n",
        "    print(\"-\"*40)\n",
        "    results=[]\n",
        "    for p,i in best_log_likelihood[-5:]:\n",
        "        results+=[[i,p]]\n",
        "    print(tabulate(results,\n",
        "                   headers=headers))"
      ],
      "metadata": {
        "id": "xVgI-FgD1joQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicializaci√≥n de los modelos"
      ],
      "metadata": {
        "id": "DbgmQWnjx1lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "# Semillas de las funciones aleatorias\n",
        "init_seeds()\n",
        "# Recoleccion de los parametros y argumentos\n",
        "params = get_params()\n",
        "args = get_args()\n",
        "# Definicion del tokenizer\n",
        "tokenize = tokenizer().tokenize\n",
        "print(\"Lectura de archivos\")\n",
        "# Lectura de los datos\n",
        "mex_data = Mex_data_class(params, args)\n",
        "# Inicializacion del modelo de ngramas\n",
        "ngram = ngram_model(args.N, tokenize=tokenize)\n",
        "ngram.fit(mex_data.train_text)\n",
        "# Argumento del tama√±o del vocabulario\n",
        "args.vocabulary_size = ngram.get_vocabulary_size()\n",
        "# Estructuraci√≥n de los datos para la red neuronal\n",
        "mex_data.obtain_data_and_labels(ngram)\n",
        "mex_data.obtain_loaders()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFjuqOpM1xvH",
        "outputId": "0e51420d-3cdf-4b8b-cd4a-0eeaa7493d2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lectura de archivos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializacion de la red neuronal\n",
        "neural_model = neural_language_model(args)\n",
        "# Inicializacion del modelo de prediccion\n",
        "model = model_class(neural_model,\n",
        "                    args,\n",
        "                    mex_data.train_loader,\n",
        "                    mex_data.validation_loader)\n",
        "# Entrenamiento de la neurona\n",
        "#model.run()\n",
        "# Lectura de los parametros de la red neuronal\n",
        "neural_model.read_model(params[\"model path\"],\n",
        "                        params[\"file model\"])"
      ],
      "metadata": {
        "id": "K9YDq0a33_i4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106152bf-04fa-41d4-ba20-2378b07f6d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init neural model\n",
            "Train neural model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 1\n",
        "Con base en la implementaci√≥n mostrada en clase, construya un modelo de lenguaje neuronal a nivel de car√°cter. Tom√© en cuenta secuencias de tama√±o 6 o m√°s para el modelo, es decir hasta 5 caracteres o m√°s en el contexto. Ponga al modelo a generar texto 3 veces, con un m√°ximo de 300 caracteres."
      ],
      "metadata": {
        "id": "t7jeTUQFypit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text=generate_text_class(ngram,\n",
        "                                  neural_model,\n",
        "                                  tokenize)\n",
        "print(\"-\"*40)\n",
        "print(\"Primer palabra\")\n",
        "print(generate_text.run(\"hello\"))\n",
        "print(\"-\"*40)\n",
        "print(\"Segunda palabra\")\n",
        "print(generate_text.run(\"corre\"))\n",
        "print(\"-\"*40)\n",
        "print(\"Tercera palabra\")\n",
        "print(generate_text.run(\"<s> <s> c\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3V022_x4Ubk",
        "outputId": "7ce6d131-ed13-48b1-b1c5-eb70d9394a47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Primer palabra\n",
            "hellos nuevos los putisio de calberlo marica de mid<unk> es un lucho musica seai boy canda<unk></s>\n",
            "----------------------------------------\n",
            "Segunda palabra\n",
            "correr que deaker sacablan vasticio que la verga a alguna dios fans x<unk> üòë @usuario rositiva y el bien quita de√±os su madre @usuario @usuario @usuario <unk>y si madrea como eso siente el tiemposo lesma√±ostabas que para mariconja enfersiera con la ver pamio un hijo y mejor porcadores ojetes hijo es lo que me ma\n",
            "----------------------------------------\n",
            "Tercera palabra\n",
            "<s> <s> cabaza y @usuario no lo huevo al paroerra si madre üòÇüòÇ</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 2\n",
        "Escriba 5 ejemplos de oraciones y m√≠dales el likelihood"
      ],
      "metadata": {
        "id": "c4JRuUEFytMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"Dejalo que termine\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9-TMSBdmjNL",
        "outputId": "d7fd116d-f594-4f0d-da43-d70d5f5f109b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -231.48743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"esperate a que tenga servicios, ya completos\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A5FNtCxprNy",
        "outputId": "ebb5c2be-f62b-436b-c491-7375fd473a3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -561.39307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"asi te ganas un chingo de gente\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4xrYIP2p4bf",
        "outputId": "4ed394f8-ca52-4e24-b011-fc4c53a406ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -409.9855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"eso que esten en redes con sus criticas\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpJbHMQVo4OG",
        "outputId": "ae0faca0-0ca6-4d42-dc2d-b29951ab8a74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -508.58026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"unas tlayudas no le hacen da√±o a nadie\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9MqZDapBvl",
        "outputId": "5ecfa742-bbbe-4f06-ba9d-0314e6870fd5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -497.09866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 3\n",
        "Escriba un ejemplo de estructura morfol√≥gica (permutaciones con caracteres) similar al de estructura sint√°ctica del profesor con 5 o m√°s caracteres de su gusto (e.g., \"ando \")"
      ],
      "metadata": {
        "id": "dfIvM8zny2ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word=\"enojada\"\n",
        "syntax_structure(neural_model,ngram,word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4r9DC2yp8a-",
        "outputId": "cc48766d-371c-4dae-f86f-4ab04962790a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Palabra      Perplejidad\n",
            "---------  -------------\n",
            "nojedaa         -72.7082\n",
            "nojedaa         -72.7082\n",
            "nojeada         -72.7082\n",
            "nojeada         -72.7082\n",
            "nojeaad         -72.7082\n",
            "----------------------------------------\n",
            "Palabra      Perplejidad\n",
            "---------  -------------\n",
            "aadenoj         -76.5028\n",
            "aadenjo         -76.5028\n",
            "aadenjo         -76.5028\n",
            "aadejno         -76.5028\n",
            "aadejno         -76.5028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 4\n",
        "Calcule la perplejidad del modelo sobre los datos val."
      ],
      "metadata": {
        "id": "5sERbz-ry_SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(neural_model,\n",
        "           mex_data.validation_text,\n",
        "           ngram)"
      ],
      "metadata": {
        "id": "MYRZVtCSqF-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dec5b31-0e58-41ac-f540-d337d328b123"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "133.76395652958152"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}