{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## datsets"
      ],
      "metadata": {
        "id": "uo1o-tLu1iNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from os import makedirs\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_params() -> dict:\n",
        "    params = {\"path data\": \"/content/drive/MyDrive/data\",\n",
        "              \"train data\": \"mex_train.txt\",\n",
        "              \"train labels\": \"mex_train_labels.txt\",\n",
        "              \"validation data\": \"mex_val.txt\",\n",
        "              \"validation labels\": \"mex_val_labels.txt\",\n",
        "              \"model path\":\"/content/drive/MyDrive/data/model_2\",\n",
        "              \"file model\":\"model_best.pt\",\n",
        "              }\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_args() -> Namespace:\n",
        "    args = Namespace()\n",
        "    args.batch_size = 64\n",
        "    args.num_workers = 2\n",
        "    args.N = 6\n",
        "    # Dimension of word Embeddings\n",
        "    args.d = 100\n",
        "    # Dimension for Hidden Layer\n",
        "    args.d_h = 200\n",
        "    args.dropout = 0.1\n",
        "    # Training hyperparameters\n",
        "    args.lr = 2.3e-1\n",
        "    args.num_epochs = 100\n",
        "    args.patience = 20\n",
        "    # Scheduler hyperparameters\n",
        "    args.lr_patience = 10\n",
        "    args.lr_factor = 0.5\n",
        "    # Save directory\n",
        "    args.savedir = \"model\"\n",
        "    makedirs(args.savedir,\n",
        "             exist_ok=True)\n",
        "    return args\n",
        "\n",
        "\n",
        "def init_seeds() -> None:\n",
        "    seed = 1111\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "M2wTp6YV1hDR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ngram_class"
      ],
      "metadata": {
        "id": "Tq7A_xys1ihw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from nltk import FreqDist, ngrams\n",
        "from numpy import array, empty\n",
        "from numpy.random import rand\n",
        "\n",
        "\n",
        "class ngram_model:\n",
        "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer = None, embeddings_model=None) -> None:\n",
        "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
        "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '!',\n",
        "                         '¡', '¿', '?', '\"', '\\'', '...', '<url>',\n",
        "                          '*', '@usuario'])\n",
        "        self.N = N\n",
        "        self.vocab_max = vocab_max\n",
        "        self.unk = '<unk>'\n",
        "        self.sos = '<s>'\n",
        "        self.eos = '</s>'\n",
        "        self.embeddings_model = embeddings_model\n",
        "\n",
        "    def get_vocabulary_size(self) -> int:\n",
        "        return len(self.vocabulary)\n",
        "\n",
        "    def default_tokenize(self, doc: str) -> list:\n",
        "        return doc.split(\"  \")\n",
        "\n",
        "    def remove_word(self, word: str) -> bool:\n",
        "        word = word.lower()\n",
        "        is_punct = word in self.punct\n",
        "        is_digit = word.isnumeric()\n",
        "        return is_punct or is_digit\n",
        "    \n",
        "    def sortFreqDisct(self, freq_dist) -> list:\n",
        "        freq_dict = dict(freq_dist)\n",
        "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
        "\n",
        "    def get_vocabulary(self, corpus: list) -> set:\n",
        "        freq_dist = FreqDist([letter.lower()\n",
        "                              for sentence in corpus\n",
        "                              for letter in sentence\n",
        "                              if not self.remove_word(letter)])\n",
        "        sorted_words = self.sortFreqDisct(freq_dist)\n",
        "        return set(sorted_words)\n",
        "\n",
        "    def fit(self, corpus: list) -> None:\n",
        "        self.vocabulary = self.get_vocabulary(corpus)\n",
        "        self.vocabulary.add(self.unk)\n",
        "        self.vocabulary.add(self.sos)\n",
        "        self.vocabulary.add(self.eos)\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        if self.embeddings_model is not None:\n",
        "            self.embeddings_matrix = empty([self.get_vocabulary_size,\n",
        "                                            self.embeddings_model.vector_size])\n",
        "        self.make_data(corpus)\n",
        "\n",
        "    def make_data(self, corpus: str) -> tuple:\n",
        "        id = 0\n",
        "        for doc in corpus:\n",
        "            for word in doc:\n",
        "                word = word.lower()\n",
        "                if word in self.vocabulary and not word in self.word_index:\n",
        "                    self.word_index[word] = id\n",
        "                    self.index_word[id] = word\n",
        "                    if self.embeddings_model is not None:\n",
        "                        if word in self.embeddings_model:\n",
        "                            self.embedding_matrix[id] = self.embeddings_model[word]\n",
        "                    id += 1\n",
        "        # Always add special tokens\n",
        "        self.word_index.update({\n",
        "            self.unk: id,\n",
        "            self.sos: id + 1,\n",
        "            self.eos: id + 2\n",
        "        })\n",
        "        self.index_word.update({\n",
        "            id: self.unk,\n",
        "            id + 1: self.sos,\n",
        "            id + 2: self.eos\n",
        "        })\n",
        "\n",
        "    def get_ngram_doc(self, doc: str) -> list:\n",
        "        doc_tokens = list(doc)\n",
        "        doc_tokens = self.replace_unk(doc_tokens)\n",
        "        doc_tokens = [word.lower() for word in doc_tokens]\n",
        "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
        "        return list(ngrams(doc_tokens, self.N))\n",
        "\n",
        "    def replace_unk(self, doc_tokens: list) -> list:\n",
        "        for i, token in enumerate(doc_tokens):\n",
        "            if token.lower() not in self.vocabulary:\n",
        "                doc_tokens[i] = self.unk\n",
        "        return doc_tokens\n",
        "\n",
        "    def transform(self, corpus: list) -> tuple:\n",
        "        X_ngrams = []\n",
        "        y = []\n",
        "        for doc in corpus:\n",
        "            doc_ngram = self.get_ngram_doc(doc)\n",
        "            for words_window in doc_ngram:\n",
        "                words_window_ids = [self.word_index[word]\n",
        "                                    for word in words_window]\n",
        "                X_ngrams.append(list(words_window_ids[:-1]))\n",
        "                y.append(words_window_ids[-1])\n",
        "        return array(X_ngrams), array(y)"
      ],
      "metadata": {
        "id": "XWztOREC1i2L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models"
      ],
      "metadata": {
        "id": "QNT_w8m81jF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy.random import multinomial\n",
        "import torch.nn.functional as F\n",
        "from argparse import Namespace\n",
        "from tabulate import tabulate\n",
        "from pandas import read_csv\n",
        "from shutil import copyfile\n",
        "from os.path import join\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "class Mex_data_class:\n",
        "    def __init__(self, params: dict, args: Namespace) -> None:\n",
        "        self.params = params\n",
        "        self.args = args\n",
        "        self.read()\n",
        "\n",
        "    def read(self) -> None:\n",
        "        \"\"\"\n",
        "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
        "        \"\"\"\n",
        "        train_filename = join(self.params[\"path data\"],\n",
        "                              self.params[\"train data\"])\n",
        "        validation_filename = join(self.params[\"path data\"],\n",
        "                                   self.params[\"train data\"])\n",
        "        self.train_data = self.read_file(train_filename)\n",
        "        self.validation_data = self.read_file(validation_filename)\n",
        "\n",
        "    def read_file(self, filename: str) -> list:\n",
        "        data = read_csv(filename,\n",
        "                        engine=\"python\",\n",
        "                        sep=\"\\r\\n\",\n",
        "                        header=None)\n",
        "        data = list(data[0])\n",
        "        return data\n",
        "\n",
        "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
        "        self.train_data, self.train_labels = ngram.transform(self.train_data)\n",
        "        self.validation_data, self.validation_labels = ngram.transform(\n",
        "            self.validation_data)\n",
        "\n",
        "    def obtain_loaders(self) -> None:\n",
        "        self.train_loader = obtain_loader(self.train_data,\n",
        "                                          self.train_labels,\n",
        "                                          self.args)\n",
        "        self.validation_loader = obtain_loader(self.validation_data,\n",
        "                                               self.validation_labels,\n",
        "                                               self.args)\n",
        "\n",
        "\n",
        "class neural_language_model(nn.Module):\n",
        "    def __init__(self, args, embeddings=None) -> None:\n",
        "        super(neural_language_model, self).__init__()\n",
        "        self.window_size = args.N-1\n",
        "        self.embeding_size = args.d\n",
        "        self.emb = nn.Embedding(args.vocabulary_size,\n",
        "                                args.d)\n",
        "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
        "                             args.d_h)\n",
        "        self.drop1 = nn.Dropout(p=args.dropout)\n",
        "        self.fc2 = nn.Linear(args.d_h,\n",
        "                             args.vocabulary_size,\n",
        "                             bias=False)\n",
        "        self.args=args\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(-1, self.window_size*self.embeding_size)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = self.drop1(h)\n",
        "        return self.fc2(h)\n",
        "\n",
        "    def read_model(self, path: str, name: str) -> None:\n",
        "        filename = join(path, name)\n",
        "        if torch.cuda.is_available():\n",
        "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
        "        else:\n",
        "            self.load_state_dict(torch.load(filename,\n",
        "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
        "        self.train(False)\n",
        "\n",
        "\n",
        "class model_class:\n",
        "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
        "        self.validation_loader = validation_loader\n",
        "        self.train_loader = train_loader\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def get_pred(self, raw_logits):\n",
        "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
        "        y_pred = torch.argmax(probs, dim=1).numpy()\n",
        "        return y_pred\n",
        "\n",
        "    def model_eval(self, data):\n",
        "        with torch.no_grad():\n",
        "            preds = []\n",
        "            tgts = []\n",
        "            for window_words, labels in data:\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                outputs = self.model(window_words)\n",
        "                # Get prediction\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.numpy()\n",
        "                tgts.append(tgt)\n",
        "                preds.append(y_pred)\n",
        "        tgts = [e for l in tgts for e in l]\n",
        "        preds = [e for l in preds for e in l]\n",
        "        return accuracy_score(tgts, preds)\n",
        "\n",
        "    def save_checkpoint(self, state,\n",
        "                        is_best: bool,\n",
        "                        checkpoint_path: str,\n",
        "                        filename: str = 'checkpoint.pt',\n",
        "                        best_model_name: str = 'model_best.pt') -> None:\n",
        "        print(checkpoint_path, filename)\n",
        "        name = join(checkpoint_path,\n",
        "                    filename)\n",
        "        torch.save(state,\n",
        "                   name)\n",
        "        if is_best:\n",
        "            filename_best = join(checkpoint_path,\n",
        "                                 best_model_name)\n",
        "            copyfile(name,\n",
        "                     filename_best)\n",
        "\n",
        "    def run(self):\n",
        "        start_time = time.time()\n",
        "        best_metric = 0\n",
        "        metric_history = []\n",
        "        train_metric_history = []\n",
        "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
        "                                                                 self.args)\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            loss_epoch = []\n",
        "            training_metric = []\n",
        "            self.model.train()\n",
        "            for window_words, labels in self.train_loader:\n",
        "                # If GPU available\n",
        "                if self.args.use_gpu:\n",
        "                    window_words = window_words.cuda()\n",
        "                    labels = labels.cuda()\n",
        "                # Forward pass\n",
        "                outputs = self.model(window_words)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss_epoch.append(loss.item())\n",
        "                # Get Trainning Metrics\n",
        "                y_pred = self.get_pred(outputs)\n",
        "                tgt = labels.cpu().numpy()\n",
        "                training_metric.append(accuracy_score(tgt, y_pred))\n",
        "                # Backward and Optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Get Metric in Trainning Dataset\n",
        "            mean_epoch_metric = mean(training_metric)\n",
        "            train_metric_history.append(mean_epoch_metric)\n",
        "            # Get Metric in Validation Dataset\n",
        "            self.model.eval()\n",
        "            tuning_metric = self.model_eval(self.validation_loader)\n",
        "            metric_history.append(mean_epoch_metric)\n",
        "            # Update Scheduler\n",
        "            scheduler.step(tuning_metric)\n",
        "            # Check for Metric Improvement\n",
        "            is_improvement = tuning_metric > best_metric\n",
        "            if is_improvement:\n",
        "                best_metric = tuning_metric\n",
        "                n_no_improve = 0\n",
        "            else:\n",
        "                n_no_improve += 1\n",
        "            # Save best model if metric improved\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': self.model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'best_metric': best_metric, }\n",
        "            self.save_checkpoint(\n",
        "                state,\n",
        "                is_improvement,\n",
        "                self.args.savedir,\n",
        "            )\n",
        "            # Early stopping\n",
        "            if n_no_improve >= self.args.patience:\n",
        "                print('No improvement. Breaking out of loop')\n",
        "                break\n",
        "            print('Train acc: {}'.format(mean_epoch_metric))\n",
        "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
        "                epoch + 1,\n",
        "                self.args.num_epochs,\n",
        "                mean(loss_epoch),\n",
        "                tuning_metric,\n",
        "                time.time() - epoch_start_time))\n",
        "            print('--- %s seconds ---' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "class generate_text_class:\n",
        "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
        "        self.ngram_data = ngram_data\n",
        "        self.tokenize = tokenize\n",
        "        self.model = model\n",
        "\n",
        "    def parse_text(self, text: str) -> tuple:\n",
        "        tokens = self.tokenize(text)\n",
        "        all_tokens = []\n",
        "        for word in tokens:\n",
        "            if word == self.ngram_data.sos:\n",
        "                all_tokens += [word]\n",
        "                all_tokens += [\" \"]\n",
        "        # División entre dos  porque se estan añadiendo dos elementos por <s> encontrado\n",
        "        n = len(all_tokens)//2\n",
        "        sentence = \" \".join(tokens[n:])\n",
        "        all_tokens += [letter.lower()\n",
        "                       if letter in self.ngram_data.word_index else self.ngram_data.unk\n",
        "                       for letter in sentence]\n",
        "        tokens_id = [self.ngram_data.word_index[letter]\n",
        "                     for letter in all_tokens]\n",
        "        return all_tokens, tokens_id\n",
        "\n",
        "    def sample_next_word(self, logits, temperature: float):\n",
        "        logits = asanyarray(logits).astype(\"float64\")\n",
        "        preds = logits/temperature\n",
        "        exp_preds = exp(preds)\n",
        "        preds = exp_preds/sum(exp_preds)\n",
        "        probability = multinomial(1, preds)\n",
        "        return argmax(probability)\n",
        "\n",
        "    def predict_next_token(self, tokens_id):\n",
        "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
        "        y_raw_predict = self.model(\n",
        "            word_index_tensor).squeeze(0).detach().numpy()\n",
        "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
        "        return y_pred\n",
        "\n",
        "    def run(self, initial_text: str):\n",
        "        tokens, window_word_index = self.parse_text(initial_text)\n",
        "        for i in range(300):\n",
        "            y_pred = self.predict_next_token(window_word_index)\n",
        "            next_word = self.ngram_data.index_word[y_pred]\n",
        "            tokens.append(next_word)\n",
        "            if next_word == self.ngram_data.eos:\n",
        "                break\n",
        "            else:\n",
        "                window_word_index.pop(0)\n",
        "                window_word_index.append(y_pred)\n",
        "        return \"\".join(tokens)\n",
        "\n",
        "\n",
        "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
        "    args.use_gpu = torch.cuda.is_available()\n",
        "    if args.use_gpu:\n",
        "        model.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                lr=args.lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                           \"min\",\n",
        "                                                           patience=args.lr_patience,\n",
        "                                                           verbose=True,\n",
        "                                                           factor=args.lr_factor)\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "\n",
        "def print_closet_words(embeddings, ngram_data, word, n):\n",
        "    word_id = torch.LongTensor([ngram_data.word_index[word]])\n",
        "    word_embed = embeddings(word_id)\n",
        "    # Compute distances to all words\n",
        "    dist = torch.norm(embeddings.weight-word_embed, dim=1).detach()\n",
        "    lst = sorted(enumerate(dist.numpy()),\n",
        "                 key=lambda x: x[1])\n",
        "    table = []\n",
        "    for idx, difference in lst[1:n+1]:\n",
        "        table += [[ngram_data.index_word[idx],\n",
        "                   difference]]\n",
        "    print(tabulate(table,\n",
        "                   headers=[\"Word\", \"Difference\"]))\n",
        "\n",
        "\n",
        "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
        "    dataset = TensorDataset(torch.tensor(data,\n",
        "                                         dtype=torch.int64),\n",
        "                            torch.tensor(labels,\n",
        "                                         dtype=torch.int64))\n",
        "    loader = DataLoader(dataset,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.num_workers,\n",
        "                        shuffle=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
        "    x, y = ngram_data.transform(text)\n",
        "    x, y = x[2:], y[2:]\n",
        "    x = torch.LongTensor(x).unsqueeze(0)\n",
        "    logits = model(x).detach()\n",
        "    probability = F.softmax(logits, dim=1).numpy()\n",
        "    return sum(log([probability[i][w]\n",
        "                    for i, w in enumerate(y)]))\n"
      ],
      "metadata": {
        "id": "xVgI-FgD1joQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer as tokenizer\n",
        "\n",
        "init_seeds()\n",
        "params = get_params()\n",
        "args = get_args()\n",
        "tokenize = tokenizer().tokenize\n",
        "print(\"Lectura de archivos\")\n",
        "mex_data = Mex_data_class(params, args)\n",
        "ngram = ngram_model(args.N, tokenize=tokenize)\n",
        "ngram.fit(mex_data.train_data)\n",
        "args.vocabulary_size = ngram.get_vocabulary_size()\n",
        "mex_data.obtain_data_and_labels(ngram)\n",
        "mex_data.obtain_loaders()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFjuqOpM1xvH",
        "outputId": "c27a6b15-4b5b-4e4e-80bd-9c7c4a174478"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lectura de archivos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Init neural model\")\n",
        "neural_model = neural_language_model(args)\n",
        "model = model_class(neural_model,\n",
        "                    args,\n",
        "                    mex_data.train_loader,\n",
        "                    mex_data.validation_loader)\n",
        "print(\"Train neural model\")\n",
        "#model.run()\n",
        "neural_model.read_model(params[\"model path\"],\n",
        "                        params[\"file model\"])"
      ],
      "metadata": {
        "id": "K9YDq0a33_i4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954983b7-7de1-4258-d7e9-23f3b74ae7be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init neural model\n",
            "Train neural model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text=generate_text_class(ngram,\n",
        "                                  neural_model,\n",
        "                                  tokenize)\n",
        "generate_text.run(\"pende\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P3V022_x4Ubk",
        "outputId": "29810208-e334-48b7-929b-97d7801041b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pendejo que cuandesmañar</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"Estamos en la clase de procesamiento de lenguaje\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9-TMSBdmjNL",
        "outputId": "a6ae1f12-6301-412e-ba4d-9a93840f4d21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -624.56274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"Estamos procesamiento clase en la clase de natural lenguaje\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A5FNtCxprNy",
        "outputId": "c8c0f60e-994f-457f-b47c-cd1eaf00b54a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -765.17914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"log likelihood\",log_likelihood(neural_model,\n",
        "                                      \"la natural Estamos clase en de de lenaguaje procesamiento\",\n",
        "                                      ngram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4xrYIP2p4bf",
        "outputId": "56ccb796-c4b0-46b5-f2b0-ec5c9df07ac7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log likelihood -743.9948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import permutations\n",
        "from random import shuffle\n",
        "word_list=\"sino gano me voy a la chingada\".split(\" \")\n",
        "perms = [\" \".join(perm) for perm in permutations(word_list)]\n",
        "best_log_likelihood=[(log_likelihood(neural_model,pharse,ngram),pharse)\n",
        "                        for pharse in perms]\n",
        "best_log_likelihood=sorted(best_log_likelihood,reverse=True)\n",
        "print(\"-\"*40)\n",
        "for p,i in best_log_likelihood[:5]:\n",
        "    print(p,i)\n",
        "print(\"-\"*40)\n",
        "for p,i in best_log_likelihood[-5:]:\n",
        "    print(p,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4r9DC2yp8a-",
        "outputId": "943f7f9c-eef4-4509-9a58-4f031a06cc51"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "-379.93832 me sino la a chingada gano voy\n",
            "-379.93832 me la sino a chingada gano voy\n",
            "-379.93832 me la gano a chingada sino voy\n",
            "-379.93832 me la a sino gano chingada voy\n",
            "-379.93835 me voy sino la chingada gano a\n",
            "----------------------------------------\n",
            "-384.52045 voy a chingada la me sino gano\n",
            "-384.52045 voy a chingada la me gano sino\n",
            "-384.52045 voy a chingada gano sino me la\n",
            "-384.52045 voy a chingada gano la me sino\n",
            "-384.52048 voy chingada gano sino la me a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MYRZVtCSqF-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}