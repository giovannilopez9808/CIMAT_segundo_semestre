{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tymlRibfzFJA"
   },
   "source": [
    "## Tarea 05 - Giovanni Gamaliel López Padilla\n",
    "### Procesamiento de lenguaje natural\n",
    "#### Ejercicio 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo1o-tLu1iNX"
   },
   "source": [
    "## datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2wTp6YV1hDR"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from os import makedirs\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_params() -> dict:\n",
    "    params = {\n",
    "        \"path data\": \"../Data\",\n",
    "        \"word2vec path\": \"./Data/word2vec\",\n",
    "        \"word2vec file\": \"word2vec_col.txt\",\n",
    "        \"train data\": \"mex_train.txt\",\n",
    "        \"train labels\": \"mex_train_labels.txt\",\n",
    "        \"validation data\": \"mex_val.txt\",\n",
    "        \"validation labels\": \"mex_val_labels.txt\",\n",
    "        \"path model\": \"../Data/Model_03\",\n",
    "        \"file model\": \"model_best.pt\",\n",
    "        \"stadistics  file\": \"stadistics.csv\",\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_args() -> Namespace:\n",
    "    args = Namespace()\n",
    "    args.batch_size = 64\n",
    "    args.num_workers = 2\n",
    "    args.N = 4\n",
    "    # Dimension of word Embeddings\n",
    "    args.d = 100\n",
    "    # Dimension for Hidden Layer\n",
    "    args.d_h = 200\n",
    "    args.dropout = 0.1\n",
    "    # Training hyperparameters\n",
    "    args.lr = 2.3e-1\n",
    "    args.num_epochs = 100\n",
    "    args.patience = 20\n",
    "    # Scheduler hyperparameters\n",
    "    args.lr_patience = 10\n",
    "    args.lr_factor = 0.5\n",
    "    # Save directory\n",
    "    args.savedir = \"model\"\n",
    "    makedirs(args.savedir,\n",
    "             exist_ok=True)\n",
    "    return args\n",
    "\n",
    "\n",
    "def init_seeds() -> None:\n",
    "    seed = 1111\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqCkLYu2noeK"
   },
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i91pQrJ-KV5i"
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "class word2vec_class:\n",
    "    def __init__(self, params: dict) -> None:\n",
    "        self.params = params\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        filename = join(self.params[\"word2vec path\"],\n",
    "                        self.params[\"word2vec file\"])\n",
    "        data = read_csv(filename,\n",
    "                        sep=\" \",\n",
    "                        skiprows=1,\n",
    "                        header=0,\n",
    "                        index_col=0)\n",
    "        data = data.T\n",
    "        self.vector_size = len(data)\n",
    "        self.data = self.to_dict(data)\n",
    "\n",
    "    def to_dict(self, data: DataFrame) -> dict:\n",
    "        return data.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq7A_xys1ihw"
   },
   "source": [
    "### ngram_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWztOREC1i2L"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from nltk import FreqDist, ngrams\n",
    "from numpy import array, empty\n",
    "\n",
    "\n",
    "class ngram_model:\n",
    "    def __init__(self, N: int, vocab_max: int = 5000, tokenize: tokenizer = None, embeddings_model: word2vec_class = None) -> None:\n",
    "        self.tokenize = tokenize if tokenize else self.default_tokenize\n",
    "        self.punct = set(['.', ',', ';', ':', '-', '^', '»', '!',\n",
    "                         '¡', '¿', '?', '\"', '\\'', '...', '<url>',\n",
    "                          '*', '@usuario'])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.unk = '<unk>'\n",
    "        self.sos = '<s>'\n",
    "        self.eos = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "\n",
    "    def get_vocabulary_size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def default_tokenize(self, doc: str) -> list:\n",
    "        return doc.split(\"  \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "\n",
    "    def sortFreqDisct(self, freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "\n",
    "    def get_vocabulary(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist([word.lower()\n",
    "                              for sentence in corpus\n",
    "                              for word in self.tokenize(sentence)\n",
    "                              if not self.remove_word(word)])\n",
    "        sorted_words = self.sortFreqDisct(freq_dist)[:self.vocab_max - 3]\n",
    "        return set(sorted_words)\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocabulary = self.get_vocabulary(corpus)\n",
    "        self.vocabulary.add(self.unk)\n",
    "        self.vocabulary.add(self.sos)\n",
    "        self.vocabulary.add(self.eos)\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embedding_matrix = empty([self.get_vocabulary_size(),\n",
    "                                           self.embeddings_model.vector_size])\n",
    "        self.make_data(corpus)\n",
    "\n",
    "    def make_data(self, corpus: str) -> tuple:\n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenize(doc):\n",
    "                word = word.lower()\n",
    "                if word in self.vocabulary and not word in self.word_index:\n",
    "                    self.word_index[word] = id\n",
    "                    self.index_word[id] = word\n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word in self.embeddings_model.data:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model.data[word]\n",
    "                    id += 1\n",
    "        # Always add special tokens\n",
    "        self.word_index.update({\n",
    "            self.unk: id,\n",
    "            self.sos: id + 1,\n",
    "            self.eos: id + 2\n",
    "        })\n",
    "        self.index_word.update({\n",
    "            id: self.unk,\n",
    "            id + 1: self.sos,\n",
    "            id + 2: self.eos\n",
    "        })\n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenize(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [word.lower() for word in doc_tokens]\n",
    "        doc_tokens = [self.sos] * (self.N - 1) + doc_tokens + [self.eos]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "\n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocabulary:\n",
    "                doc_tokens[i] = self.unk\n",
    "        return doc_tokens\n",
    "\n",
    "    def transform(self, corpus: list) -> tuple:\n",
    "        X_ngrams = []\n",
    "        y = []\n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.word_index[word]\n",
    "                                    for word in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return array(X_ngrams), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNT_w8m81jF2"
   },
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVgI-FgD1joQ"
   },
   "outputs": [],
   "source": [
    "from numpy import array, mean, asanyarray, sum, exp, argmax, log\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas import DataFrame, read_csv\n",
    "from numpy.random import multinomial\n",
    "from itertools import permutations\n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "from tabulate import tabulate\n",
    "from shutil import copyfile\n",
    "from os.path import join\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class Mex_data_class:\n",
    "    def __init__(self, params: dict, args: Namespace) -> None:\n",
    "        self.params = params\n",
    "        self.args = args\n",
    "        self.read()\n",
    "\n",
    "    def read(self) -> None:\n",
    "        \"\"\"\n",
    "        Lectura de los archivos de datos a partir de su ruta y nombre de archivo\n",
    "        \"\"\"\n",
    "        train_filename = join(self.params[\"path data\"],\n",
    "                              self.params[\"train data\"])\n",
    "        validation_filename = join(self.params[\"path data\"],\n",
    "                                   self.params[\"train data\"])\n",
    "        self.train_text = self.read_file(train_filename)\n",
    "        self.validation_text = self.read_file(validation_filename)\n",
    "\n",
    "    def read_file(self, filename: str) -> list:\n",
    "        data = read_csv(filename,\n",
    "                        engine=\"python\",\n",
    "                        sep=\"\\r\\n\",\n",
    "                        header=None)\n",
    "        data = list(data[0])\n",
    "        return data\n",
    "\n",
    "    def obtain_data_and_labels(self, ngram: ngram_model) -> None:\n",
    "        self.train_data, self.train_labels = ngram.transform(self.train_text)\n",
    "        self.validation_data, self.validation_labels = ngram.transform(\n",
    "            self.validation_text)\n",
    "\n",
    "    def obtain_loaders(self) -> None:\n",
    "        self.train_loader = obtain_loader(self.train_data,\n",
    "                                          self.train_labels,\n",
    "                                          self.args)\n",
    "        self.validation_loader = obtain_loader(self.validation_data,\n",
    "                                               self.validation_labels,\n",
    "                                               self.args)\n",
    "\n",
    "\n",
    "class neural_language_model(nn.Module):\n",
    "    def __init__(self, args, embeddings: array = None) -> None:\n",
    "        super(neural_language_model, self).__init__()\n",
    "        self.window_size = args.N-1\n",
    "        self.embeding_size = args.d\n",
    "        self.emb = nn.Embedding(args.vocabulary_size,\n",
    "                                args.d)\n",
    "        if embeddings is not None:\n",
    "            for i in range(embeddings.shape[0]):\n",
    "                for j in range(embeddings.shape[1]):\n",
    "                    self.emb.weight.data[i][j] = embeddings[i][j]\n",
    "        self.fc1 = nn.Linear(args.d*(args.N-1),\n",
    "                             args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h,\n",
    "                             args.vocabulary_size,\n",
    "                             bias=False)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size*self.embeding_size)\n",
    "        h = torch.tanh(self.fc1(x))\n",
    "        h = self.fc2(h)\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        h = self.drop1(h)\n",
    "        return h\n",
    "\n",
    "    def read_model(self, path: str, name: str) -> None:\n",
    "        filename = join(path, name)\n",
    "        if torch.cuda.is_available():\n",
    "            self.load_state_dict(torch.load(filename)[\"state_dict\"])\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(filename,\n",
    "                                            map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "        self.train(False)\n",
    "\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self, model: neural_language_model, args: Namespace, train_loader, validation_loader):\n",
    "        self.validation_loader = validation_loader\n",
    "        self.train_loader = train_loader\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def get_pred(self, raw_logits):\n",
    "        probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "        y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def model_eval(self, data):\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            tgts = []\n",
    "            for window_words, labels in data:\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                outputs = self.model(window_words)\n",
    "                # Get prediction\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.numpy()\n",
    "                tgts.append(tgt)\n",
    "                preds.append(y_pred)\n",
    "        tgts = [e for l in tgts for e in l]\n",
    "        preds = [e for l in preds for e in l]\n",
    "        return accuracy_score(tgts, preds)\n",
    "\n",
    "    def save_checkpoint(self, state,\n",
    "                        is_best: bool,\n",
    "                        checkpoint_path: str,\n",
    "                        filename: str = 'checkpoint.pt',\n",
    "                        best_model_name: str = 'model_best.pt') -> None:\n",
    "        print(checkpoint_path, filename)\n",
    "        name = join(checkpoint_path,\n",
    "                    filename)\n",
    "        torch.save(state,\n",
    "                   name)\n",
    "        if is_best:\n",
    "            filename_best = join(checkpoint_path,\n",
    "                                 best_model_name)\n",
    "            copyfile(name,\n",
    "                     filename_best)\n",
    "\n",
    "    def run(self):\n",
    "        stadistics = DataFrame(columns=[\"Train acc\",\n",
    "                                        \"Loss\",\n",
    "                                        \"Val acc\",\n",
    "                                        \"Time\"])\n",
    "        start_time = time.time()\n",
    "        best_metric = 0\n",
    "        metric_history = []\n",
    "        train_metric_history = []\n",
    "        criterion, optimizer, scheduler = init_models_parameters(self.model,\n",
    "                                                                 self.args)\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            loss_epoch = []\n",
    "            training_metric = []\n",
    "            self.model.train()\n",
    "            for window_words, labels in self.train_loader:\n",
    "                # If GPU available\n",
    "                if self.args.use_gpu:\n",
    "                    window_words = window_words.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                # Forward pass\n",
    "                outputs = self.model(window_words)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss_epoch.append(loss.item())\n",
    "                # Get Trainning Metrics\n",
    "                y_pred = self.get_pred(outputs)\n",
    "                tgt = labels.cpu().numpy()\n",
    "                training_metric.append(accuracy_score(tgt, y_pred))\n",
    "                # Backward and Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # Get Metric in Trainning Dataset\n",
    "            mean_epoch_metric = mean(training_metric)\n",
    "            train_metric_history.append(mean_epoch_metric)\n",
    "            # Get Metric in Validation Dataset\n",
    "            self.model.eval()\n",
    "            tuning_metric = self.model_eval(self.validation_loader)\n",
    "            metric_history.append(mean_epoch_metric)\n",
    "            # Update Scheduler\n",
    "            scheduler.step(tuning_metric)\n",
    "            # Check for Metric Improvement\n",
    "            is_improvement = tuning_metric > best_metric\n",
    "            if is_improvement:\n",
    "                best_metric = tuning_metric\n",
    "                n_no_improve = 0\n",
    "            else:\n",
    "                n_no_improve += 1\n",
    "            # Save best model if metric improved\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_metric': best_metric, }\n",
    "            self.save_checkpoint(\n",
    "                state,\n",
    "                is_improvement,\n",
    "                self.args.savedir,\n",
    "            )\n",
    "            # Early stopping\n",
    "            if n_no_improve >= self.args.patience:\n",
    "                print('No improvement. Breaking out of loop')\n",
    "                break\n",
    "            finish_time = time.time()-epoch_start_time\n",
    "            stadistics.loc[epoch+1] = [mean_epoch_metric,\n",
    "                                       mean(loss_epoch),\n",
    "                                       tuning_metric,\n",
    "                                       finish_time]\n",
    "            print('Train acc: {}'.format(mean_epoch_metric))\n",
    "            print('Epoch[{}/{}], Loss : {:4f} - Val accuracy: {:4f} - Epoch time: {:2f}'.format(\n",
    "                epoch + 1,\n",
    "                self.args.num_epochs,\n",
    "                mean(loss_epoch),\n",
    "                tuning_metric,\n",
    "                time.time() - epoch_start_time))\n",
    "            print('--- %s seconds ---' % (time.time() - start_time))\n",
    "        return stadistics\n",
    "\n",
    "\n",
    "class generate_text_class:\n",
    "    def __init__(self, ngram_data: ngram_model, model: neural_language_model, tokenize: tokenizer) -> None:\n",
    "        self.ngram_data = ngram_data\n",
    "        self.tokenize = tokenize\n",
    "        self.model = model\n",
    "\n",
    "    def parse_text(self, text: str) -> tuple:\n",
    "        all_tokens = [word.lower()\n",
    "                      if word in self.ngram_data.word_index else self.ngram_data.eos\n",
    "                      for word in self.tokenize(text)]\n",
    "        tokens_id = [self.ngram_data.word_index[word]\n",
    "                     for word in all_tokens]\n",
    "        return all_tokens, tokens_id\n",
    "\n",
    "    def sample_next_word(self, logits: array, temperature: float) -> int:\n",
    "        logits = asanyarray(logits).astype(\"float64\")\n",
    "        preds = logits/temperature\n",
    "        exp_preds = exp(preds)\n",
    "        preds = exp_preds/sum(exp_preds)\n",
    "        probability = multinomial(1, preds)\n",
    "        return argmax(probability)\n",
    "\n",
    "    def predict_next_token(self, tokens_id: list) -> int:\n",
    "        word_index_tensor = torch.LongTensor(tokens_id).unsqueeze(0)\n",
    "        y_raw_predict = self.model(\n",
    "            word_index_tensor).squeeze(0).detach().numpy()\n",
    "        y_pred = self.sample_next_word(y_raw_predict, 1.0)\n",
    "        return y_pred\n",
    "\n",
    "    def run(self, initial_text: str):\n",
    "        tokens, window_word_index = self.parse_text(initial_text)\n",
    "        for i in range(300):\n",
    "            y_pred = self.predict_next_token(window_word_index)\n",
    "            next_word = self.ngram_data.index_word[y_pred]\n",
    "            tokens.append(next_word)\n",
    "            if next_word == self.ngram_data.eos:\n",
    "                break\n",
    "            else:\n",
    "                window_word_index.pop(0)\n",
    "                window_word_index.append(y_pred)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def obtain_closet_words(self, word: str, n: int) -> None:\n",
    "        print(\"Palabras cercanas a {}\".format(word))\n",
    "        word_id = torch.LongTensor([self.ngram_data.word_index[word]])\n",
    "        word_embed = self.model.emb(word_id)\n",
    "        # Compute distances to all words\n",
    "        dist = torch.norm(self.model.emb.weight-word_embed, dim=1).detach()\n",
    "        lst = sorted(enumerate(dist.numpy()),\n",
    "                     key=lambda x: x[1])\n",
    "        table = []\n",
    "        for idx, difference in lst[1:n+1]:\n",
    "            table += [[self.ngram_data.index_word[idx],\n",
    "                       difference]]\n",
    "        print(tabulate(table,\n",
    "                       headers=[\"Word\", \"Difference\"]))\n",
    "        print()\n",
    "\n",
    "\n",
    "def init_models_parameters(model: neural_language_model, args: Namespace) -> tuple:\n",
    "    args.use_gpu = torch.cuda.is_available()\n",
    "    if args.use_gpu:\n",
    "        model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           \"min\",\n",
    "                                                           patience=args.lr_patience,\n",
    "                                                           verbose=True,\n",
    "                                                           factor=args.lr_factor)\n",
    "    return criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "def obtain_loader(data: array, labels: array, args: Namespace) -> DataLoader:\n",
    "    dataset = TensorDataset(torch.tensor(data,\n",
    "                                         dtype=torch.int64),\n",
    "                            torch.tensor(labels,\n",
    "                                         dtype=torch.int64))\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        num_workers=args.num_workers,\n",
    "                        shuffle=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def log_likelihood(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
    "    x, y = ngram_data.transform(text)\n",
    "    x, y = x[2:], y[2:]\n",
    "    x = torch.LongTensor(x).unsqueeze(0)\n",
    "    logits = model(x).detach()\n",
    "    probability = F.softmax(logits, dim=1).numpy()\n",
    "    return sum(log([probability[i][w]\n",
    "                    for i, w in enumerate(y)]))\n",
    "\n",
    "\n",
    "def perplexity(model: neural_language_model, text: str, ngram_data: ngram_model) -> float:\n",
    "    perplexity_value = log_likelihood(model, text, ngram_data)\n",
    "    perplexity_value = - perplexity_value / len(text)\n",
    "    return perplexity_value\n",
    "\n",
    "\n",
    "def syntax_structure(model: neural_language_model, ngram_data: ngram_model, word: str, tokenize: tokenizer) -> None:\n",
    "    words = tokenize(word)\n",
    "    perms = [\" \".join(perm) for perm in permutations(words)]\n",
    "    best_log_likelihood = [(log_likelihood(model,\n",
    "                                           pharse,\n",
    "                                           ngram_data),\n",
    "                            pharse)\n",
    "                           for pharse in perms]\n",
    "    best_log_likelihood = sorted(best_log_likelihood, reverse=True)\n",
    "    headers = [\"Palabra\", \"Perplejidad\"]\n",
    "    print(\"-\"*40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[:5]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results,\n",
    "                   headers=headers))\n",
    "    print(\"-\"*40)\n",
    "    results = []\n",
    "    for p, i in best_log_likelihood[-5:]:\n",
    "        results += [[i, p]]\n",
    "    print(tabulate(results,\n",
    "                   headers=headers))\n",
    "\n",
    "\n",
    "def save_stadistics(params: dict, stadistics: DataFrame) -> None:\n",
    "    filename = join(params[\"path data\"],\n",
    "                    params[\"stadistics  file\"])\n",
    "    stadistics.index.name = \"Epoch\"\n",
    "    stadistics.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbgmQWnjx1lE"
   },
   "source": [
    "### Inicialización de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFjuqOpM1xvH",
    "outputId": "65fc6def-0fa4-439b-ca18-43e581ea0fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura de archivos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer as tokenizer\n",
    "# Semillas de las funciones aleatorias\n",
    "init_seeds()\n",
    "# Recoleccion de los parametros y argumentos\n",
    "params = get_params()\n",
    "args = get_args()\n",
    "# Definicion del tokenizer\n",
    "tokenize = tokenizer().tokenize\n",
    "print(\"Lectura de archivos\")\n",
    "# Lectura de los datos\n",
    "mex_data = Mex_data_class(params, args)\n",
    "# Lectura de word2vec embeddings\n",
    "word2vec = word2vec_class(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x27lyw6GIYSG"
   },
   "outputs": [],
   "source": [
    "# Inicializacion del modelo de ngramas\n",
    "ngram = ngram_model(args.N,\n",
    "                    tokenize=tokenize,\n",
    "                    embeddings_model=word2vec)\n",
    "ngram.fit(mex_data.train_text)\n",
    "# Argumento del tamaño del vocabulario\n",
    "args.vocabulary_size = ngram.get_vocabulary_size()\n",
    "# # Estructuración de los datos para la red neuronal\n",
    "mex_data.obtain_data_and_labels(ngram)\n",
    "mex_data.obtain_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9YDq0a33_i4"
   },
   "outputs": [],
   "source": [
    "# Inicializacion de la red neuronal\n",
    "neural_model = neural_language_model(args)\n",
    "# Inicializacion del modelo de prediccion\n",
    "model = model_class(neural_model,\n",
    "                    args,\n",
    "                    mex_data.train_loader,\n",
    "                    mex_data.validation_loader)\n",
    "# Entrenamiento de la neurona\n",
    "# stadistics=model.run()\n",
    "# save_stadistics(params,stadistics)\n",
    "# Lectura de los parametros de la red neuronal\n",
    "neural_model.read_model(params[\"path model\"],\n",
    "                        params[\"file model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8flRw3ISitfv"
   },
   "source": [
    "### Punto 1\n",
    "Con base en la implementación mostrada en clase, construya un modelo de lenguaje neuronal a nivel de palabra, pero preinicializado con los embeddings proporcionados. Tomé en cuenta secuencias de tamaño 4 para el modelo, es decir hasta 3 palabras en el contexto. Después de haber entrenado el modelo, recupere las 10 palabras más similares a tres palabras de su gusto dadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNXUaOBHitQC",
    "outputId": "0f0e6f17-62a4-4336-dd68-c5a49d360d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras cercanas a pinche\n",
      "Word          Difference\n",
      "----------  ------------\n",
      "clave            11.4113\n",
      "lamentable       11.5535\n",
      "destino          11.5777\n",
      "paraguayos       11.6101\n",
      "atención         11.7115\n",
      "<unk>            11.7257\n",
      "disculpa         11.7937\n",
      "sano             11.8065\n",
      "malas            11.8514\n",
      "cada             11.9422\n",
      "\n",
      "Palabras cercanas a saludo\n",
      "Word          Difference\n",
      "----------  ------------\n",
      "daca             11.6043\n",
      "estaríamos       11.6057\n",
      "<unk>            11.6599\n",
      "tirarse          11.8545\n",
      "celosa           11.8973\n",
      "policía          11.9049\n",
      "diputado         11.9619\n",
      "vídeos           11.9854\n",
      "bastardo         12.0951\n",
      "trabajan         12.1184\n",
      "\n",
      "Palabras cercanas a verga\n",
      "Word        Difference\n",
      "--------  ------------\n",
      "cambiar        10.6213\n",
      "<unk>          10.6276\n",
      "entramos       10.831\n",
      "déjense        10.9058\n",
      "fotitos        11.0836\n",
      "borrar         11.0878\n",
      "valí           11.1153\n",
      "pasarse        11.1421\n",
      "exista         11.2387\n",
      "reventar       11.2413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text = generate_text_class(ngram,\n",
    "                                    neural_model,\n",
    "                                    tokenize)\n",
    "generate_text.obtain_closet_words(\"pinche\", 10)\n",
    "generate_text.obtain_closet_words(\"saludo\", 10)\n",
    "generate_text.obtain_closet_words(\"verga\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7jeTUQFypit"
   },
   "source": [
    "## Punto 2\n",
    "Ponga al modelo a generar texto a partir\n",
    "de tres secuencias de inicio de su gusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3V022_x4Ubk",
    "outputId": "e2f9c9af-5169-407a-e8a4-75bbb29dd2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Primer palabra\n",
      "hola como estas putos y después me <unk> ni nada por méxico con la <unk> <unk> además que no le importa nada que <unk> <unk> esta mierda eso sophie el mismo <unk> hdp jajajaj </s>\n",
      "----------------------------------------\n",
      "Segunda palabra\n",
      "espero me sigan loca desde aquellos tiempos <unk> <unk> <unk> </s>\n",
      "----------------------------------------\n",
      "Tercera palabra\n",
      "<s> <s> me estoy convertido de <unk> masculinos solo por <unk> marica <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"Primer palabra\")\n",
    "print(generate_text.run(\"hola como estas\"))\n",
    "print(\"-\"*40)\n",
    "print(\"Segunda palabra\")\n",
    "print(generate_text.run(\"espero me sigan\"))\n",
    "print(\"-\"*40)\n",
    "print(\"Tercera palabra\")\n",
    "print(generate_text.run(\"<s> <s> me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4JRuUEFytMJ"
   },
   "source": [
    "## Punto 3\n",
    "Escriba 5 ejemplos de oraciones y mídales el likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9-TMSBdmjNL",
    "outputId": "bf2387b8-8391-4fbf-ed30-92865a7db5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -365.38785\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"Dejalo que termine\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5A5FNtCxprNy",
    "outputId": "4ef8e90e-0e9a-4584-8f85-4397ec6d0e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -924.7808\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"esperate a que tenga servicios, ya completos\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4xrYIP2p4bf",
    "outputId": "b77196cb-203f-4c8f-82a6-933283705aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -644.4791\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"asi te ganas un chingo de gente\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpJbHMQVo4OG",
    "outputId": "c841892e-21b4-4b42-d083-7edf1e1e76bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -830.0174\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"eso que esten en redes con sus criticas\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gG9MqZDapBvl",
    "outputId": "9442199a-9138-4296-f12f-318a354f696e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood -771.1207\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood\", log_likelihood(neural_model,\n",
    "                                       \"unas tlayudas no le hacen daño a nadie\",\n",
    "                                       ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfIvM8zny2ti"
   },
   "source": [
    "## Punto 4\n",
    "Proponga un ejemplo para ver estructuras sintácticas (permutaciones de palabras de alguna oración) buenas usando el likelihood a partir de una oración que usted proponga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4r9DC2yp8a-",
    "outputId": "ea361c4b-23d1-4d43-a4bc-3ea2364cfd11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Palabra                 Perplejidad\n",
      "--------------------  -------------\n",
      "lleva me la chingada       -420.757\n",
      "lleva me chingada la       -420.757\n",
      "lleva la me chingada       -420.757\n",
      "lleva chingada me la       -420.757\n",
      "la me lleva chingada       -420.757\n",
      "----------------------------------------\n",
      "Palabra                 Perplejidad\n",
      "--------------------  -------------\n",
      "chingada lleva me la       -424.423\n",
      "chingada me la lleva       -424.423\n",
      "chingada lleva la me       -424.423\n",
      "chingada la me lleva       -424.423\n",
      "chingada la lleva me       -424.423\n"
     ]
    }
   ],
   "source": [
    "word = \"me lleva la chingada\"\n",
    "syntax_structure(neural_model, ngram, word, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sERbz-ry_SF"
   },
   "source": [
    "## Punto 4\n",
    "Calcule la perplejidad del modelo sobre los datos val. Compárelo con la perplejidad del modelo de lenguaje sin embeddings preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYRZVtCSqF-A",
    "outputId": "99625fef-3b55-49d2-a72b-35bee012cbe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.21065566378066"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(neural_model,\n",
    "           mex_data.validation_text,\n",
    "           ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xovw8vJTzJ1r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Problema_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
