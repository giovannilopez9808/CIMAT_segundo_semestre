\section{Methods}

\subsection{Transformers}

In the works of NLP, the use of pre-entrained language models hace become a useful block to get a better result on every task. One of the most competitive neural sequence transduction models have an encoder-decoder structure\cite{Bahdanau_2014,Cho_2014}. Here, the encoder maps an input sequence of symbol representations $(x_1 , \dots, x_n)$ to a sequence of continuous representations $z = (z_1 , \dots, z_n )$. Given z, the decoder then generates an output sequence $(y_1 , \dots, y_m )$ of symbols one element at a time. At each step the model is auto-regressive\cite{Graves_2013}, consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder (figure \ref{fig:transformer}). The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position wise fully connected feed-forward network.

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{Graphics/transformer.png}
    \caption{Transformer model representation\cite{Vaswani_2017}.}
    \label{fig:transformer}
\end{figure}

\subsection{Bert}

\subsection{Robertuito}

\subsection{MEX-A3T}

\input{Files/mex_a3t_results.tex}